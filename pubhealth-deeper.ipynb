{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets transformers evaluate imbalanced-learn umap-learn wandb GPUtil","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:59:10.164714Z","iopub.execute_input":"2023-03-21T09:59:10.165042Z","iopub.status.idle":"2023-03-21T09:59:24.894823Z","shell.execute_reply.started":"2023-03-21T09:59:10.164990Z","shell.execute_reply":"2023-03-21T09:59:24.893587Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (2.1.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.26.1)\nCollecting evaluate\n  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: imbalanced-learn in /opt/conda/lib/python3.7/site-packages (0.10.1)\nRequirement already satisfied: umap-learn in /opt/conda/lib/python3.7/site-packages (0.5.3)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.7/site-packages (0.13.10)\nCollecting GPUtil\n  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.21.6)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.28.2)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (5.0.0)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2023.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.11.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.12.1)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.64.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (23.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: scikit-learn>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn) (1.0.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn) (3.1.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn) (1.2.0)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn) (1.7.3)\nRequirement already satisfied: pynndescent>=0.5 in /opt/conda/lib/python3.7/site-packages (from umap-learn) (0.5.8)\nRequirement already satisfied: numba>=0.49 in /opt/conda/lib/python3.7/site-packages (from umap-learn) (0.56.4)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (8.1.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from wandb) (59.8.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from wandb) (4.4.0)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.7/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.1.30)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.15.0)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (22.2.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.8.2)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /opt/conda/lib/python3.7/site-packages (from numba>=0.49->umap-learn) (0.39.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.11.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.14)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2022.7.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\nBuilding wheels for collected packages: GPUtil\n  Building wheel for GPUtil (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7409 sha256=f6d54ccaf10f112e1f8ee4755b5de55a99abe289d87867218fdd82f357832ff3\n  Stored in directory: /root/.cache/pip/wheels/b1/e7/99/2b32600270cf23194c9860f029d3d5db075f250bc39028c045\nSuccessfully built GPUtil\nInstalling collected packages: GPUtil, evaluate\nSuccessfully installed GPUtil-1.4.0 evaluate-0.4.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import gc\nimport torch\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:59:24.897864Z","iopub.execute_input":"2023-03-21T09:59:24.898488Z","iopub.status.idle":"2023-03-21T09:59:27.011190Z","shell.execute_reply.started":"2023-03-21T09:59:24.898444Z","shell.execute_reply":"2023-03-21T09:59:27.010064Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:59:27.012435Z","iopub.execute_input":"2023-03-21T09:59:27.013075Z","iopub.status.idle":"2023-03-21T09:59:27.398801Z","shell.execute_reply.started":"2023-03-21T09:59:27.013033Z","shell.execute_reply":"2023-03-21T09:59:27.397779Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-21T09:59:27.401584Z","iopub.execute_input":"2023-03-21T09:59:27.401963Z","iopub.status.idle":"2023-03-21T09:59:27.409585Z","shell.execute_reply.started":"2023-03-21T09:59:27.401924Z","shell.execute_reply":"2023-03-21T09:59:27.407961Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\nhealth_fact = load_dataset('health_fact')","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:59:27.411036Z","iopub.execute_input":"2023-03-21T09:59:27.411695Z","iopub.status.idle":"2023-03-21T09:59:44.322115Z","shell.execute_reply.started":"2023-03-21T09:59:27.411656Z","shell.execute_reply":"2023-03-21T09:59:44.321118Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c7b18983a4a4769aae5afd1cfc50e04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df066845374844aca03d45f03a41e67d"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset health_fact/default (download: 23.74 MiB, generated: 64.34 MiB, post-processed: Unknown size, total: 88.08 MiB) to /root/.cache/huggingface/datasets/health_fact/default/1.1.0/99503637e4255bd805f84d57031c18fe4dd88298f00299d56c94fc59ed68ec19...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/24.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c39b18ee900405fb13e5e3a64c95bcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/9832 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1235 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1225 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset health_fact downloaded and prepared to /root/.cache/huggingface/datasets/health_fact/default/1.1.0/99503637e4255bd805f84d57031c18fe4dd88298f00299d56c94fc59ed68ec19. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a3ebdfd0a5e4a039d52136905435a1e"}},"metadata":{}}]},{"cell_type":"code","source":"# Filter out instances with a -1 label\nhealth_fact['train'] = health_fact['train'].filter(lambda x: x['label'] != -1)\nhealth_fact['validation'] = health_fact['validation'].filter(lambda x: x['label'] != -1)\nhealth_fact['test'] = health_fact['test'].filter(lambda x: x['label'] != -1)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:59:44.323832Z","iopub.execute_input":"2023-03-21T09:59:44.324624Z","iopub.status.idle":"2023-03-21T09:59:44.766775Z","shell.execute_reply.started":"2023-03-21T09:59:44.324583Z","shell.execute_reply":"2023-03-21T09:59:44.765626Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5da234aae6fc4d938525b565cd391114"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a1cbb9ea7ca49b7b6a3927e5e64673b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40b3ed24b5b344f191795e4d41ed45f7"}},"metadata":{}}]},{"cell_type":"code","source":"health_fact.set_format(type=\"pandas\")\ndf = health_fact[\"train\"][:]\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:59:44.768296Z","iopub.execute_input":"2023-03-21T09:59:44.768851Z","iopub.status.idle":"2023-03-21T09:59:45.026767Z","shell.execute_reply.started":"2023-03-21T09:59:44.768812Z","shell.execute_reply":"2023-03-21T09:59:45.025603Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"  claim_id                                              claim  \\\n0    15661  \"The money the Clinton Foundation took from fr...   \n1     9893    Annual Mammograms May Have More False-Positives   \n2    11358  SBRT Offers Prostate Cancer Patients High Canc...   \n3    10166  Study: Vaccine for Breast, Ovarian Cancer Has ...   \n4    11276  Some appendicitis cases may not require ’emerg...   \n\n       date_published                                        explanation  \\\n0      April 26, 2015  \"Gingrich said the Clinton Foundation \"\"took m...   \n1    October 18, 2011  This article reports on the results of a study...   \n2  September 28, 2016  This news release describes five-year outcomes...   \n3    November 8, 2011  While the story does many things well, the ove...   \n4  September 20, 2010  We really don’t understand why only a handful ...   \n\n                                       fact_checkers  \\\n0                                      Katie Sanders   \n1                                                      \n2  Mary Chris Jaklevic,Steven J. Atlas, MD, MPH,K...   \n3                                                      \n4                                                      \n\n                                           main_text  \\\n0  \"Hillary Clinton is in the political crosshair...   \n1  While the financial costs of screening mammogr...   \n2  The news release quotes lead researcher Robert...   \n3  The story does discuss costs, but the framing ...   \n4  \"Although the story didn’t cite the cost of ap...   \n\n                                             sources  label  \\\n0  https://www.wsj.com/articles/clinton-foundatio...      0   \n1                                                         1   \n2  https://www.healthnewsreview.org/wp-content/up...      1   \n3  http://clinicaltrials.gov/ct2/results?term=can...      2   \n4                                                         2   \n\n                                      subjects  \n0  Foreign Policy, PunditFact, Newt Gingrich,   \n1               Screening,WebMD,women's health  \n2      Association/Society news release,Cancer  \n3                  Cancer,WebMD,women's health  \n4                                               ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>claim_id</th>\n      <th>claim</th>\n      <th>date_published</th>\n      <th>explanation</th>\n      <th>fact_checkers</th>\n      <th>main_text</th>\n      <th>sources</th>\n      <th>label</th>\n      <th>subjects</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>15661</td>\n      <td>\"The money the Clinton Foundation took from fr...</td>\n      <td>April 26, 2015</td>\n      <td>\"Gingrich said the Clinton Foundation \"\"took m...</td>\n      <td>Katie Sanders</td>\n      <td>\"Hillary Clinton is in the political crosshair...</td>\n      <td>https://www.wsj.com/articles/clinton-foundatio...</td>\n      <td>0</td>\n      <td>Foreign Policy, PunditFact, Newt Gingrich,</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9893</td>\n      <td>Annual Mammograms May Have More False-Positives</td>\n      <td>October 18, 2011</td>\n      <td>This article reports on the results of a study...</td>\n      <td></td>\n      <td>While the financial costs of screening mammogr...</td>\n      <td></td>\n      <td>1</td>\n      <td>Screening,WebMD,women's health</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11358</td>\n      <td>SBRT Offers Prostate Cancer Patients High Canc...</td>\n      <td>September 28, 2016</td>\n      <td>This news release describes five-year outcomes...</td>\n      <td>Mary Chris Jaklevic,Steven J. Atlas, MD, MPH,K...</td>\n      <td>The news release quotes lead researcher Robert...</td>\n      <td>https://www.healthnewsreview.org/wp-content/up...</td>\n      <td>1</td>\n      <td>Association/Society news release,Cancer</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10166</td>\n      <td>Study: Vaccine for Breast, Ovarian Cancer Has ...</td>\n      <td>November 8, 2011</td>\n      <td>While the story does many things well, the ove...</td>\n      <td></td>\n      <td>The story does discuss costs, but the framing ...</td>\n      <td>http://clinicaltrials.gov/ct2/results?term=can...</td>\n      <td>2</td>\n      <td>Cancer,WebMD,women's health</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11276</td>\n      <td>Some appendicitis cases may not require ’emerg...</td>\n      <td>September 20, 2010</td>\n      <td>We really don’t understand why only a handful ...</td>\n      <td></td>\n      <td>\"Although the story didn’t cite the cost of ap...</td>\n      <td></td>\n      <td>2</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def label_int2str(row):\n    if row == -1:\n        return 'invalid'\n    return health_fact[\"train\"].features[\"label\"].int2str(row)\n\ndf[\"label_name\"] = df[\"label\"].apply(label_int2str)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:59:45.028383Z","iopub.execute_input":"2023-03-21T09:59:45.030305Z","iopub.status.idle":"2023-03-21T09:59:45.064942Z","shell.execute_reply.started":"2023-03-21T09:59:45.030260Z","shell.execute_reply":"2023-03-21T09:59:45.063852Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:59:45.066295Z","iopub.execute_input":"2023-03-21T09:59:45.066648Z","iopub.status.idle":"2023-03-21T09:59:45.083230Z","shell.execute_reply.started":"2023-03-21T09:59:45.066610Z","shell.execute_reply":"2023-03-21T09:59:45.082097Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"  claim_id                                              claim  \\\n0    15661  \"The money the Clinton Foundation took from fr...   \n1     9893    Annual Mammograms May Have More False-Positives   \n2    11358  SBRT Offers Prostate Cancer Patients High Canc...   \n3    10166  Study: Vaccine for Breast, Ovarian Cancer Has ...   \n4    11276  Some appendicitis cases may not require ’emerg...   \n\n       date_published                                        explanation  \\\n0      April 26, 2015  \"Gingrich said the Clinton Foundation \"\"took m...   \n1    October 18, 2011  This article reports on the results of a study...   \n2  September 28, 2016  This news release describes five-year outcomes...   \n3    November 8, 2011  While the story does many things well, the ove...   \n4  September 20, 2010  We really don’t understand why only a handful ...   \n\n                                       fact_checkers  \\\n0                                      Katie Sanders   \n1                                                      \n2  Mary Chris Jaklevic,Steven J. Atlas, MD, MPH,K...   \n3                                                      \n4                                                      \n\n                                           main_text  \\\n0  \"Hillary Clinton is in the political crosshair...   \n1  While the financial costs of screening mammogr...   \n2  The news release quotes lead researcher Robert...   \n3  The story does discuss costs, but the framing ...   \n4  \"Although the story didn’t cite the cost of ap...   \n\n                                             sources  label  \\\n0  https://www.wsj.com/articles/clinton-foundatio...      0   \n1                                                         1   \n2  https://www.healthnewsreview.org/wp-content/up...      1   \n3  http://clinicaltrials.gov/ct2/results?term=can...      2   \n4                                                         2   \n\n                                      subjects label_name  \n0  Foreign Policy, PunditFact, Newt Gingrich,       false  \n1               Screening,WebMD,women's health    mixture  \n2      Association/Society news release,Cancer    mixture  \n3                  Cancer,WebMD,women's health       true  \n4                                                    true  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>claim_id</th>\n      <th>claim</th>\n      <th>date_published</th>\n      <th>explanation</th>\n      <th>fact_checkers</th>\n      <th>main_text</th>\n      <th>sources</th>\n      <th>label</th>\n      <th>subjects</th>\n      <th>label_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>15661</td>\n      <td>\"The money the Clinton Foundation took from fr...</td>\n      <td>April 26, 2015</td>\n      <td>\"Gingrich said the Clinton Foundation \"\"took m...</td>\n      <td>Katie Sanders</td>\n      <td>\"Hillary Clinton is in the political crosshair...</td>\n      <td>https://www.wsj.com/articles/clinton-foundatio...</td>\n      <td>0</td>\n      <td>Foreign Policy, PunditFact, Newt Gingrich,</td>\n      <td>false</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9893</td>\n      <td>Annual Mammograms May Have More False-Positives</td>\n      <td>October 18, 2011</td>\n      <td>This article reports on the results of a study...</td>\n      <td></td>\n      <td>While the financial costs of screening mammogr...</td>\n      <td></td>\n      <td>1</td>\n      <td>Screening,WebMD,women's health</td>\n      <td>mixture</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11358</td>\n      <td>SBRT Offers Prostate Cancer Patients High Canc...</td>\n      <td>September 28, 2016</td>\n      <td>This news release describes five-year outcomes...</td>\n      <td>Mary Chris Jaklevic,Steven J. Atlas, MD, MPH,K...</td>\n      <td>The news release quotes lead researcher Robert...</td>\n      <td>https://www.healthnewsreview.org/wp-content/up...</td>\n      <td>1</td>\n      <td>Association/Society news release,Cancer</td>\n      <td>mixture</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10166</td>\n      <td>Study: Vaccine for Breast, Ovarian Cancer Has ...</td>\n      <td>November 8, 2011</td>\n      <td>While the story does many things well, the ove...</td>\n      <td></td>\n      <td>The story does discuss costs, but the framing ...</td>\n      <td>http://clinicaltrials.gov/ct2/results?term=can...</td>\n      <td>2</td>\n      <td>Cancer,WebMD,women's health</td>\n      <td>true</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11276</td>\n      <td>Some appendicitis cases may not require ’emerg...</td>\n      <td>September 20, 2010</td>\n      <td>We really don’t understand why only a handful ...</td>\n      <td></td>\n      <td>\"Although the story didn’t cite the cost of ap...</td>\n      <td></td>\n      <td>2</td>\n      <td></td>\n      <td>true</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# df[\"label_name\"].value_counts(ascending=True).plot.barh()\n\nax = sns.countplot(x='label_name', data=df, order = df['label_name'].value_counts().index)\nax.bar_label(ax.containers[0])\nplt.title(\"Frequency of Classes\")","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:59:45.088686Z","iopub.execute_input":"2023-03-21T09:59:45.089440Z","iopub.status.idle":"2023-03-21T09:59:45.346441Z","shell.execute_reply.started":"2023-03-21T09:59:45.089410Z","shell.execute_reply":"2023-03-21T09:59:45.345434Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Text(0.5, 1.0, 'Frequency of Classes')"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"df['Words Per Explanation'] = df['explanation'].str.split().apply(len)\nsns.boxplot(x=df['Words Per Explanation'], y=df['label_name'], data=df, order = df['label_name'].value_counts().index)\nplt.title(\"Words Per Explanation\")\nplt.xlabel(\"\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:59:45.348981Z","iopub.execute_input":"2023-03-21T09:59:45.349777Z","iopub.status.idle":"2023-03-21T09:59:45.870149Z","shell.execute_reply.started":"2023-03-21T09:59:45.349705Z","shell.execute_reply":"2023-03-21T09:59:45.868991Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_ckpt = 'distilbert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:59:45.871846Z","iopub.execute_input":"2023-03-21T09:59:45.872581Z","iopub.status.idle":"2023-03-21T10:00:01.489634Z","shell.execute_reply.started":"2023-03-21T09:59:45.872541Z","shell.execute_reply":"2023-03-21T10:00:01.488358Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77f4dc63f1f34a0eb2498424b7316008"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca48dd8133a34b7e8e4269d7c11ea25a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9446efa1a9ed45ee982c70e418f91206"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4aad7faf3b4433c95044065ae5ac845"}},"metadata":{}}]},{"cell_type":"code","source":"# checkpoint = \"bert-base-uncased\"\n# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# tokenized_explanations = tokenizer(health_fact[\"train\"][\"explanation\"], max_length=512, truncation=True, padding=True, return_tensors=\"pt\")","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:00:01.491145Z","iopub.execute_input":"2023-03-21T10:00:01.492211Z","iopub.status.idle":"2023-03-21T10:00:01.498071Z","shell.execute_reply.started":"2023-03-21T10:00:01.492175Z","shell.execute_reply":"2023-03-21T10:00:01.496825Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"health_fact.reset_format()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:00:01.499923Z","iopub.execute_input":"2023-03-21T10:00:01.500593Z","iopub.status.idle":"2023-03-21T10:00:01.518744Z","shell.execute_reply.started":"2023-03-21T10:00:01.500555Z","shell.execute_reply":"2023-03-21T10:00:01.517670Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def tokenize(batch):\n    return tokenizer(batch['explanation'], truncation=True, padding=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:00:01.520545Z","iopub.execute_input":"2023-03-21T10:00:01.520922Z","iopub.status.idle":"2023-03-21T10:00:01.530330Z","shell.execute_reply.started":"2023-03-21T10:00:01.520873Z","shell.execute_reply":"2023-03-21T10:00:01.529100Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"health_fact_encoded = health_fact.map(tokenize, batched=True, batch_size=None)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:00:01.531583Z","iopub.execute_input":"2023-03-21T10:00:01.531872Z","iopub.status.idle":"2023-03-21T10:00:07.763279Z","shell.execute_reply.started":"2023-03-21T10:00:01.531845Z","shell.execute_reply":"2023-03-21T10:00:07.762257Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac49de244b804e5d8306f24c1d82bc37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d098fc902fc64cc39016d558064b808d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96d80dfb51f84032999aaefb1db8c7d3"}},"metadata":{}}]},{"cell_type":"code","source":"print(health_fact_encoded[\"train\"].column_names)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:00:07.765097Z","iopub.execute_input":"2023-03-21T10:00:07.765394Z","iopub.status.idle":"2023-03-21T10:00:07.770424Z","shell.execute_reply.started":"2023-03-21T10:00:07.765367Z","shell.execute_reply":"2023-03-21T10:00:07.769246Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"['claim_id', 'claim', 'date_published', 'explanation', 'fact_checkers', 'main_text', 'sources', 'label', 'subjects', 'input_ids', 'attention_mask']\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModel\n\ndevice= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = AutoModel.from_pretrained(model_ckpt).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:00:07.771981Z","iopub.execute_input":"2023-03-21T10:00:07.772733Z","iopub.status.idle":"2023-03-21T10:00:17.671769Z","shell.execute_reply.started":"2023-03-21T10:00:07.772686Z","shell.execute_reply":"2023-03-21T10:00:17.670452Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e624882099054c6da17ec3e713bd49d0"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"def extract_hidden_states(batch):\n    inputs = {k:v.to(device) for k,v in batch.items() if k in tokenizer.model_input_names}\n    with torch.no_grad():\n        last_hidden_state = model(**inputs).last_hidden_state\n    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:00:17.673630Z","iopub.execute_input":"2023-03-21T10:00:17.674744Z","iopub.status.idle":"2023-03-21T10:00:17.682921Z","shell.execute_reply.started":"2023-03-21T10:00:17.674693Z","shell.execute_reply":"2023-03-21T10:00:17.681838Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"health_fact_encoded.set_format('torch', columns=[\"input_ids\", \"attention_mask\", \"label\"])","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:00:17.684375Z","iopub.execute_input":"2023-03-21T10:00:17.684933Z","iopub.status.idle":"2023-03-21T10:00:17.716468Z","shell.execute_reply.started":"2023-03-21T10:00:17.684892Z","shell.execute_reply":"2023-03-21T10:00:17.715457Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from GPUtil import showUtilization as gpu_usage\ngpu_usage()  ","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:00:17.717886Z","iopub.execute_input":"2023-03-21T10:00:17.718925Z","iopub.status.idle":"2023-03-21T10:00:17.795183Z","shell.execute_reply.started":"2023-03-21T10:00:17.718880Z","shell.execute_reply":"2023-03-21T10:00:17.793967Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n| ID | GPU | MEM |\n------------------\n|  0 | 11% |  6% |\n","output_type":"stream"}]},{"cell_type":"code","source":"del df","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:00:17.797186Z","iopub.execute_input":"2023-03-21T10:00:17.797583Z","iopub.status.idle":"2023-03-21T10:00:17.804998Z","shell.execute_reply.started":"2023-03-21T10:00:17.797536Z","shell.execute_reply":"2023-03-21T10:00:17.802524Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:00:17.806654Z","iopub.execute_input":"2023-03-21T10:00:17.807539Z","iopub.status.idle":"2023-03-21T10:00:18.104834Z","shell.execute_reply.started":"2023-03-21T10:00:17.807511Z","shell.execute_reply":"2023-03-21T10:00:18.103737Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from GPUtil import showUtilization as gpu_usage\ngpu_usage()  ","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:00:18.106567Z","iopub.execute_input":"2023-03-21T10:00:18.106913Z","iopub.status.idle":"2023-03-21T10:00:18.184032Z","shell.execute_reply.started":"2023-03-21T10:00:18.106876Z","shell.execute_reply":"2023-03-21T10:00:18.182743Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n| ID | GPU | MEM |\n------------------\n|  0 |  0% |  6% |\n","output_type":"stream"}]},{"cell_type":"code","source":"health_fact_hidden = health_fact_encoded.map(extract_hidden_states, batched=True, batch_size=100)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:00:18.185801Z","iopub.execute_input":"2023-03-21T10:00:18.186346Z","iopub.status.idle":"2023-03-21T10:02:03.990204Z","shell.execute_reply.started":"2023-03-21T10:00:18.186300Z","shell.execute_reply":"2023-03-21T10:02:03.989075Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/99 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5d0cbb7dd7f4d9b9fe3a424340aac71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/13 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c056e89c4854272b5ecd1d607ece8c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/13 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2a42ad8efbc4f378cb462f08b31236d"}},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\n\nX_train = np.array(health_fact_hidden[\"train\"][\"hidden_state\"])\nX_valid = np.array(health_fact_hidden[\"validation\"][\"hidden_state\"])\ny_train = np.array(health_fact_hidden[\"train\"][\"label\"])\ny_valid = np.array(health_fact_hidden[\"validation\"][\"label\"])\nX_train.shape, X_valid.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:02:03.991893Z","iopub.execute_input":"2023-03-21T10:02:03.992259Z","iopub.status.idle":"2023-03-21T10:02:04.126573Z","shell.execute_reply.started":"2023-03-21T10:02:03.992220Z","shell.execute_reply":"2023-03-21T10:02:04.125570Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"((9804, 768), (1214, 768))"},"metadata":{}}]},{"cell_type":"code","source":"from umap import UMAP\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Scale features to [0,1] range\nX_scaled = MinMaxScaler().fit_transform(X_train)\n# Initialize and fit UMAP\nmapper = UMAP(n_components=2, metric=\"cosine\").fit(X_scaled)\n# Create a DataFrame of 2D embeddings\ndf_emb = pd.DataFrame(mapper.embedding_, columns=[\"X\", \"Y\"])\ndf_emb[\"label\"] = y_train\ndf_emb.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:02:04.127938Z","iopub.execute_input":"2023-03-21T10:02:04.128793Z","iopub.status.idle":"2023-03-21T10:02:50.750912Z","shell.execute_reply.started":"2023-03-21T10:02:04.128754Z","shell.execute_reply":"2023-03-21T10:02:50.749800Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"          X         Y  label\n0  0.317551  1.756561      0\n1  8.033961  4.350843      1\n2  7.818738  4.717035      1\n3  6.820696  3.574553      2\n4  6.592365  2.539713      2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X</th>\n      <th>Y</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.317551</td>\n      <td>1.756561</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8.033961</td>\n      <td>4.350843</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.818738</td>\n      <td>4.717035</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6.820696</td>\n      <td>3.574553</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6.592365</td>\n      <td>2.539713</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize=(5,5))\naxes = axes.flatten()\ncmaps = [\"Greys\", \"Blues\", \"Oranges\", \"Reds\", \"Greens\"]\nlabels = health_fact[\"train\"].features[\"label\"].names\n\nfor i, (label, cmap) in enumerate(zip(labels, cmaps)):\n    df_emb_sub = df_emb.query(f\"label == {i}\")\n    axes[i].hexbin(df_emb_sub[\"X\"], df_emb_sub[\"Y\"], cmap=cmap,\n                   gridsize=20, linewidths=(0,))\n    axes[i].set_title(label)\n    axes[i].set_xticks([]), axes[i].set_yticks([])\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:02:50.757782Z","iopub.execute_input":"2023-03-21T10:02:50.758129Z","iopub.status.idle":"2023-03-21T10:02:51.100846Z","shell.execute_reply.started":"2023-03-21T10:02:50.758098Z","shell.execute_reply":"2023-03-21T10:02:51.097396Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 500x500 with 4 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr_clf = LogisticRegression(max_iter=3000)\nlr_clf.fit(X_train, y_train)\nlr_clf.score(X_valid, y_valid)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:02:51.103197Z","iopub.execute_input":"2023-03-21T10:02:51.103603Z","iopub.status.idle":"2023-03-21T10:04:00.821337Z","shell.execute_reply.started":"2023-03-21T10:02:51.103557Z","shell.execute_reply":"2023-03-21T10:04:00.820077Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"0.6845140032948929"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.dummy import DummyClassifier\n\ndummy_clf = DummyClassifier(strategy=\"most_frequent\")\ndummy_clf.fit(X_train, y_train)\ndummy_clf.score(X_valid, y_valid)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:04:00.823464Z","iopub.execute_input":"2023-03-21T10:04:00.824278Z","iopub.status.idle":"2023-03-21T10:04:00.845172Z","shell.execute_reply.started":"2023-03-21T10:04:00.824237Z","shell.execute_reply":"2023-03-21T10:04:00.843080Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"0.5181219110378913"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n\ndef plot_confusion_matrix(y_preds, y_true, labels):\n    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n    fig, ax = plt.subplots(figsize=(6, 6))\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n    plt.title(\"Normalized confusion matrix\")\n    plt.show()\n\ny_preds = lr_clf.predict(X_valid)\nplot_confusion_matrix(y_preds, y_valid, labels)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:04:00.847111Z","iopub.execute_input":"2023-03-21T10:04:00.847873Z","iopub.status.idle":"2023-03-21T10:04:01.136561Z","shell.execute_reply.started":"2023-03-21T10:04:00.847830Z","shell.execute_reply":"2023-03-21T10:04:01.135455Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 600x600 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nnum_labels = 4\nmodel = (AutoModelForSequenceClassification\n         .from_pretrained(model_ckpt, num_labels=num_labels)\n         .to(device))\n","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:04:01.138120Z","iopub.execute_input":"2023-03-21T10:04:01.138685Z","iopub.status.idle":"2023-03-21T10:04:02.857817Z","shell.execute_reply.started":"2023-03-21T10:04:01.138639Z","shell.execute_reply":"2023-03-21T10:04:02.856683Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    f1 = f1_score(labels, preds, average=\"weighted\")\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc, \"f1\": f1}\n","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:04:02.859596Z","iopub.execute_input":"2023-03-21T10:04:02.860373Z","iopub.status.idle":"2023-03-21T10:04:02.866797Z","shell.execute_reply.started":"2023-03-21T10:04:02.860322Z","shell.execute_reply":"2023-03-21T10:04:02.865790Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\nbatch_size = 32\nlogging_steps = len(health_fact_encoded[\"train\"]) // batch_size\nmodel_name = f\"{model_ckpt}-finetuned-health-fact\"\ntraining_args = TrainingArguments(output_dir=model_name,\n                                  num_train_epochs=2,\n                                  learning_rate=2e-5,\n                                  per_device_train_batch_size=batch_size,\n                                  per_device_eval_batch_size=batch_size,\n                                  weight_decay=0.01,\n                                  warmup_steps=100,\n                                  evaluation_strategy=\"epoch\",\n                                  disable_tqdm=False,\n                                  logging_steps=logging_steps,\n                                  push_to_hub=False,\n                                  log_level=\"error\",)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:12:40.941210Z","iopub.execute_input":"2023-03-21T10:12:40.942136Z","iopub.status.idle":"2023-03-21T10:12:40.956450Z","shell.execute_reply.started":"2023-03-21T10:12:40.942083Z","shell.execute_reply":"2023-03-21T10:12:40.955223Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:09:32.515658Z","iopub.execute_input":"2023-03-21T10:09:32.516380Z","iopub.status.idle":"2023-03-21T10:09:33.188496Z","shell.execute_reply.started":"2023-03-21T10:09:32.516333Z","shell.execute_reply":"2023-03-21T10:09:33.187432Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"gpu_usage() ","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:09:34.113230Z","iopub.execute_input":"2023-03-21T10:09:34.113987Z","iopub.status.idle":"2023-03-21T10:09:34.207930Z","shell.execute_reply.started":"2023-03-21T10:09:34.113947Z","shell.execute_reply":"2023-03-21T10:09:34.206738Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n| ID | GPU | MEM |\n------------------\n|  0 |  0% |  7% |\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import Trainer\nfrom transformers import EarlyStoppingCallback\n\nclass CustomEarlyStoppingCallback(EarlyStoppingCallback):\n    def __init__(self, early_stopping_patience=3):\n        super().__init__(early_stopping_patience=early_stopping_patience)\n\nearly_stopping_callback = CustomEarlyStoppingCallback(early_stopping_patience=1)\n        \ntrainer = Trainer(model=model, args=training_args,\n                  compute_metrics=compute_metrics,\n                  train_dataset=health_fact_encoded[\"train\"],\n                  eval_dataset=health_fact_encoded[\"validation\"],\n                  tokenizer=tokenizer,\n                  callbacks=[early_stopping_callback],)\n# trainer.train();","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:10:37.028951Z","iopub.execute_input":"2023-03-21T10:10:37.030138Z","iopub.status.idle":"2023-03-21T10:10:37.044796Z","shell.execute_reply.started":"2023-03-21T10:10:37.030062Z","shell.execute_reply":"2023-03-21T10:10:37.043766Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"import wandb\nwandb.init(project=\"pubhealth-hyperparameter-search\")","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:10:43.261583Z","iopub.execute_input":"2023-03-21T10:10:43.262519Z","iopub.status.idle":"2023-03-21T10:11:29.627291Z","shell.execute_reply.started":"2023-03-21T10:10:43.262464Z","shell.execute_reply":"2023-03-21T10:11:29.626261Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.14.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.13.10"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230321_101057-va73sjls</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/tansaku/pubhealth-hyperparameter-search/runs/va73sjls' target=\"_blank\">summer-sea-10</a></strong> to <a href='https://wandb.ai/tansaku/pubhealth-hyperparameter-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/tansaku/pubhealth-hyperparameter-search' target=\"_blank\">https://wandb.ai/tansaku/pubhealth-hyperparameter-search</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/tansaku/pubhealth-hyperparameter-search/runs/va73sjls' target=\"_blank\">https://wandb.ai/tansaku/pubhealth-hyperparameter-search/runs/va73sjls</a>"},"metadata":{}},{"execution_count":39,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tansaku/pubhealth-hyperparameter-search/runs/va73sjls?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7f0101c0a6d0>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nnum_labels=4\n# Define the model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\n# Function to train and evaluate the model with given hyperparameters\ndef train_evaluate_model(learning_rate, batch_size, weight_decay, warmup_step):\n    model = (AutoModelForSequenceClassification\n         .from_pretrained(model_ckpt, num_labels=num_labels)\n         .to(device))\n\n    training_args = TrainingArguments(\n        output_dir='./results',\n        num_train_epochs=4,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        learning_rate=learning_rate,\n        weight_decay=weight_decay,\n        warmup_steps=warmup_step,\n        logging_dir='./logs',\n        logging_steps=100,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        save_total_limit=1,\n        report_to=\"wandb\",\n        load_best_model_at_end=True,\n        metric_for_best_model=\"accuracy\",\n        greater_is_better=True,\n    )\n\n    trainer = Trainer(\n        model=model, \n        args=training_args,\n        compute_metrics=compute_metrics,\n        train_dataset=health_fact_encoded[\"train\"],\n        eval_dataset=health_fact_encoded[\"validation\"],\n        tokenizer=tokenizer,\n        callbacks=[CustomEarlyStoppingCallback(early_stopping_patience=1)],\n    )\n\n    # Train the model\n    trainer.train()\n    \n    # would be good to run the below up front to check that this actually works ...\n\n    # Evaluate the model on the validation set\n    results = trainer.evaluate(health_fact_encoded[\"validation\"])\n    \n    summary = {\n        \"learning_rate\": learning_rate,\n        \"batch_size\": batch_size,\n        \"weight_decay\": weight_decay,\n        \"warmup_step\": warmup_step,\n        \"accuracy\": results[\"eval_accuracy\"],\n    }\n    print(summary)\n    # Log the hyperparameters and results in wandb\n    wandb.log(summary)\n\n# Perform hyperparameter search\nlearning_rates = [2e-5, 3e-5, 5e-5]\nbatch_sizes = [8, 16]\nweight_decays = [0.0, 0.01]\nwarmup_steps = [0, 100, 200]\n\nfor learning_rate in learning_rates:\n    for batch_size in batch_sizes:\n        for weight_decay in weight_decays:\n            for warmup_step in warmup_steps:\n                train_evaluate_model(learning_rate, batch_size, weight_decay, warmup_step)\n\n\n# Close wandb\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:12:57.064776Z","iopub.execute_input":"2023-03-21T10:12:57.065531Z","iopub.status.idle":"2023-03-21T20:28:01.965027Z","shell.execute_reply.started":"2023-03-21T10:12:57.065491Z","shell.execute_reply":"2023-03-21T20:28:01.964109Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 1.0252, 'learning_rate': 1.959216965742251e-05, 'epoch': 0.08}\n{'loss': 0.8932, 'learning_rate': 1.9184339314845025e-05, 'epoch': 0.16}\n{'loss': 0.801, 'learning_rate': 1.877650897226754e-05, 'epoch': 0.24}\n{'loss': 0.7737, 'learning_rate': 1.8368678629690052e-05, 'epoch': 0.33}\n{'loss': 0.7832, 'learning_rate': 1.7960848287112562e-05, 'epoch': 0.41}\n{'loss': 0.7944, 'learning_rate': 1.7553017944535075e-05, 'epoch': 0.49}\n{'loss': 0.7521, 'learning_rate': 1.714518760195759e-05, 'epoch': 0.57}\n{'loss': 0.7588, 'learning_rate': 1.67373572593801e-05, 'epoch': 0.65}\n{'loss': 0.7484, 'learning_rate': 1.6329526916802612e-05, 'epoch': 0.73}\n{'loss': 0.7169, 'learning_rate': 1.5921696574225122e-05, 'epoch': 0.82}\n{'loss': 0.6943, 'learning_rate': 1.5513866231647635e-05, 'epoch': 0.9}\n{'loss': 0.6838, 'learning_rate': 1.5106035889070147e-05, 'epoch': 0.98}\n","output_type":"stream"},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\n","output_type":"stream"},{"name":"stdout","text":"{'eval_loss': 0.7055772542953491, 'eval_accuracy': 0.71334431630972, 'eval_f1': 0.6903688677860969, 'eval_runtime': 11.1119, 'eval_samples_per_second': 109.252, 'eval_steps_per_second': 13.679, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Model weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.6305, 'learning_rate': 1.469820554649266e-05, 'epoch': 1.06}\n{'loss': 0.6263, 'learning_rate': 1.4290375203915172e-05, 'epoch': 1.14}\n{'loss': 0.5928, 'learning_rate': 1.3882544861337685e-05, 'epoch': 1.22}\n{'loss': 0.6054, 'learning_rate': 1.3474714518760197e-05, 'epoch': 1.31}\n{'loss': 0.6606, 'learning_rate': 1.3066884176182709e-05, 'epoch': 1.39}\n{'loss': 0.5922, 'learning_rate': 1.2659053833605222e-05, 'epoch': 1.47}\n{'loss': 0.6103, 'learning_rate': 1.2251223491027732e-05, 'epoch': 1.55}\n{'loss': 0.57, 'learning_rate': 1.1843393148450246e-05, 'epoch': 1.63}\n{'loss': 0.6311, 'learning_rate': 1.1435562805872757e-05, 'epoch': 1.71}\n{'loss': 0.5665, 'learning_rate': 1.102773246329527e-05, 'epoch': 1.79}\n{'loss': 0.5433, 'learning_rate': 1.0619902120717782e-05, 'epoch': 1.88}\n{'loss': 0.6221, 'learning_rate': 1.0212071778140294e-05, 'epoch': 1.96}\n","output_type":"stream"},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\n","output_type":"stream"},{"name":"stdout","text":"{'eval_loss': 0.6940823197364807, 'eval_accuracy': 0.7397034596375618, 'eval_f1': 0.724394130116743, 'eval_runtime': 11.1378, 'eval_samples_per_second': 108.999, 'eval_steps_per_second': 13.647, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"Model weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.5241, 'learning_rate': 9.804241435562807e-06, 'epoch': 2.04}\n{'loss': 0.4754, 'learning_rate': 9.396411092985319e-06, 'epoch': 2.12}\n{'loss': 0.4245, 'learning_rate': 8.98858075040783e-06, 'epoch': 2.2}\n{'loss': 0.4892, 'learning_rate': 8.580750407830342e-06, 'epoch': 2.28}\n{'loss': 0.4707, 'learning_rate': 8.172920065252856e-06, 'epoch': 2.37}\n{'loss': 0.4692, 'learning_rate': 7.765089722675368e-06, 'epoch': 2.45}\n{'loss': 0.4223, 'learning_rate': 7.35725938009788e-06, 'epoch': 2.53}\n{'loss': 0.4077, 'learning_rate': 6.949429037520392e-06, 'epoch': 2.61}\n{'loss': 0.4449, 'learning_rate': 6.541598694942904e-06, 'epoch': 2.69}\n{'loss': 0.4781, 'learning_rate': 6.133768352365417e-06, 'epoch': 2.77}\n{'loss': 0.4735, 'learning_rate': 5.725938009787929e-06, 'epoch': 2.85}\n{'loss': 0.3973, 'learning_rate': 5.31810766721044e-06, 'epoch': 2.94}\n","output_type":"stream"},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\n","output_type":"stream"},{"name":"stdout","text":"{'eval_loss': 0.7572960257530212, 'eval_accuracy': 0.7397034596375618, 'eval_f1': 0.735014788109258, 'eval_runtime': 11.0975, 'eval_samples_per_second': 109.394, 'eval_steps_per_second': 13.697, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"Model weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7397034596375618).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\n","output_type":"stream"},{"name":"stdout","text":"{'train_runtime': 863.3584, 'train_samples_per_second': 45.423, 'train_steps_per_second': 5.68, 'train_loss': 0.6134814085553301, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"{'eval_loss': 0.6940823197364807, 'eval_accuracy': 0.7397034596375618, 'eval_f1': 0.724394130116743, 'eval_runtime': 11.1364, 'eval_samples_per_second': 109.011, 'eval_steps_per_second': 13.649, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3678' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3678/4904 14:39 < 04:53, 4.18 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.702300</td>\n      <td>0.695786</td>\n      <td>0.712521</td>\n      <td>0.688563</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.616100</td>\n      <td>0.680612</td>\n      <td>0.744646</td>\n      <td>0.729268</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.405800</td>\n      <td>0.747778</td>\n      <td>0.742175</td>\n      <td>0.737500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7446457990115322).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3678' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3678/4904 14:41 < 04:53, 4.17 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.698400</td>\n      <td>0.695706</td>\n      <td>0.711697</td>\n      <td>0.686455</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.619600</td>\n      <td>0.669680</td>\n      <td>0.750412</td>\n      <td>0.737101</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.410500</td>\n      <td>0.737341</td>\n      <td>0.747941</td>\n      <td>0.744122</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7504118616144976).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3678' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3678/4904 14:42 < 04:54, 4.17 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.692100</td>\n      <td>0.694163</td>\n      <td>0.712521</td>\n      <td>0.688047</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.608800</td>\n      <td>0.687121</td>\n      <td>0.745470</td>\n      <td>0.732108</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.394100</td>\n      <td>0.748222</td>\n      <td>0.745470</td>\n      <td>0.745778</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7454695222405272).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3678' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3678/4904 14:42 < 04:54, 4.17 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.703100</td>\n      <td>0.695265</td>\n      <td>0.712521</td>\n      <td>0.688829</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.616300</td>\n      <td>0.682636</td>\n      <td>0.745470</td>\n      <td>0.731350</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.405400</td>\n      <td>0.751938</td>\n      <td>0.744646</td>\n      <td>0.739652</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7454695222405272).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3678' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3678/4904 14:43 < 04:54, 4.16 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.698600</td>\n      <td>0.695440</td>\n      <td>0.716639</td>\n      <td>0.692249</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.620200</td>\n      <td>0.669181</td>\n      <td>0.750412</td>\n      <td>0.736323</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.408800</td>\n      <td>0.737615</td>\n      <td>0.747941</td>\n      <td>0.744051</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7504118616144976).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1226' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1226/2452 09:09 < 09:10, 2.23 it/s, Epoch 2/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.695400</td>\n      <td>0.677611</td>\n      <td>0.726524</td>\n      <td>0.713957</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.593400</td>\n      <td>0.674695</td>\n      <td>0.724876</td>\n      <td>0.704755</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-613 (score: 0.7265238879736409).\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.697800</td>\n      <td>0.677509</td>\n      <td>0.723229</td>\n      <td>0.713332</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.595200</td>\n      <td>0.665070</td>\n      <td>0.737232</td>\n      <td>0.718229</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.476000</td>\n      <td>0.674750</td>\n      <td>0.742998</td>\n      <td>0.738985</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.395300</td>\n      <td>0.725540</td>\n      <td>0.739703</td>\n      <td>0.731357</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.742998352553542).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:20, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.703700</td>\n      <td>0.680429</td>\n      <td>0.727348</td>\n      <td>0.714810</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.598200</td>\n      <td>0.665102</td>\n      <td>0.737232</td>\n      <td>0.717386</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.468500</td>\n      <td>0.668433</td>\n      <td>0.741351</td>\n      <td>0.738134</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.378600</td>\n      <td>0.720160</td>\n      <td>0.741351</td>\n      <td>0.735116</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.7413509060955519).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.694500</td>\n      <td>0.677555</td>\n      <td>0.726524</td>\n      <td>0.714023</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.587900</td>\n      <td>0.674347</td>\n      <td>0.734761</td>\n      <td>0.714436</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.467100</td>\n      <td>0.669087</td>\n      <td>0.749588</td>\n      <td>0.744347</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.388400</td>\n      <td>0.716362</td>\n      <td>0.748764</td>\n      <td>0.740981</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.7495881383855024).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.696900</td>\n      <td>0.674794</td>\n      <td>0.725700</td>\n      <td>0.713512</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.589600</td>\n      <td>0.672211</td>\n      <td>0.733937</td>\n      <td>0.714470</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.466000</td>\n      <td>0.669973</td>\n      <td>0.747941</td>\n      <td>0.744986</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.380900</td>\n      <td>0.711991</td>\n      <td>0.754530</td>\n      <td>0.748496</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7545304777594728).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.703200</td>\n      <td>0.679749</td>\n      <td>0.725700</td>\n      <td>0.712406</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.597800</td>\n      <td>0.665815</td>\n      <td>0.734761</td>\n      <td>0.714839</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.469200</td>\n      <td>0.670259</td>\n      <td>0.742998</td>\n      <td>0.739851</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.378600</td>\n      <td>0.721529</td>\n      <td>0.741351</td>\n      <td>0.735104</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.742998352553542).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:35, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.702500</td>\n      <td>0.681688</td>\n      <td>0.723229</td>\n      <td>0.696683</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.577900</td>\n      <td>0.728081</td>\n      <td>0.734761</td>\n      <td>0.716302</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.322500</td>\n      <td>0.861972</td>\n      <td>0.738880</td>\n      <td>0.735830</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.300600</td>\n      <td>1.084361</td>\n      <td>0.736409</td>\n      <td>0.733986</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-3678 (score: 0.7388797364085667).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3678' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3678/4904 14:43 < 04:54, 4.16 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.702500</td>\n      <td>0.679674</td>\n      <td>0.722405</td>\n      <td>0.697467</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.597700</td>\n      <td>0.683762</td>\n      <td>0.744646</td>\n      <td>0.729278</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.356100</td>\n      <td>0.835116</td>\n      <td>0.736409</td>\n      <td>0.732184</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7446457990115322).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:39, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.714200</td>\n      <td>0.693460</td>\n      <td>0.715815</td>\n      <td>0.688587</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.611000</td>\n      <td>0.705205</td>\n      <td>0.733114</td>\n      <td>0.717944</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.368500</td>\n      <td>0.779808</td>\n      <td>0.733937</td>\n      <td>0.728725</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.307300</td>\n      <td>0.966724</td>\n      <td>0.738056</td>\n      <td>0.732295</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-4904 (score: 0.7380560131795717).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:41, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.702800</td>\n      <td>0.678670</td>\n      <td>0.726524</td>\n      <td>0.700565</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.574800</td>\n      <td>0.699367</td>\n      <td>0.736409</td>\n      <td>0.720355</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.327300</td>\n      <td>0.843683</td>\n      <td>0.742998</td>\n      <td>0.739571</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.304100</td>\n      <td>1.053005</td>\n      <td>0.743822</td>\n      <td>0.740185</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-4904 (score: 0.7438220757825371).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:41, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.704500</td>\n      <td>0.681482</td>\n      <td>0.716639</td>\n      <td>0.691288</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.600900</td>\n      <td>0.692552</td>\n      <td>0.739703</td>\n      <td>0.724360</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.357300</td>\n      <td>0.821473</td>\n      <td>0.740527</td>\n      <td>0.736732</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.277900</td>\n      <td>1.027645</td>\n      <td>0.740527</td>\n      <td>0.738213</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-3678 (score: 0.7405271828665568).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:41, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.703000</td>\n      <td>0.692957</td>\n      <td>0.708402</td>\n      <td>0.680576</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.605900</td>\n      <td>0.690434</td>\n      <td>0.734761</td>\n      <td>0.718889</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.354000</td>\n      <td>0.809770</td>\n      <td>0.741351</td>\n      <td>0.736511</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.312200</td>\n      <td>1.023119</td>\n      <td>0.742998</td>\n      <td>0.739305</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-4904 (score: 0.742998352553542).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1839' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1839/2452 13:46 < 04:35, 2.22 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.692800</td>\n      <td>0.666872</td>\n      <td>0.728995</td>\n      <td>0.718880</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.565600</td>\n      <td>0.668586</td>\n      <td>0.747941</td>\n      <td>0.731736</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.404400</td>\n      <td>0.705550</td>\n      <td>0.740527</td>\n      <td>0.738255</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1226 (score: 0.7479406919275123).\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1226' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1226/2452 09:10 < 09:11, 2.22 it/s, Epoch 2/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.693900</td>\n      <td>0.676170</td>\n      <td>0.728171</td>\n      <td>0.718216</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.576900</td>\n      <td>0.676756</td>\n      <td>0.727348</td>\n      <td>0.710966</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-613 (score: 0.728171334431631).\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1839' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1839/2452 13:45 < 04:35, 2.23 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.690600</td>\n      <td>0.679991</td>\n      <td>0.724053</td>\n      <td>0.712898</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.584600</td>\n      <td>0.659111</td>\n      <td>0.744646</td>\n      <td>0.728230</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.417000</td>\n      <td>0.717380</td>\n      <td>0.738056</td>\n      <td>0.732364</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1226 (score: 0.7446457990115322).\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.687400</td>\n      <td>0.679228</td>\n      <td>0.726524</td>\n      <td>0.710899</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.566300</td>\n      <td>0.667801</td>\n      <td>0.735585</td>\n      <td>0.717613</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.409200</td>\n      <td>0.706129</td>\n      <td>0.738056</td>\n      <td>0.734821</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.300500</td>\n      <td>0.779258</td>\n      <td>0.742998</td>\n      <td>0.737677</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.742998352553542).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:22, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.697300</td>\n      <td>0.667620</td>\n      <td>0.730643</td>\n      <td>0.716925</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.570000</td>\n      <td>0.665627</td>\n      <td>0.734761</td>\n      <td>0.719291</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.407400</td>\n      <td>0.696674</td>\n      <td>0.738056</td>\n      <td>0.738038</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.301000</td>\n      <td>0.788392</td>\n      <td>0.736409</td>\n      <td>0.730842</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.7380560131795717).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1839' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1839/2452 13:46 < 04:35, 2.22 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.696800</td>\n      <td>0.665594</td>\n      <td>0.731466</td>\n      <td>0.715351</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.577300</td>\n      <td>0.667241</td>\n      <td>0.739703</td>\n      <td>0.721338</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.399200</td>\n      <td>0.691265</td>\n      <td>0.738056</td>\n      <td>0.736906</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1226 (score: 0.7397034596375618).\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3678' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3678/4904 14:43 < 04:54, 4.16 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.737600</td>\n      <td>0.689678</td>\n      <td>0.716639</td>\n      <td>0.693725</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.609600</td>\n      <td>0.725373</td>\n      <td>0.738880</td>\n      <td>0.724048</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.367900</td>\n      <td>0.893925</td>\n      <td>0.735585</td>\n      <td>0.725919</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7388797364085667).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:40, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.744600</td>\n      <td>0.707010</td>\n      <td>0.718287</td>\n      <td>0.689123</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.609200</td>\n      <td>0.738522</td>\n      <td>0.742175</td>\n      <td>0.725627</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.305400</td>\n      <td>0.837749</td>\n      <td>0.752059</td>\n      <td>0.748631</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.230400</td>\n      <td>1.150624</td>\n      <td>0.752059</td>\n      <td>0.747039</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-3678 (score: 0.7520593080724877).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:36, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.710100</td>\n      <td>0.729627</td>\n      <td>0.700165</td>\n      <td>0.668853</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.602600</td>\n      <td>0.698634</td>\n      <td>0.736409</td>\n      <td>0.721260</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.353300</td>\n      <td>0.903453</td>\n      <td>0.744646</td>\n      <td>0.732206</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.244000</td>\n      <td>1.150242</td>\n      <td>0.736409</td>\n      <td>0.732604</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-3678 (score: 0.7446457990115322).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3678' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3678/4904 14:43 < 04:54, 4.16 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.726500</td>\n      <td>0.686998</td>\n      <td>0.720758</td>\n      <td>0.692170</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.575700</td>\n      <td>0.684614</td>\n      <td>0.751236</td>\n      <td>0.738765</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.331000</td>\n      <td>0.973908</td>\n      <td>0.747117</td>\n      <td>0.732642</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7512355848434926).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:38, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.727000</td>\n      <td>0.712084</td>\n      <td>0.713344</td>\n      <td>0.683962</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.638200</td>\n      <td>0.695704</td>\n      <td>0.737232</td>\n      <td>0.723807</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.324400</td>\n      <td>0.869525</td>\n      <td>0.745470</td>\n      <td>0.735129</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.254900</td>\n      <td>1.126469</td>\n      <td>0.740527</td>\n      <td>0.738411</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-3678 (score: 0.7454695222405272).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:37, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.727000</td>\n      <td>0.715475</td>\n      <td>0.703460</td>\n      <td>0.672203</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.583200</td>\n      <td>0.700296</td>\n      <td>0.742998</td>\n      <td>0.730058</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.364000</td>\n      <td>0.871863</td>\n      <td>0.750412</td>\n      <td>0.744476</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.284500</td>\n      <td>1.180633</td>\n      <td>0.733114</td>\n      <td>0.728366</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-3678 (score: 0.7504118616144976).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.689000</td>\n      <td>0.671063</td>\n      <td>0.724053</td>\n      <td>0.708760</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.553400</td>\n      <td>0.678851</td>\n      <td>0.731466</td>\n      <td>0.712602</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.343100</td>\n      <td>0.755990</td>\n      <td>0.746293</td>\n      <td>0.744534</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.224000</td>\n      <td>0.919033</td>\n      <td>0.738056</td>\n      <td>0.734555</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.7462932454695222).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.702000</td>\n      <td>0.673133</td>\n      <td>0.728171</td>\n      <td>0.719515</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.552500</td>\n      <td>0.697145</td>\n      <td>0.732290</td>\n      <td>0.713608</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.347100</td>\n      <td>0.756907</td>\n      <td>0.739703</td>\n      <td>0.736556</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.236300</td>\n      <td>0.920691</td>\n      <td>0.734761</td>\n      <td>0.731255</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.7397034596375618).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.706100</td>\n      <td>0.675854</td>\n      <td>0.715815</td>\n      <td>0.695075</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.576000</td>\n      <td>0.689090</td>\n      <td>0.721582</td>\n      <td>0.699489</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.355100</td>\n      <td>0.749004</td>\n      <td>0.738056</td>\n      <td>0.733153</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.237000</td>\n      <td>0.900966</td>\n      <td>0.753707</td>\n      <td>0.750294</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7537067545304778).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.688500</td>\n      <td>0.665402</td>\n      <td>0.723229</td>\n      <td>0.710728</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.546900</td>\n      <td>0.690440</td>\n      <td>0.730643</td>\n      <td>0.710562</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.334300</td>\n      <td>0.757644</td>\n      <td>0.740527</td>\n      <td>0.736749</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.225900</td>\n      <td>0.921424</td>\n      <td>0.741351</td>\n      <td>0.737079</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7413509060955519).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:17, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.703200</td>\n      <td>0.669617</td>\n      <td>0.730643</td>\n      <td>0.720140</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.557500</td>\n      <td>0.692083</td>\n      <td>0.733114</td>\n      <td>0.717699</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.359500</td>\n      <td>0.753943</td>\n      <td>0.741351</td>\n      <td>0.740165</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.229700</td>\n      <td>0.945377</td>\n      <td>0.734761</td>\n      <td>0.729591</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.7413509060955519).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:16, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.700100</td>\n      <td>0.671288</td>\n      <td>0.722405</td>\n      <td>0.705562</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.575300</td>\n      <td>0.677237</td>\n      <td>0.738056</td>\n      <td>0.721710</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.348400</td>\n      <td>0.752762</td>\n      <td>0.749588</td>\n      <td>0.745155</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.228200</td>\n      <td>0.919576</td>\n      <td>0.740527</td>\n      <td>0.738035</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.7495881383855024).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▄▆▇▆▆▇▁▅▅▇█▅▄▆▄▅▅▅▆▁▆▅▄▄▄▇▆▇▆▇▆▄█▅▅▇</td></tr><tr><td>batch_size</td><td>▁▁▁▁▁▁██████▁▁▁▁▁▁██████▁▁▁▁▁▁██████</td></tr><tr><td>eval/accuracy</td><td>▆▇▇▇▇▇▄▆▆▄▇█▆▆▆▅▆▃▆▆▆▄▇▆▆▆▆█▇█▆▁▇▆▆▃▄▆▅▇</td></tr><tr><td>eval/f1</td><td>▆▆▇▆▆▇▄▇▅▅██▇▇▇▆▅▃▇▇▇▅▆▇▇▆▆█▇▇▆▁█▇▇▄▅▇▆█</td></tr><tr><td>eval/loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▁▇▄▃▂▁▃▆▂▁▁▃▂▁▂█▄▁▂▂▄▅▂▁▁▅▅▂</td></tr><tr><td>eval/runtime</td><td>▄▇▇▇▇▇▂▂▂▂▂▂▂▇▇█▇▇█▇▂▂▁▂▂▂█▇▇▇▆▆▇▂▂▃▂▁▁▂</td></tr><tr><td>eval/samples_per_second</td><td>▅▂▂▂▂▂▇▆▇▇▇▇▇▂▂▁▂▂▁▂▇▇█▇▇▇▁▂▂▂▂▃▂▇▇▆▇██▇</td></tr><tr><td>eval/steps_per_second</td><td>██████▁▁▁▁▁▁▁███████▁▁▁▁▁▁▇██████▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▃▃▃▃████████████</td></tr><tr><td>train/epoch</td><td>▃▂▆▅▄▂▁▆███▁▁▆▃▁█▅▂▆▃▇▁▅▄▄▄▃▇▄█▅▆▃▇▇██▁▆</td></tr><tr><td>train/global_step</td><td>▃▁▆▄▃▂▃▁▃▃▃▃▂▆▃▁▆▅▁▆▃▇▃▁▁▁▃▁█▅▁▆▄▁█▄▄▄▄▄</td></tr><tr><td>train/learning_rate</td><td>▃▂▂▂▃▃▂▂▂▁▃▃▅▂▄▄▁▄▁▃▄▁▄▃▅▄▅▅▃▆█▃▄▇▄▄▂▇▅▂</td></tr><tr><td>train/loss</td><td>▆▄▄▆▆▆▄▅▆▄█▆█▃▅▆▂▆▂▄▇▂▆▄█▆▅▆▃▅█▃▃▆▃▄▁▇▄▁</td></tr><tr><td>train/total_flos</td><td>▅▅▅▅▅▅▁██████▅████▅▁▅██▅▅██▅████████</td></tr><tr><td>train/train_loss</td><td>▅▅▆▅▅▆█▄▄▄▄▄▁▄▂▁▂▂▅█▅▂▃▅▄▂▁▄▂▁▁▂▂▁▂▂</td></tr><tr><td>train/train_runtime</td><td>▄▅▅▅▅▅▁▇▇▇▇▇█▅████▄▁▄▇▇▄▅██▅██▇▇▇▇▇▇</td></tr><tr><td>train/train_samples_per_second</td><td>▃▃▃▃▃▃█▁▁▁▁▁▁▃▁▁▁▁▄█▄▁▁▄▃▁▁▃▁▁▁▁▁▁▁▁</td></tr><tr><td>train/train_steps_per_second</td><td>██████▆▁▁▁▁▁▅█▅▅▅▅▃▆▃▁▁▂█▅▅█▅▅▁▁▁▁▁▁</td></tr><tr><td>warmup_step</td><td>▁▅█▁▅█▁▅█▁▅█▁▅█▁▅█▁▅█▁▅█▁▅█▁▅█▁▅█▁▅█</td></tr><tr><td>weight_decay</td><td>▁▁▁███▁▁▁███▁▁▁███▁▁▁███▁▁▁███▁▁▁███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.74959</td></tr><tr><td>batch_size</td><td>16</td></tr><tr><td>eval/accuracy</td><td>0.74959</td></tr><tr><td>eval/f1</td><td>0.74515</td></tr><tr><td>eval/loss</td><td>0.75276</td></tr><tr><td>eval/runtime</td><td>10.9199</td></tr><tr><td>eval/samples_per_second</td><td>111.173</td></tr><tr><td>eval/steps_per_second</td><td>6.96</td></tr><tr><td>learning_rate</td><td>5e-05</td></tr><tr><td>train/epoch</td><td>4.0</td></tr><tr><td>train/global_step</td><td>2452</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.2282</td></tr><tr><td>train/total_flos</td><td>5195026790940672.0</td></tr><tr><td>train/train_loss</td><td>0.50441</td></tr><tr><td>train/train_runtime</td><td>1096.4899</td></tr><tr><td>train/train_samples_per_second</td><td>35.765</td></tr><tr><td>train/train_steps_per_second</td><td>2.236</td></tr><tr><td>warmup_step</td><td>200</td></tr><tr><td>weight_decay</td><td>0.01</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">summer-sea-10</strong> at: <a href='https://wandb.ai/tansaku/pubhealth-hyperparameter-search/runs/va73sjls' target=\"_blank\">https://wandb.ai/tansaku/pubhealth-hyperparameter-search/runs/va73sjls</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20230321_101057-va73sjls/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"# from transformers import AutoModelForSequenceClassification\n# from transformers import Trainer, TrainingArguments\n# num_labels=4\n# # Define the model and tokenizer\n# tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\n# def main(verbose=False):\n#     # Initialise run\n#     run = wandb.init()\n#     model = (AutoModelForSequenceClassification\n#          .from_pretrained(model_ckpt, num_labels=num_labels)\n#          .to(device))\n\n#     training_args = TrainingArguments(\n#         output_dir='./results',\n#         num_train_epochs=4,\n#         per_device_train_batch_size=wandb.config.batch_size,\n#         per_device_eval_batch_size=wandb.config.batch_size,\n#         learning_rate=wandb.config.learning_rate,\n#         weight_decay=wandb.config.weight_decay,\n#         dropout_rate=wandb.config.dropout_rate,\n#         logging_dir='./logs',\n#         logging_steps=100,\n#         evaluation_strategy=\"epoch\",\n#         save_strategy=\"epoch\",\n#         save_total_limit=1,\n#         report_to=\"wandb\",\n#         load_best_model_at_end=True,\n#         metric_for_best_model=\"accuracy\",\n#         greater_is_better=True,\n#         patience=1,  # Number of epochs with no improvement after which training will be stopped\n#     )\n\n#     trainer = Trainer(\n#         model=model, \n#         args=training_args,\n#         compute_metrics=compute_metrics,\n#         train_dataset=health_fact_encoded[\"train\"],\n#         eval_dataset=health_fact_encoded[\"validation\"],\n#         tokenizer=tokenizer\n#     )\n\n#     # Train the model\n#     trainer.train()\n\n#     # Evaluate the model on the validation set\n#     results = trainer.evaluate(health_fact_encoded[\"validation\"])\n\n#     # Log the hyperparameters and results in wandb\n#     wandb.log({\n#         \"learning_rate\": learning_rate,\n#         \"batch_size\": batch_size,\n#         \"weight_decay\": weight_decay,\n#         \"warmup_step\": warmup_step,\n#         \"accuracy\": results[\"eval_accuracy\"],\n#     })","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:04:03.538122Z","iopub.status.idle":"2023-03-21T10:04:03.538893Z","shell.execute_reply.started":"2023-03-21T10:04:03.538621Z","shell.execute_reply":"2023-03-21T10:04:03.538648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sweep_configuration = {\n#     'method': 'bayes',  # random, grid or bayes\n#     'name': 'sweep-bayes',\n#     'metric': {'goal': 'maximize', 'name': 'eval_accuracy'},\n#     'parameters': \n#     {\n#         'batch_size': {'values': [8, 16, 32]},\n#         'learning_rate': {'max': 0.1, 'min': 0.0001},\n#         'weight_decay': {'values': [0.0, 0.1, 0.2]},\n#         'dropout_rate': {'max': 0.5, 'min': 0.1}\n#      }\n# }","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:04:03.540377Z","iopub.status.idle":"2023-03-21T10:04:03.541673Z","shell.execute_reply.started":"2023-03-21T10:04:03.541148Z","shell.execute_reply":"2023-03-21T10:04:03.541175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sweep_id = wandb.sweep(sweep=sweep_configuration, entity='tansaku', project='pubhealth-hyperparameter-search')","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:04:03.543056Z","iopub.status.idle":"2023-03-21T10:04:03.543814Z","shell.execute_reply.started":"2023-03-21T10:04:03.543547Z","shell.execute_reply":"2023-03-21T10:04:03.543573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Maximum 'count' runs\n# wandb.agent(sweep_id, function=main, count=30)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:04:03.545185Z","iopub.status.idle":"2023-03-21T10:04:03.545949Z","shell.execute_reply.started":"2023-03-21T10:04:03.545684Z","shell.execute_reply":"2023-03-21T10:04:03.545711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}