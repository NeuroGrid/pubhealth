{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"    !pip install datasets transformers evaluate imbalanced-learn umap-learn wandb GPUtil","metadata":{"execution":{"iopub.status.busy":"2023-03-23T14:40:17.973754Z","iopub.execute_input":"2023-03-23T14:40:17.974116Z","iopub.status.idle":"2023-03-23T14:40:34.879889Z","shell.execute_reply.started":"2023-03-23T14:40:17.974083Z","shell.execute_reply":"2023-03-23T14:40:34.878653Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (2.1.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.26.1)\nCollecting evaluate\n  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: imbalanced-learn in /opt/conda/lib/python3.7/site-packages (0.10.1)\nRequirement already satisfied: umap-learn in /opt/conda/lib/python3.7/site-packages (0.5.3)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.7/site-packages (0.13.10)\nCollecting GPUtil\n  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.28.2)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.64.1)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (5.0.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.11.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.21.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.3)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2023.1.0)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.12.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (23.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: scikit-learn>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn) (1.0.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn) (1.7.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn) (3.1.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn) (1.2.0)\nRequirement already satisfied: pynndescent>=0.5 in /opt/conda/lib/python3.7/site-packages (from umap-learn) (0.5.8)\nRequirement already satisfied: numba>=0.49 in /opt/conda/lib/python3.7/site-packages (from umap-learn) (0.56.4)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.1.30)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.7/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.15.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from wandb) (59.8.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from wandb) (4.4.0)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (8.1.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.8.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (22.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /opt/conda/lib/python3.7/site-packages (from numba>=0.49->umap-learn) (0.39.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.11.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2022.7.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\nBuilding wheels for collected packages: GPUtil\n  Building wheel for GPUtil (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7409 sha256=bc5eee260ac90fd5cd0f350d368676a1890d406dfd3fe2acee0eafc471c91765\n  Stored in directory: /root/.cache/pip/wheels/b1/e7/99/2b32600270cf23194c9860f029d3d5db075f250bc39028c045\nSuccessfully built GPUtil\nInstalling collected packages: GPUtil, evaluate\nSuccessfully installed GPUtil-1.4.0 evaluate-0.4.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Tuning a Longformer model on a Custom Feature","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-23T17:05:18.034997Z","iopub.execute_input":"2023-03-23T17:05:18.035437Z","iopub.status.idle":"2023-03-23T17:05:18.045492Z","shell.execute_reply.started":"2023-03-23T17:05:18.035399Z","shell.execute_reply":"2023-03-23T17:05:18.044264Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\nhealth_fact = load_dataset('health_fact')","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:14:45.327849Z","iopub.execute_input":"2023-03-23T17:14:45.328530Z","iopub.status.idle":"2023-03-23T17:14:45.954983Z","shell.execute_reply.started":"2023-03-23T17:14:45.328486Z","shell.execute_reply":"2023-03-23T17:14:45.953849Z"},"trusted":true},"execution_count":66,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"876027a5406a475ba76014bfdd4f18b7"}},"metadata":{}}]},{"cell_type":"code","source":"# Filter out instances with a -1 label\nhealth_fact['train'] = health_fact['train'].filter(lambda x: x['label'] != -1)\nhealth_fact['validation'] = health_fact['validation'].filter(lambda x: x['label'] != -1)\nhealth_fact['test'] = health_fact['test'].filter(lambda x: x['label'] != -1)","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:14:50.597952Z","iopub.execute_input":"2023-03-23T17:14:50.598751Z","iopub.status.idle":"2023-03-23T17:14:51.358774Z","shell.execute_reply.started":"2023-03-23T17:14:50.598705Z","shell.execute_reply":"2023-03-23T17:14:51.357384Z"},"trusted":true},"execution_count":67,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47072f6b11b7408d8862f21817b2e39d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a4da72d595c457f9b782f3c0e4266e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a56603a866944fb69baf63cd6794cc53"}},"metadata":{}}]},{"cell_type":"code","source":"def label_int2str(row):\n    if row == -1:\n        return 'invalid'\n    return health_fact[\"train\"].features[\"label\"].int2str(row)","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:18:43.560480Z","iopub.execute_input":"2023-03-23T17:18:43.561354Z","iopub.status.idle":"2023-03-23T17:18:43.573912Z","shell.execute_reply.started":"2023-03-23T17:18:43.561297Z","shell.execute_reply":"2023-03-23T17:18:43.572636Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# model_ckpt = 'distilbert-base-uncased'\nmodel_ckpt = 'allenai/longformer-base-4096'\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)","metadata":{"execution":{"iopub.status.busy":"2023-03-23T14:41:43.374153Z","iopub.execute_input":"2023-03-23T14:41:43.375193Z","iopub.status.idle":"2023-03-23T14:41:53.068982Z","shell.execute_reply.started":"2023-03-23T14:41:43.375151Z","shell.execute_reply":"2023-03-23T14:41:53.067751Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5424c5fdfc804deb98592cccc70a1628"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a441ead4aeb4433a902a7e1da0e3639"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3b15a5c2d794d1abaf16a8a5cfa1c06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b94db8dd3cf0417b91339c01d102d9c9"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Constructing the Custom Feature","metadata":{}},{"cell_type":"code","source":"def new_column(example):\n    example[\"summary\"] = f\"SUBJECTS: {example['subjects']} - CLAIM: {example['claim']} - EXPLANATION: {example['explanation']}\"\n    return example\n\nhealth_fact = health_fact.map(new_column)\nhealth_fact['train'][0]","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:18:52.360117Z","iopub.execute_input":"2023-03-23T17:18:52.361381Z","iopub.status.idle":"2023-03-23T17:18:54.745154Z","shell.execute_reply.started":"2023-03-23T17:18:52.361325Z","shell.execute_reply":"2023-03-23T17:18:54.743554Z"},"trusted":true},"execution_count":73,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9804 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54b32679c19f4e5e897963d29f18097a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1233 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce254da233344fcebe5bd667a20b475f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1214 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fcfab801d904386922874a06d7161e0"}},"metadata":{}},{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"{'claim_id': '15661',\n 'claim': '\"The money the Clinton Foundation took from from foreign governments while Hillary Clinton was secretary of state \"\"is clearly illegal. … The Constitution says you can’t take this stuff.\"',\n 'date_published': 'April 26, 2015',\n 'explanation': '\"Gingrich said the Clinton Foundation \"\"took money from from foreign governments while (Hillary Clinton) was secretary of state. It is clearly illegal. … The Constitution says you can’t take this stuff.\"\" A clause in the Constitution does prohibit U.S. officials such as former Secretary of State Hillary Clinton from receiving gifts, or emoluments, from foreign governments. But the gifts in this case were donations from foreign governments that went to the Clinton Foundation, not Hillary Clinton. She was not part of the foundation her husband founded while she was secretary of state. Does that violate the Constitution? Some libertarian-minded constitutional law experts say it very well could. Others are skeptical. What’s clear is there is room for ambiguity, and the donations are anything but \"\"clearly illegal.\"\" The reality is this a hazy part of U.S. constitutional\\xa0law. ',\n 'fact_checkers': 'Katie Sanders',\n 'main_text': '\"Hillary Clinton is in the political crosshairs as the author of a new book alleges improper financial ties between her public and personal life. At issue in conservative author Peter Schweizer’s forthcoming book Clinton Cash are donations from foreign governments to the Clinton Foundation during the four years she served as secretary of state. George Stephanopoulos used an interview with Schweizer on ABC This Week to point out what other nonpartisan journalists have found: There is no \"\"smoking gun\"\" showing that donations to the foundation influenced her foreign policy decisions. Still, former Republican House Speaker Newt Gingrich says the donations are \"\"clearly illegal\"\" under federal law. In his view, a donation by a foreign government to the Clinton Foundation while Clinton was secretary of state is the same as money sent directly to her, he said, even though she did not join the foundation’s board until she left her post. \"\"The Constitution of the United States says you cannot take money from foreign governments without explicit permission of the Congress. They wrote that in there because they knew the danger of corrupting our system by foreign money is enormous,\"\" Gingrich said. \"\"You had a sitting secretary of state whose husband radically increased his speech fees, you have a whole series of dots on the wall now where people gave millions of dollars — oh, by the way, they happen to get taken care of by the State Department.\"\" He continued, \"\"My point is they took money from foreign governments while she was secretary of State. That is clearly illegal.\"\" PunditFact wanted to know if a criminal case against Clinton is that open and shut. Is what happened \"\"clearly illegal\"\"? A spokesman for the Clinton Foundation certainly disagreed, calling Gingrich’s accusation \"\"a baseless leap\"\" because Clinton was not part of her husband’s foundation while serving as a senator or secretary of state. We did not hear from Gingrich by our deadline. Foundation basics Former President Clinton started the William J. Clinton Foundation in 2001, the year after Hillary Clinton won her first term as a New York senator. The foundation works with non-governmental organizations, the private sector and governments around the world on health, anti-poverty, HIV/AIDS and climate change initiatives. Spokesman Craig Minassian said it’s reasonable for the foundation to accept money from foreign governments because of the global scope of its programs, and the donations are usually in the form of tailored grants for specific missions. Hillary Clinton was not part of her husband’s foundation while she was a senator or\\xa0secretary of state. Her appointment to the latter post required Senate confirmation and came with an agreement between the White House and Clinton Foundation that the foundation would be more transparent about its donors. According to the 2008 memorandum of understanding, the foundation would release information behind new donations and could continue to collect donations from countries with which it had existing relationships or running grant programs. If countries with existing contributions significantly stepped up their contributions, or if a new foreign government wanted to donate, the State Department would have to approve. Clinton took an active role in fundraising when she left the State Department and the foundation became the Bill, Hillary & Chelsea Clinton Foundation in 2013. But she left the board when she announced her run for the presidency in April 2015. The Emoluments Clause So how does Gingrich come up with the claim that Clinton Foundation donations are \"\"clearly illegal\"\" and unconstitutional? The answer is something known as the Emoluments Clause. A few conservative websites have made similar arguments in recent days, including the Federalist blog. The Emoluments Clause, found in Article 1, Section 9 of the Constitution, reads in part: \"\"No Title of Nobility shall be granted by the United States: And no Person holding any Office of Profit or Trust under them, shall, without the Consent of the Congress, accept of any present, Emolument, Office, or Title, of any kind whatever, from any King, Prince, or foreign State.\"\" The framers came up with this clause to prevent the government and leaders from granting or receiving titles of nobility and to keep leaders free of external influence. (An emolument, per Merriam-Webster Dictionary, is \"\"the returns arising from office or employment usually in the form of compensation or perquisites.\"\") Lest you think the law is no longer relevant, the Pentagon ethics office in 2013 warned employees the \"\"little known provision\"\" applies to all federal employees and military retirees. There’s no mention of spouses in the memo. J. Peter Pham, director of the Atlantic Council’s Africa Center, said interpretation of the clause has evolved since its adoption at the Constitutional Convention, when the primary concern was about overseas diplomats not seeking gifts from foreign powers they were dealing with. The Defense Department memo, in his view, goes beyond what the framers envisioned for the part of the memo dealing with gifts. \"\"I think that, aside from the unambiguous parts, the burden would be on those invoking the clause to show actual causality that would be in violation of the clause,\"\" Pham said. Expert discussion We asked seven different constitutional law experts on whether the Clinton Foundation foreign donations were \"\"clearly illegal\"\" and a violation of the Emoluments Clause. We did not reach a consensus with their responses, though a majority thought the layers of separation between the foundation and Hillary Clinton work against Gingrich. The American system often distinguishes between public officers and private foundations, \"\"even if real life tends to blur some of those distinctions,\"\" said American University law professor Steve Vladeck. Vladeck added that the Emoluments Clause has never been enforced. \"\"I very much doubt that the first case in its history would be because a foreign government made charitable donations to a private foundation controlled by a government employee’s relative,\"\" he said. \"\"Gingrich may think that giving money to the Clinton Foundation and giving money to then-Secretary Clinton are the same thing. Unfortunately for him, for purposes of federal regulations, statutes, and the Constitution, they’re formally — and, thus, legally — distinct.\"\" Robert Delahunty, a University of St. Thomas constitutional law professor who worked in the Justice Department’s Office of Legal Counsel from 1989 to 2003, also called Gingrich’s link between Clinton and the foreign governments’ gifts to the Clinton Foundation as \"\"implausible, and in any case I don’t think we have the facts to support it.\"\" \"\"The truth is that we establish corporate bodies like the Clinton Foundation because the law endows these entities with a separate and distinct legal personhood,\"\" Delahunty said. John Harrison, University of Virginia law professor and former deputy assistant attorney general in the Office of Legal Counsel from 1990 to 1993, pointed to the Foreign Gifts Act, 5 U.S.C. 7432, which sets rules for how the Emoluments Clause should work in practice. The statute spells out the minimal value for acceptable gifts, and says it applies to spouses of the individuals covered, but \"\"it doesn’t say anything about receipt of foreign gifts by other entities such as the Clinton Foundation.\"\" \"\"I don’t know whether there’s any other provision of federal law that would treat a foreign gift to the foundation as having made to either of the Clintons personally,\"\" Harrison said, who added that agencies have their own supplemental rules for this section, and he did not know if the State Department addressed this. Other experts on the libertarian side of the scale thought Gingrich was more right in his assertion. Clinton violates the clause because of its intentionally broad phrasing about gifts of \"\"any kind whatever,\"\" which would cover indirect gifts via the foundation, said Dave Kopel, a constitutional law professor at Denver University and research director at the libertarian Independence Institute. Kopel also brought up bribery statutes, which would require that a gift had some influence in Clinton’s decision while secretary of state. Delahunty thought Kopel’s reasoning would have \"\"strange consequences,\"\" such as whether a state-owned airline flying Bill Clinton to a conference of former heads of state counted\\xa0as a gift to Hillary Clinton. Our ruling Gingrich said the Clinton Foundation \"\"took money from from foreign governments while (Hillary Clinton) was secretary of state. It is clearly illegal. … The Constitution says you can’t take this stuff.\"\" A clause in the Constitution does prohibit U.S. officials such as former Secretary of State Hillary Clinton from receiving gifts, or emoluments, from foreign governments. But the gifts in this case were donations from foreign governments that went to the Clinton Foundation, not Hillary Clinton. She was not part of the foundation her husband founded while she was secretary of state. Does that violate the Constitution? Some libertarian-minded constitutional law experts say it very well could. Others are skeptical. What’s clear is there is room for ambiguity, and the donations are anything but \"\"clearly illegal.\"\" The reality is this a hazy part of U.S. constitutional\\xa0law.',\n 'sources': 'https://www.wsj.com/articles/clinton-foundation-defends-acceptance-of-foreign-donations-1424302856, https://www.washingtonpost.com/politics/for-clintons-speech-income-shows-how-their-wealth-is-intertwined-with-charity/2015/04/22/12709ec0-dc8d-11e4-a500-1c5bb1d8ff6a_story.html?tid=pm_politics_pop_b, https://www.politifact.com/truth-o-meter/statements/2009/oct/29/ginny-brown-waite/does-president-need-permission-congress-accept-nob/, https://www.politifact.com/truth-o-meter/statements/2015/feb/26/american-crossroads/conservative-group-claims-hillary-clintons-foundat/, http://thefederalist.com/2015/03/02/the-u-s-constitution-actually-bans-hillarys-foreign-government-payola/, https://www.wsj.com/articles/foreign-government-gifts-to-clinton-foundation-on-the-rise-1424223031, https://www.washingtonpost.com/politics/foreign-governments-gave-millions-to-foundation-while-clinton-was-at-state-dept/2015/02/25/31937c1e-bc3f-11e4-8668-4e7ba8439ca6_story.html, https://www.politifact.com/truth-o-meter/statements/2014/apr/01/facebook-posts/meme-says-barack-obamas-acceptance-islamic-order-a/, https://www.nytimes.com/2015/04/20/us/politics/new-book-clinton-cash-questions-foreign-donations-to-foundation.html?&assetType=nyt_now',\n 'label': 0,\n 'subjects': 'Foreign Policy, PunditFact, Newt Gingrich, ',\n 'summary': 'SUBJECTS: Foreign Policy, PunditFact, Newt Gingrich,  - CLAIM: \"The money the Clinton Foundation took from from foreign governments while Hillary Clinton was secretary of state \"\"is clearly illegal. … The Constitution says you can’t take this stuff.\" - EXPLANATION: \"Gingrich said the Clinton Foundation \"\"took money from from foreign governments while (Hillary Clinton) was secretary of state. It is clearly illegal. … The Constitution says you can’t take this stuff.\"\" A clause in the Constitution does prohibit U.S. officials such as former Secretary of State Hillary Clinton from receiving gifts, or emoluments, from foreign governments. But the gifts in this case were donations from foreign governments that went to the Clinton Foundation, not Hillary Clinton. She was not part of the foundation her husband founded while she was secretary of state. Does that violate the Constitution? Some libertarian-minded constitutional law experts say it very well could. Others are skeptical. What’s clear is there is room for ambiguity, and the donations are anything but \"\"clearly illegal.\"\" The reality is this a hazy part of U.S. constitutional\\xa0law. '}"},"metadata":{}}]},{"cell_type":"code","source":"def tokenize(batch):\n    return tokenizer(batch['summary'], truncation=True, padding=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:19:00.679866Z","iopub.execute_input":"2023-03-23T17:19:00.680306Z","iopub.status.idle":"2023-03-23T17:19:00.688027Z","shell.execute_reply.started":"2023-03-23T17:19:00.680254Z","shell.execute_reply":"2023-03-23T17:19:00.686839Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"health_fact_encoded = health_fact.map(tokenize, batched=True, batch_size=None)","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:19:02.863032Z","iopub.execute_input":"2023-03-23T17:19:02.863484Z","iopub.status.idle":"2023-03-23T17:19:12.195420Z","shell.execute_reply.started":"2023-03-23T17:19:02.863447Z","shell.execute_reply":"2023-03-23T17:19:12.193694Z"},"trusted":true},"execution_count":75,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27fc729488024bf2bb34588e7303eb3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"071dbf7cde2a4940b41689f4a3560f6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4735a839f15849bf9859d169d7b3ea01"}},"metadata":{}}]},{"cell_type":"code","source":"print(health_fact_encoded[\"train\"].column_names)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T15:56:49.569537Z","iopub.execute_input":"2023-03-22T15:56:49.570267Z","iopub.status.idle":"2023-03-22T15:56:49.576102Z","shell.execute_reply.started":"2023-03-22T15:56:49.570227Z","shell.execute_reply":"2023-03-22T15:56:49.574829Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"['claim_id', 'claim', 'date_published', 'explanation', 'fact_checkers', 'main_text', 'sources', 'label', 'subjects', 'summary', 'input_ids', 'attention_mask']\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\ndevice= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:20:12.409061Z","iopub.execute_input":"2023-03-23T17:20:12.409541Z","iopub.status.idle":"2023-03-23T17:20:12.416877Z","shell.execute_reply.started":"2023-03-23T17:20:12.409498Z","shell.execute_reply":"2023-03-23T17:20:12.415016Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nnum_labels = 4\nmodel = (AutoModelForSequenceClassification\n         .from_pretrained(model_ckpt, num_labels=num_labels)\n         .to(device))\n","metadata":{"execution":{"iopub.status.busy":"2023-03-23T14:43:02.352584Z","iopub.execute_input":"2023-03-23T14:43:02.353867Z","iopub.status.idle":"2023-03-23T14:43:04.687026Z","shell.execute_reply.started":"2023-03-23T14:43:02.353806Z","shell.execute_reply":"2023-03-23T14:43:04.685817Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    f1 = f1_score(labels, preds, average=\"weighted\")\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc, \"f1\": f1}\n","metadata":{"execution":{"iopub.status.busy":"2023-03-23T14:43:48.384121Z","iopub.execute_input":"2023-03-23T14:43:48.384882Z","iopub.status.idle":"2023-03-23T14:43:48.391431Z","shell.execute_reply.started":"2023-03-23T14:43:48.384840Z","shell.execute_reply":"2023-03-23T14:43:48.389989Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\nbatch_size = 4\nlogging_steps = len(health_fact_encoded[\"train\"]) // batch_size\nmodel_name = f\"{model_ckpt}-finetuned-health-fact\"\ntraining_args = TrainingArguments(output_dir=model_name,\n                                  num_train_epochs=3,\n                                  learning_rate=1e-5,\n                                  per_device_train_batch_size=batch_size,\n                                  per_device_eval_batch_size=batch_size,\n                                  weight_decay=0.0,\n                                  warmup_steps=200,\n                                  evaluation_strategy=\"epoch\",\n                                  save_strategy=\"epoch\",\n                                  disable_tqdm=False,\n                                  logging_steps=logging_steps,\n                                  push_to_hub=False,\n                                  load_best_model_at_end = True,\n                                  log_level=\"error\",)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-23T14:43:07.774350Z","iopub.execute_input":"2023-03-23T14:43:07.774786Z","iopub.status.idle":"2023-03-23T14:43:07.945879Z","shell.execute_reply.started":"2023-03-23T14:43:07.774748Z","shell.execute_reply":"2023-03-23T14:43:07.944812Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()\n\nfrom transformers import Trainer\nfrom transformers import EarlyStoppingCallback\n\nclass CustomEarlyStoppingCallback(EarlyStoppingCallback):\n    def __init__(self, early_stopping_patience=3):\n        super().__init__(early_stopping_patience=early_stopping_patience)\n\nearly_stopping_callback = CustomEarlyStoppingCallback(early_stopping_patience=1)\n        \ntrainer = Trainer(model=model, args=training_args,\n                  compute_metrics=compute_metrics,\n                  train_dataset=health_fact_encoded[\"train\"],\n                  eval_dataset=health_fact_encoded[\"validation\"],\n                  tokenizer=tokenizer,\n                  callbacks=[early_stopping_callback],)\ntrainer.train();","metadata":{"execution":{"iopub.status.busy":"2023-03-23T14:43:53.945335Z","iopub.execute_input":"2023-03-23T14:43:53.946367Z","iopub.status.idle":"2023-03-23T16:55:51.779193Z","shell.execute_reply.started":"2023-03-23T14:43:53.946309Z","shell.execute_reply":"2023-03-23T16:55:51.777951Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.14.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.13.10"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230323_144413-zftu0sna</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/tansaku/huggingface/runs/zftu0sna' target=\"_blank\">elated-lion-13</a></strong> to <a href='https://wandb.ai/tansaku/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/tansaku/huggingface' target=\"_blank\">https://wandb.ai/tansaku/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/tansaku/huggingface/runs/zftu0sna' target=\"_blank\">https://wandb.ai/tansaku/huggingface/runs/zftu0sna</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7353' max='7353' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7353/7353 2:11:04, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.739100</td>\n      <td>0.588144</td>\n      <td>0.759473</td>\n      <td>0.744247</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.584300</td>\n      <td>0.581178</td>\n      <td>0.787479</td>\n      <td>0.782440</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.515500</td>\n      <td>0.785792</td>\n      <td>0.785832</td>\n      <td>0.782677</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Results\n\nWe can run predictions over the validation and test sets for the best model to see how we did","metadata":{}},{"cell_type":"code","source":"trainer.save_model('longformer_on_summary')","metadata":{"execution":{"iopub.status.busy":"2023-03-23T16:56:27.528586Z","iopub.execute_input":"2023-03-23T16:56:27.529051Z","iopub.status.idle":"2023-03-23T16:56:28.562127Z","shell.execute_reply.started":"2023-03-23T16:56:27.529010Z","shell.execute_reply":"2023-03-23T16:56:28.560207Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"preds_output = trainer.predict(health_fact_encoded[\"validation\"])","metadata":{"execution":{"iopub.status.busy":"2023-03-23T16:56:33.915748Z","iopub.execute_input":"2023-03-23T16:56:33.916142Z","iopub.status.idle":"2023-03-23T16:57:56.884823Z","shell.execute_reply.started":"2023-03-23T16:56:33.916106Z","shell.execute_reply":"2023-03-23T16:57:56.883069Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"preds_output.metrics","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:00:08.727745Z","iopub.execute_input":"2023-03-23T17:00:08.728151Z","iopub.status.idle":"2023-03-23T17:00:08.739064Z","shell.execute_reply.started":"2023-03-23T17:00:08.728115Z","shell.execute_reply":"2023-03-23T17:00:08.737936Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"{'test_loss': 0.5811779499053955,\n 'test_accuracy': 0.7874794069192751,\n 'test_f1': 0.7824398745506814,\n 'test_runtime': 82.9499,\n 'test_samples_per_second': 14.635,\n 'test_steps_per_second': 3.665}"},"metadata":{}}]},{"cell_type":"code","source":"test_preds_output = trainer.predict(health_fact_encoded[\"test\"])","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:02:06.938301Z","iopub.execute_input":"2023-03-23T17:02:06.938997Z","iopub.status.idle":"2023-03-23T17:03:30.863764Z","shell.execute_reply.started":"2023-03-23T17:02:06.938946Z","shell.execute_reply":"2023-03-23T17:03:30.858832Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"test_preds_output.metrics","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:03:40.671865Z","iopub.execute_input":"2023-03-23T17:03:40.672397Z","iopub.status.idle":"2023-03-23T17:03:40.695399Z","shell.execute_reply.started":"2023-03-23T17:03:40.672347Z","shell.execute_reply":"2023-03-23T17:03:40.694224Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"{'test_loss': 0.7174000144004822,\n 'test_accuracy': 0.7429034874290349,\n 'test_f1': 0.7363373286634284,\n 'test_runtime': 83.9016,\n 'test_samples_per_second': 14.696,\n 'test_steps_per_second': 3.683}"},"metadata":{}}]},{"cell_type":"markdown","source":"From the above we can see that we achieved an accuracy of ~0.742 and an f1 of ~0.736 on the held out test set","metadata":{}},{"cell_type":"markdown","source":"## HyperParameter Search on Distilbert","metadata":{}},{"cell_type":"code","source":"import wandb\nwandb.init(project=\"pubhealth-hyperparameter-search\")","metadata":{"execution":{"iopub.status.busy":"2023-03-22T12:17:32.489935Z","iopub.execute_input":"2023-03-22T12:17:32.490718Z","iopub.status.idle":"2023-03-22T12:18:17.525347Z","shell.execute_reply.started":"2023-03-22T12:17:32.490680Z","shell.execute_reply":"2023-03-22T12:18:17.524271Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.14.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.13.10"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230322_121746-inlz973z</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/tansaku/pubhealth-hyperparameter-search/runs/inlz973z' target=\"_blank\">driven-feather-12</a></strong> to <a href='https://wandb.ai/tansaku/pubhealth-hyperparameter-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/tansaku/pubhealth-hyperparameter-search' target=\"_blank\">https://wandb.ai/tansaku/pubhealth-hyperparameter-search</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/tansaku/pubhealth-hyperparameter-search/runs/inlz973z' target=\"_blank\">https://wandb.ai/tansaku/pubhealth-hyperparameter-search/runs/inlz973z</a>"},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tansaku/pubhealth-hyperparameter-search/runs/inlz973z?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7f65d17d7e50>"},"metadata":{}}]},{"cell_type":"code","source":"learning_rates = [2e-5, 3e-5, 5e-5]\nbatch_sizes = [8, 16]\nweight_decays = [0.0, 0.01]\nwarmup_steps = [0, 100, 200]\nidx = 1\nfor learning_rate in learning_rates:\n    for batch_size in batch_sizes:\n        for weight_decay in weight_decays:\n            for warmup_step in warmup_steps:\n                print(idx, learning_rate, batch_size, weight_decay, warmup_step)\n                idx += 1","metadata":{"execution":{"iopub.status.busy":"2023-03-22T12:44:10.314098Z","iopub.execute_input":"2023-03-22T12:44:10.314806Z","iopub.status.idle":"2023-03-22T12:44:10.363642Z","shell.execute_reply.started":"2023-03-22T12:44:10.314769Z","shell.execute_reply":"2023-03-22T12:44:10.360419Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"1 2e-05 8 0.0 0\n2 2e-05 8 0.0 100\n3 2e-05 8 0.0 200\n4 2e-05 8 0.01 0\n5 2e-05 8 0.01 100\n6 2e-05 8 0.01 200\n7 2e-05 16 0.0 0\n8 2e-05 16 0.0 100\n9 2e-05 16 0.0 200\n10 2e-05 16 0.01 0\n11 2e-05 16 0.01 100\n12 2e-05 16 0.01 200\n13 3e-05 8 0.0 0\n14 3e-05 8 0.0 100\n15 3e-05 8 0.0 200\n16 3e-05 8 0.01 0\n17 3e-05 8 0.01 100\n18 3e-05 8 0.01 200\n19 3e-05 16 0.0 0\n20 3e-05 16 0.0 100\n21 3e-05 16 0.0 200\n22 3e-05 16 0.01 0\n23 3e-05 16 0.01 100\n24 3e-05 16 0.01 200\n25 5e-05 8 0.0 0\n26 5e-05 8 0.0 100\n27 5e-05 8 0.0 200\n28 5e-05 8 0.01 0\n29 5e-05 8 0.01 100\n30 5e-05 8 0.01 200\n31 5e-05 16 0.0 0\n32 5e-05 16 0.0 100\n33 5e-05 16 0.0 200\n34 5e-05 16 0.01 0\n35 5e-05 16 0.01 100\n36 5e-05 16 0.01 200\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nnum_labels=4\n# Define the model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\n# Function to train and evaluate the model with given hyperparameters\ndef train_evaluate_model(learning_rate, batch_size, weight_decay, warmup_step):\n    model = (AutoModelForSequenceClassification\n         .from_pretrained(model_ckpt, num_labels=num_labels)\n         .to(device))\n\n    training_args = TrainingArguments(\n        output_dir='./results',\n        num_train_epochs=4,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        learning_rate=learning_rate,\n        weight_decay=weight_decay,\n        warmup_steps=warmup_step,\n        logging_dir='./logs',\n        logging_steps=100,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        save_total_limit=1,\n        report_to=\"wandb\",\n        load_best_model_at_end=True,\n        metric_for_best_model=\"accuracy\",\n        greater_is_better=True,\n    )\n\n    trainer = Trainer(\n        model=model, \n        args=training_args,\n        compute_metrics=compute_metrics,\n        train_dataset=health_fact_encoded[\"train\"],\n        eval_dataset=health_fact_encoded[\"validation\"],\n        tokenizer=tokenizer,\n        callbacks=[CustomEarlyStoppingCallback(early_stopping_patience=1)],\n    )\n\n    # Train the model\n    trainer.train()\n    \n    # would be good to run the below up front to check that this actually works ...\n\n    # Evaluate the model on the validation set\n    results = trainer.evaluate(health_fact_encoded[\"validation\"])\n    \n    summary = {\n        \"learning_rate\": learning_rate,\n        \"batch_size\": batch_size,\n        \"weight_decay\": weight_decay,\n        \"warmup_step\": warmup_step,\n        \"accuracy\": results[\"eval_accuracy\"],\n    }\n    print(summary)\n    # Log the hyperparameters and results in wandb\n    wandb.log(summary)\n\n# Perform hyperparameter search\nlearning_rates = [2e-5, 3e-5, 5e-5]\nbatch_sizes = [8, 16]\nweight_decays = [0.0, 0.01]\nwarmup_steps = [0, 100, 200]\n\nfor learning_rate in learning_rates:\n    for batch_size in batch_sizes:\n        for weight_decay in weight_decays:\n            for warmup_step in warmup_steps:\n                train_evaluate_model(learning_rate, batch_size, weight_decay, warmup_step)\n\n\n# Close wandb\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:12:57.064776Z","iopub.execute_input":"2023-03-21T10:12:57.065531Z","iopub.status.idle":"2023-03-21T20:28:01.965027Z","shell.execute_reply.started":"2023-03-21T10:12:57.065491Z","shell.execute_reply":"2023-03-21T20:28:01.964109Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 1.0252, 'learning_rate': 1.959216965742251e-05, 'epoch': 0.08}\n{'loss': 0.8932, 'learning_rate': 1.9184339314845025e-05, 'epoch': 0.16}\n{'loss': 0.801, 'learning_rate': 1.877650897226754e-05, 'epoch': 0.24}\n{'loss': 0.7737, 'learning_rate': 1.8368678629690052e-05, 'epoch': 0.33}\n{'loss': 0.7832, 'learning_rate': 1.7960848287112562e-05, 'epoch': 0.41}\n{'loss': 0.7944, 'learning_rate': 1.7553017944535075e-05, 'epoch': 0.49}\n{'loss': 0.7521, 'learning_rate': 1.714518760195759e-05, 'epoch': 0.57}\n{'loss': 0.7588, 'learning_rate': 1.67373572593801e-05, 'epoch': 0.65}\n{'loss': 0.7484, 'learning_rate': 1.6329526916802612e-05, 'epoch': 0.73}\n{'loss': 0.7169, 'learning_rate': 1.5921696574225122e-05, 'epoch': 0.82}\n{'loss': 0.6943, 'learning_rate': 1.5513866231647635e-05, 'epoch': 0.9}\n{'loss': 0.6838, 'learning_rate': 1.5106035889070147e-05, 'epoch': 0.98}\n","output_type":"stream"},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\n","output_type":"stream"},{"name":"stdout","text":"{'eval_loss': 0.7055772542953491, 'eval_accuracy': 0.71334431630972, 'eval_f1': 0.6903688677860969, 'eval_runtime': 11.1119, 'eval_samples_per_second': 109.252, 'eval_steps_per_second': 13.679, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Model weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.6305, 'learning_rate': 1.469820554649266e-05, 'epoch': 1.06}\n{'loss': 0.6263, 'learning_rate': 1.4290375203915172e-05, 'epoch': 1.14}\n{'loss': 0.5928, 'learning_rate': 1.3882544861337685e-05, 'epoch': 1.22}\n{'loss': 0.6054, 'learning_rate': 1.3474714518760197e-05, 'epoch': 1.31}\n{'loss': 0.6606, 'learning_rate': 1.3066884176182709e-05, 'epoch': 1.39}\n{'loss': 0.5922, 'learning_rate': 1.2659053833605222e-05, 'epoch': 1.47}\n{'loss': 0.6103, 'learning_rate': 1.2251223491027732e-05, 'epoch': 1.55}\n{'loss': 0.57, 'learning_rate': 1.1843393148450246e-05, 'epoch': 1.63}\n{'loss': 0.6311, 'learning_rate': 1.1435562805872757e-05, 'epoch': 1.71}\n{'loss': 0.5665, 'learning_rate': 1.102773246329527e-05, 'epoch': 1.79}\n{'loss': 0.5433, 'learning_rate': 1.0619902120717782e-05, 'epoch': 1.88}\n{'loss': 0.6221, 'learning_rate': 1.0212071778140294e-05, 'epoch': 1.96}\n","output_type":"stream"},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\n","output_type":"stream"},{"name":"stdout","text":"{'eval_loss': 0.6940823197364807, 'eval_accuracy': 0.7397034596375618, 'eval_f1': 0.724394130116743, 'eval_runtime': 11.1378, 'eval_samples_per_second': 108.999, 'eval_steps_per_second': 13.647, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"Model weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.5241, 'learning_rate': 9.804241435562807e-06, 'epoch': 2.04}\n{'loss': 0.4754, 'learning_rate': 9.396411092985319e-06, 'epoch': 2.12}\n{'loss': 0.4245, 'learning_rate': 8.98858075040783e-06, 'epoch': 2.2}\n{'loss': 0.4892, 'learning_rate': 8.580750407830342e-06, 'epoch': 2.28}\n{'loss': 0.4707, 'learning_rate': 8.172920065252856e-06, 'epoch': 2.37}\n{'loss': 0.4692, 'learning_rate': 7.765089722675368e-06, 'epoch': 2.45}\n{'loss': 0.4223, 'learning_rate': 7.35725938009788e-06, 'epoch': 2.53}\n{'loss': 0.4077, 'learning_rate': 6.949429037520392e-06, 'epoch': 2.61}\n{'loss': 0.4449, 'learning_rate': 6.541598694942904e-06, 'epoch': 2.69}\n{'loss': 0.4781, 'learning_rate': 6.133768352365417e-06, 'epoch': 2.77}\n{'loss': 0.4735, 'learning_rate': 5.725938009787929e-06, 'epoch': 2.85}\n{'loss': 0.3973, 'learning_rate': 5.31810766721044e-06, 'epoch': 2.94}\n","output_type":"stream"},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\n","output_type":"stream"},{"name":"stdout","text":"{'eval_loss': 0.7572960257530212, 'eval_accuracy': 0.7397034596375618, 'eval_f1': 0.735014788109258, 'eval_runtime': 11.0975, 'eval_samples_per_second': 109.394, 'eval_steps_per_second': 13.697, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"Model weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7397034596375618).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\n","output_type":"stream"},{"name":"stdout","text":"{'train_runtime': 863.3584, 'train_samples_per_second': 45.423, 'train_steps_per_second': 5.68, 'train_loss': 0.6134814085553301, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"{'eval_loss': 0.6940823197364807, 'eval_accuracy': 0.7397034596375618, 'eval_f1': 0.724394130116743, 'eval_runtime': 11.1364, 'eval_samples_per_second': 109.011, 'eval_steps_per_second': 13.649, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3678' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3678/4904 14:39 < 04:53, 4.18 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.702300</td>\n      <td>0.695786</td>\n      <td>0.712521</td>\n      <td>0.688563</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.616100</td>\n      <td>0.680612</td>\n      <td>0.744646</td>\n      <td>0.729268</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.405800</td>\n      <td>0.747778</td>\n      <td>0.742175</td>\n      <td>0.737500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7446457990115322).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3678' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3678/4904 14:41 < 04:53, 4.17 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.698400</td>\n      <td>0.695706</td>\n      <td>0.711697</td>\n      <td>0.686455</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.619600</td>\n      <td>0.669680</td>\n      <td>0.750412</td>\n      <td>0.737101</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.410500</td>\n      <td>0.737341</td>\n      <td>0.747941</td>\n      <td>0.744122</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7504118616144976).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3678' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3678/4904 14:42 < 04:54, 4.17 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.692100</td>\n      <td>0.694163</td>\n      <td>0.712521</td>\n      <td>0.688047</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.608800</td>\n      <td>0.687121</td>\n      <td>0.745470</td>\n      <td>0.732108</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.394100</td>\n      <td>0.748222</td>\n      <td>0.745470</td>\n      <td>0.745778</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7454695222405272).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3678' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3678/4904 14:42 < 04:54, 4.17 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.703100</td>\n      <td>0.695265</td>\n      <td>0.712521</td>\n      <td>0.688829</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.616300</td>\n      <td>0.682636</td>\n      <td>0.745470</td>\n      <td>0.731350</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.405400</td>\n      <td>0.751938</td>\n      <td>0.744646</td>\n      <td>0.739652</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7454695222405272).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3678' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3678/4904 14:43 < 04:54, 4.16 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.698600</td>\n      <td>0.695440</td>\n      <td>0.716639</td>\n      <td>0.692249</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.620200</td>\n      <td>0.669181</td>\n      <td>0.750412</td>\n      <td>0.736323</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.408800</td>\n      <td>0.737615</td>\n      <td>0.747941</td>\n      <td>0.744051</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7504118616144976).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1226' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1226/2452 09:09 < 09:10, 2.23 it/s, Epoch 2/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.695400</td>\n      <td>0.677611</td>\n      <td>0.726524</td>\n      <td>0.713957</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.593400</td>\n      <td>0.674695</td>\n      <td>0.724876</td>\n      <td>0.704755</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-613 (score: 0.7265238879736409).\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.697800</td>\n      <td>0.677509</td>\n      <td>0.723229</td>\n      <td>0.713332</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.595200</td>\n      <td>0.665070</td>\n      <td>0.737232</td>\n      <td>0.718229</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.476000</td>\n      <td>0.674750</td>\n      <td>0.742998</td>\n      <td>0.738985</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.395300</td>\n      <td>0.725540</td>\n      <td>0.739703</td>\n      <td>0.731357</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.742998352553542).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:20, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.703700</td>\n      <td>0.680429</td>\n      <td>0.727348</td>\n      <td>0.714810</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.598200</td>\n      <td>0.665102</td>\n      <td>0.737232</td>\n      <td>0.717386</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.468500</td>\n      <td>0.668433</td>\n      <td>0.741351</td>\n      <td>0.738134</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.378600</td>\n      <td>0.720160</td>\n      <td>0.741351</td>\n      <td>0.735116</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.7413509060955519).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.694500</td>\n      <td>0.677555</td>\n      <td>0.726524</td>\n      <td>0.714023</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.587900</td>\n      <td>0.674347</td>\n      <td>0.734761</td>\n      <td>0.714436</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.467100</td>\n      <td>0.669087</td>\n      <td>0.749588</td>\n      <td>0.744347</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.388400</td>\n      <td>0.716362</td>\n      <td>0.748764</td>\n      <td>0.740981</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.7495881383855024).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.696900</td>\n      <td>0.674794</td>\n      <td>0.725700</td>\n      <td>0.713512</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.589600</td>\n      <td>0.672211</td>\n      <td>0.733937</td>\n      <td>0.714470</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.466000</td>\n      <td>0.669973</td>\n      <td>0.747941</td>\n      <td>0.744986</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.380900</td>\n      <td>0.711991</td>\n      <td>0.754530</td>\n      <td>0.748496</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7545304777594728).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.703200</td>\n      <td>0.679749</td>\n      <td>0.725700</td>\n      <td>0.712406</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.597800</td>\n      <td>0.665815</td>\n      <td>0.734761</td>\n      <td>0.714839</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.469200</td>\n      <td>0.670259</td>\n      <td>0.742998</td>\n      <td>0.739851</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.378600</td>\n      <td>0.721529</td>\n      <td>0.741351</td>\n      <td>0.735104</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.742998352553542).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:35, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.702500</td>\n      <td>0.681688</td>\n      <td>0.723229</td>\n      <td>0.696683</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.577900</td>\n      <td>0.728081</td>\n      <td>0.734761</td>\n      <td>0.716302</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.322500</td>\n      <td>0.861972</td>\n      <td>0.738880</td>\n      <td>0.735830</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.300600</td>\n      <td>1.084361</td>\n      <td>0.736409</td>\n      <td>0.733986</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-3678 (score: 0.7388797364085667).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3678' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3678/4904 14:43 < 04:54, 4.16 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.702500</td>\n      <td>0.679674</td>\n      <td>0.722405</td>\n      <td>0.697467</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.597700</td>\n      <td>0.683762</td>\n      <td>0.744646</td>\n      <td>0.729278</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.356100</td>\n      <td>0.835116</td>\n      <td>0.736409</td>\n      <td>0.732184</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7446457990115322).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:39, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.714200</td>\n      <td>0.693460</td>\n      <td>0.715815</td>\n      <td>0.688587</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.611000</td>\n      <td>0.705205</td>\n      <td>0.733114</td>\n      <td>0.717944</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.368500</td>\n      <td>0.779808</td>\n      <td>0.733937</td>\n      <td>0.728725</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.307300</td>\n      <td>0.966724</td>\n      <td>0.738056</td>\n      <td>0.732295</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-4904 (score: 0.7380560131795717).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:41, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.702800</td>\n      <td>0.678670</td>\n      <td>0.726524</td>\n      <td>0.700565</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.574800</td>\n      <td>0.699367</td>\n      <td>0.736409</td>\n      <td>0.720355</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.327300</td>\n      <td>0.843683</td>\n      <td>0.742998</td>\n      <td>0.739571</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.304100</td>\n      <td>1.053005</td>\n      <td>0.743822</td>\n      <td>0.740185</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-4904 (score: 0.7438220757825371).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:41, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.704500</td>\n      <td>0.681482</td>\n      <td>0.716639</td>\n      <td>0.691288</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.600900</td>\n      <td>0.692552</td>\n      <td>0.739703</td>\n      <td>0.724360</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.357300</td>\n      <td>0.821473</td>\n      <td>0.740527</td>\n      <td>0.736732</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.277900</td>\n      <td>1.027645</td>\n      <td>0.740527</td>\n      <td>0.738213</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-3678 (score: 0.7405271828665568).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:41, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.703000</td>\n      <td>0.692957</td>\n      <td>0.708402</td>\n      <td>0.680576</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.605900</td>\n      <td>0.690434</td>\n      <td>0.734761</td>\n      <td>0.718889</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.354000</td>\n      <td>0.809770</td>\n      <td>0.741351</td>\n      <td>0.736511</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.312200</td>\n      <td>1.023119</td>\n      <td>0.742998</td>\n      <td>0.739305</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-4904 (score: 0.742998352553542).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1839' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1839/2452 13:46 < 04:35, 2.22 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.692800</td>\n      <td>0.666872</td>\n      <td>0.728995</td>\n      <td>0.718880</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.565600</td>\n      <td>0.668586</td>\n      <td>0.747941</td>\n      <td>0.731736</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.404400</td>\n      <td>0.705550</td>\n      <td>0.740527</td>\n      <td>0.738255</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1226 (score: 0.7479406919275123).\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1226' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1226/2452 09:10 < 09:11, 2.22 it/s, Epoch 2/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.693900</td>\n      <td>0.676170</td>\n      <td>0.728171</td>\n      <td>0.718216</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.576900</td>\n      <td>0.676756</td>\n      <td>0.727348</td>\n      <td>0.710966</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-613 (score: 0.728171334431631).\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1839' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1839/2452 13:45 < 04:35, 2.23 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.690600</td>\n      <td>0.679991</td>\n      <td>0.724053</td>\n      <td>0.712898</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.584600</td>\n      <td>0.659111</td>\n      <td>0.744646</td>\n      <td>0.728230</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.417000</td>\n      <td>0.717380</td>\n      <td>0.738056</td>\n      <td>0.732364</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1226 (score: 0.7446457990115322).\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.687400</td>\n      <td>0.679228</td>\n      <td>0.726524</td>\n      <td>0.710899</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.566300</td>\n      <td>0.667801</td>\n      <td>0.735585</td>\n      <td>0.717613</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.409200</td>\n      <td>0.706129</td>\n      <td>0.738056</td>\n      <td>0.734821</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.300500</td>\n      <td>0.779258</td>\n      <td>0.742998</td>\n      <td>0.737677</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.742998352553542).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:22, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.697300</td>\n      <td>0.667620</td>\n      <td>0.730643</td>\n      <td>0.716925</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.570000</td>\n      <td>0.665627</td>\n      <td>0.734761</td>\n      <td>0.719291</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.407400</td>\n      <td>0.696674</td>\n      <td>0.738056</td>\n      <td>0.738038</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.301000</td>\n      <td>0.788392</td>\n      <td>0.736409</td>\n      <td>0.730842</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.7380560131795717).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1839' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1839/2452 13:46 < 04:35, 2.22 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.696800</td>\n      <td>0.665594</td>\n      <td>0.731466</td>\n      <td>0.715351</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.577300</td>\n      <td>0.667241</td>\n      <td>0.739703</td>\n      <td>0.721338</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.399200</td>\n      <td>0.691265</td>\n      <td>0.738056</td>\n      <td>0.736906</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1226 (score: 0.7397034596375618).\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3678' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3678/4904 14:43 < 04:54, 4.16 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.737600</td>\n      <td>0.689678</td>\n      <td>0.716639</td>\n      <td>0.693725</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.609600</td>\n      <td>0.725373</td>\n      <td>0.738880</td>\n      <td>0.724048</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.367900</td>\n      <td>0.893925</td>\n      <td>0.735585</td>\n      <td>0.725919</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7388797364085667).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:40, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.744600</td>\n      <td>0.707010</td>\n      <td>0.718287</td>\n      <td>0.689123</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.609200</td>\n      <td>0.738522</td>\n      <td>0.742175</td>\n      <td>0.725627</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.305400</td>\n      <td>0.837749</td>\n      <td>0.752059</td>\n      <td>0.748631</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.230400</td>\n      <td>1.150624</td>\n      <td>0.752059</td>\n      <td>0.747039</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-3678 (score: 0.7520593080724877).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:36, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.710100</td>\n      <td>0.729627</td>\n      <td>0.700165</td>\n      <td>0.668853</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.602600</td>\n      <td>0.698634</td>\n      <td>0.736409</td>\n      <td>0.721260</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.353300</td>\n      <td>0.903453</td>\n      <td>0.744646</td>\n      <td>0.732206</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.244000</td>\n      <td>1.150242</td>\n      <td>0.736409</td>\n      <td>0.732604</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-3678 (score: 0.7446457990115322).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3678' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3678/4904 14:43 < 04:54, 4.16 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.726500</td>\n      <td>0.686998</td>\n      <td>0.720758</td>\n      <td>0.692170</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.575700</td>\n      <td>0.684614</td>\n      <td>0.751236</td>\n      <td>0.738765</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.331000</td>\n      <td>0.973908</td>\n      <td>0.747117</td>\n      <td>0.732642</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7512355848434926).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:38, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.727000</td>\n      <td>0.712084</td>\n      <td>0.713344</td>\n      <td>0.683962</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.638200</td>\n      <td>0.695704</td>\n      <td>0.737232</td>\n      <td>0.723807</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.324400</td>\n      <td>0.869525</td>\n      <td>0.745470</td>\n      <td>0.735129</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.254900</td>\n      <td>1.126469</td>\n      <td>0.740527</td>\n      <td>0.738411</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-3678 (score: 0.7454695222405272).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:37, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.727000</td>\n      <td>0.715475</td>\n      <td>0.703460</td>\n      <td>0.672203</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.583200</td>\n      <td>0.700296</td>\n      <td>0.742998</td>\n      <td>0.730058</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.364000</td>\n      <td>0.871863</td>\n      <td>0.750412</td>\n      <td>0.744476</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.284500</td>\n      <td>1.180633</td>\n      <td>0.733114</td>\n      <td>0.728366</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-3678 (score: 0.7504118616144976).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.689000</td>\n      <td>0.671063</td>\n      <td>0.724053</td>\n      <td>0.708760</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.553400</td>\n      <td>0.678851</td>\n      <td>0.731466</td>\n      <td>0.712602</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.343100</td>\n      <td>0.755990</td>\n      <td>0.746293</td>\n      <td>0.744534</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.224000</td>\n      <td>0.919033</td>\n      <td>0.738056</td>\n      <td>0.734555</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.7462932454695222).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.702000</td>\n      <td>0.673133</td>\n      <td>0.728171</td>\n      <td>0.719515</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.552500</td>\n      <td>0.697145</td>\n      <td>0.732290</td>\n      <td>0.713608</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.347100</td>\n      <td>0.756907</td>\n      <td>0.739703</td>\n      <td>0.736556</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.236300</td>\n      <td>0.920691</td>\n      <td>0.734761</td>\n      <td>0.731255</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.7397034596375618).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.706100</td>\n      <td>0.675854</td>\n      <td>0.715815</td>\n      <td>0.695075</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.576000</td>\n      <td>0.689090</td>\n      <td>0.721582</td>\n      <td>0.699489</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.355100</td>\n      <td>0.749004</td>\n      <td>0.738056</td>\n      <td>0.733153</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.237000</td>\n      <td>0.900966</td>\n      <td>0.753707</td>\n      <td>0.750294</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7537067545304778).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.688500</td>\n      <td>0.665402</td>\n      <td>0.723229</td>\n      <td>0.710728</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.546900</td>\n      <td>0.690440</td>\n      <td>0.730643</td>\n      <td>0.710562</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.334300</td>\n      <td>0.757644</td>\n      <td>0.740527</td>\n      <td>0.736749</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.225900</td>\n      <td>0.921424</td>\n      <td>0.741351</td>\n      <td>0.737079</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7413509060955519).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:17, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.703200</td>\n      <td>0.669617</td>\n      <td>0.730643</td>\n      <td>0.720140</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.557500</td>\n      <td>0.692083</td>\n      <td>0.733114</td>\n      <td>0.717699</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.359500</td>\n      <td>0.753943</td>\n      <td>0.741351</td>\n      <td>0.740165</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.229700</td>\n      <td>0.945377</td>\n      <td>0.734761</td>\n      <td>0.729591</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.7413509060955519).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:16, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.700100</td>\n      <td>0.671288</td>\n      <td>0.722405</td>\n      <td>0.705562</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.575300</td>\n      <td>0.677237</td>\n      <td>0.738056</td>\n      <td>0.721710</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.348400</td>\n      <td>0.752762</td>\n      <td>0.749588</td>\n      <td>0.745155</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.228200</td>\n      <td>0.919576</td>\n      <td>0.740527</td>\n      <td>0.738035</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.7495881383855024).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▄▆▇▆▆▇▁▅▅▇█▅▄▆▄▅▅▅▆▁▆▅▄▄▄▇▆▇▆▇▆▄█▅▅▇</td></tr><tr><td>batch_size</td><td>▁▁▁▁▁▁██████▁▁▁▁▁▁██████▁▁▁▁▁▁██████</td></tr><tr><td>eval/accuracy</td><td>▆▇▇▇▇▇▄▆▆▄▇█▆▆▆▅▆▃▆▆▆▄▇▆▆▆▆█▇█▆▁▇▆▆▃▄▆▅▇</td></tr><tr><td>eval/f1</td><td>▆▆▇▆▆▇▄▇▅▅██▇▇▇▆▅▃▇▇▇▅▆▇▇▆▆█▇▇▆▁█▇▇▄▅▇▆█</td></tr><tr><td>eval/loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▁▇▄▃▂▁▃▆▂▁▁▃▂▁▂█▄▁▂▂▄▅▂▁▁▅▅▂</td></tr><tr><td>eval/runtime</td><td>▄▇▇▇▇▇▂▂▂▂▂▂▂▇▇█▇▇█▇▂▂▁▂▂▂█▇▇▇▆▆▇▂▂▃▂▁▁▂</td></tr><tr><td>eval/samples_per_second</td><td>▅▂▂▂▂▂▇▆▇▇▇▇▇▂▂▁▂▂▁▂▇▇█▇▇▇▁▂▂▂▂▃▂▇▇▆▇██▇</td></tr><tr><td>eval/steps_per_second</td><td>██████▁▁▁▁▁▁▁███████▁▁▁▁▁▁▇██████▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▃▃▃▃████████████</td></tr><tr><td>train/epoch</td><td>▃▂▆▅▄▂▁▆███▁▁▆▃▁█▅▂▆▃▇▁▅▄▄▄▃▇▄█▅▆▃▇▇██▁▆</td></tr><tr><td>train/global_step</td><td>▃▁▆▄▃▂▃▁▃▃▃▃▂▆▃▁▆▅▁▆▃▇▃▁▁▁▃▁█▅▁▆▄▁█▄▄▄▄▄</td></tr><tr><td>train/learning_rate</td><td>▃▂▂▂▃▃▂▂▂▁▃▃▅▂▄▄▁▄▁▃▄▁▄▃▅▄▅▅▃▆█▃▄▇▄▄▂▇▅▂</td></tr><tr><td>train/loss</td><td>▆▄▄▆▆▆▄▅▆▄█▆█▃▅▆▂▆▂▄▇▂▆▄█▆▅▆▃▅█▃▃▆▃▄▁▇▄▁</td></tr><tr><td>train/total_flos</td><td>▅▅▅▅▅▅▁██████▅████▅▁▅██▅▅██▅████████</td></tr><tr><td>train/train_loss</td><td>▅▅▆▅▅▆█▄▄▄▄▄▁▄▂▁▂▂▅█▅▂▃▅▄▂▁▄▂▁▁▂▂▁▂▂</td></tr><tr><td>train/train_runtime</td><td>▄▅▅▅▅▅▁▇▇▇▇▇█▅████▄▁▄▇▇▄▅██▅██▇▇▇▇▇▇</td></tr><tr><td>train/train_samples_per_second</td><td>▃▃▃▃▃▃█▁▁▁▁▁▁▃▁▁▁▁▄█▄▁▁▄▃▁▁▃▁▁▁▁▁▁▁▁</td></tr><tr><td>train/train_steps_per_second</td><td>██████▆▁▁▁▁▁▅█▅▅▅▅▃▆▃▁▁▂█▅▅█▅▅▁▁▁▁▁▁</td></tr><tr><td>warmup_step</td><td>▁▅█▁▅█▁▅█▁▅█▁▅█▁▅█▁▅█▁▅█▁▅█▁▅█▁▅█▁▅█</td></tr><tr><td>weight_decay</td><td>▁▁▁███▁▁▁███▁▁▁███▁▁▁███▁▁▁███▁▁▁███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.74959</td></tr><tr><td>batch_size</td><td>16</td></tr><tr><td>eval/accuracy</td><td>0.74959</td></tr><tr><td>eval/f1</td><td>0.74515</td></tr><tr><td>eval/loss</td><td>0.75276</td></tr><tr><td>eval/runtime</td><td>10.9199</td></tr><tr><td>eval/samples_per_second</td><td>111.173</td></tr><tr><td>eval/steps_per_second</td><td>6.96</td></tr><tr><td>learning_rate</td><td>5e-05</td></tr><tr><td>train/epoch</td><td>4.0</td></tr><tr><td>train/global_step</td><td>2452</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.2282</td></tr><tr><td>train/total_flos</td><td>5195026790940672.0</td></tr><tr><td>train/train_loss</td><td>0.50441</td></tr><tr><td>train/train_runtime</td><td>1096.4899</td></tr><tr><td>train/train_samples_per_second</td><td>35.765</td></tr><tr><td>train/train_steps_per_second</td><td>2.236</td></tr><tr><td>warmup_step</td><td>200</td></tr><tr><td>weight_decay</td><td>0.01</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">summer-sea-10</strong> at: <a href='https://wandb.ai/tansaku/pubhealth-hyperparameter-search/runs/va73sjls' target=\"_blank\">https://wandb.ai/tansaku/pubhealth-hyperparameter-search/runs/va73sjls</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20230321_101057-va73sjls/logs</code>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Experimenting with Sweep Configuration","metadata":{}},{"cell_type":"code","source":"# from transformers import AutoModelForSequenceClassification\n# from transformers import Trainer, TrainingArguments\n# num_labels=4\n# # Define the model and tokenizer\n# tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\n# def main(verbose=False):\n#     # Initialise run\n#     run = wandb.init()\n#     model = (AutoModelForSequenceClassification\n#          .from_pretrained(model_ckpt, num_labels=num_labels)\n#          .to(device))\n\n#     training_args = TrainingArguments(\n#         output_dir='./results',\n#         num_train_epochs=4,\n#         per_device_train_batch_size=wandb.config.batch_size,\n#         per_device_eval_batch_size=wandb.config.batch_size,\n#         learning_rate=wandb.config.learning_rate,\n#         weight_decay=wandb.config.weight_decay,\n#         dropout_rate=wandb.config.dropout_rate,\n#         logging_dir='./logs',\n#         logging_steps=100,\n#         evaluation_strategy=\"epoch\",\n#         save_strategy=\"epoch\",\n#         save_total_limit=1,\n#         report_to=\"wandb\",\n#         load_best_model_at_end=True,\n#         metric_for_best_model=\"accuracy\",\n#         greater_is_better=True,\n#         patience=1,  # Number of epochs with no improvement after which training will be stopped\n#     )\n\n#     trainer = Trainer(\n#         model=model, \n#         args=training_args,\n#         compute_metrics=compute_metrics,\n#         train_dataset=health_fact_encoded[\"train\"],\n#         eval_dataset=health_fact_encoded[\"validation\"],\n#         tokenizer=tokenizer\n#     )\n\n#     # Train the model\n#     trainer.train()\n\n#     # Evaluate the model on the validation set\n#     results = trainer.evaluate(health_fact_encoded[\"validation\"])\n\n#     # Log the hyperparameters and results in wandb\n#     wandb.log({\n#         \"learning_rate\": learning_rate,\n#         \"batch_size\": batch_size,\n#         \"weight_decay\": weight_decay,\n#         \"warmup_step\": warmup_step,\n#         \"accuracy\": results[\"eval_accuracy\"],\n#     })","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:04:03.538122Z","iopub.status.idle":"2023-03-21T10:04:03.538893Z","shell.execute_reply.started":"2023-03-21T10:04:03.538621Z","shell.execute_reply":"2023-03-21T10:04:03.538648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sweep_configuration = {\n#     'method': 'bayes',  # random, grid or bayes\n#     'name': 'sweep-bayes',\n#     'metric': {'goal': 'maximize', 'name': 'eval_accuracy'},\n#     'parameters': \n#     {\n#         'batch_size': {'values': [8, 16, 32]},\n#         'learning_rate': {'max': 0.1, 'min': 0.0001},\n#         'weight_decay': {'values': [0.0, 0.1, 0.2]},\n#         'dropout_rate': {'max': 0.5, 'min': 0.1}\n#      }\n# }","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:04:03.540377Z","iopub.status.idle":"2023-03-21T10:04:03.541673Z","shell.execute_reply.started":"2023-03-21T10:04:03.541148Z","shell.execute_reply":"2023-03-21T10:04:03.541175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sweep_id = wandb.sweep(sweep=sweep_configuration, entity='tansaku', project='pubhealth-hyperparameter-search')","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:04:03.543056Z","iopub.status.idle":"2023-03-21T10:04:03.543814Z","shell.execute_reply.started":"2023-03-21T10:04:03.543547Z","shell.execute_reply":"2023-03-21T10:04:03.543573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Maximum 'count' runs\n# wandb.agent(sweep_id, function=main, count=30)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:04:03.545185Z","iopub.status.idle":"2023-03-21T10:04:03.545949Z","shell.execute_reply.started":"2023-03-21T10:04:03.545684Z","shell.execute_reply":"2023-03-21T10:04:03.545711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}