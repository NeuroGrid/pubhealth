{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"    !pip install datasets transformers evaluate imbalanced-learn umap-learn wandb GPUtil","metadata":{"execution":{"iopub.status.busy":"2023-03-22T15:45:38.679624Z","iopub.execute_input":"2023-03-22T15:45:38.680865Z","iopub.status.idle":"2023-03-22T15:45:53.652998Z","shell.execute_reply.started":"2023-03-22T15:45:38.680812Z","shell.execute_reply":"2023-03-22T15:45:53.651694Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (2.1.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.26.1)\nCollecting evaluate\n  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: imbalanced-learn in /opt/conda/lib/python3.7/site-packages (0.10.1)\nRequirement already satisfied: umap-learn in /opt/conda/lib/python3.7/site-packages (0.5.3)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.7/site-packages (0.13.10)\nCollecting GPUtil\n  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2023.1.0)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.28.2)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (5.0.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.11.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.21.6)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.12.1)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.64.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (23.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn) (1.2.0)\nRequirement already satisfied: scikit-learn>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn) (1.0.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn) (3.1.0)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn) (1.7.3)\nRequirement already satisfied: pynndescent>=0.5 in /opt/conda/lib/python3.7/site-packages (from umap-learn) (0.5.8)\nRequirement already satisfied: numba>=0.49 in /opt/conda/lib/python3.7/site-packages (from umap-learn) (0.56.4)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.15.0)\nRequirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.1.30)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from wandb) (4.4.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from wandb) (59.8.0)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.7/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (8.1.3)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.8.2)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (22.2.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /opt/conda/lib/python3.7/site-packages (from numba>=0.49->umap-learn) (0.39.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.11.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.14)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2022.7.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\nBuilding wheels for collected packages: GPUtil\n  Building wheel for GPUtil (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7409 sha256=9aa2457b8f4254263b6345320c2e031a69a7d8eb2c8913a981a6ee0ea4e8a5f3\n  Stored in directory: /root/.cache/pip/wheels/b1/e7/99/2b32600270cf23194c9860f029d3d5db075f250bc39028c045\nSuccessfully built GPUtil\nInstalling collected packages: GPUtil, evaluate\nSuccessfully installed GPUtil-1.4.0 evaluate-0.4.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import gc\nimport torch\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T15:46:02.463445Z","iopub.execute_input":"2023-03-22T15:46:02.464469Z","iopub.status.idle":"2023-03-22T15:46:04.921746Z","shell.execute_reply.started":"2023-03-22T15:46:02.464403Z","shell.execute_reply":"2023-03-22T15:46:04.920544Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2023-03-22T15:46:07.405835Z","iopub.execute_input":"2023-03-22T15:46:07.406712Z","iopub.status.idle":"2023-03-22T15:46:07.905515Z","shell.execute_reply.started":"2023-03-22T15:46:07.406663Z","shell.execute_reply":"2023-03-22T15:46:07.904457Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-22T15:46:09.383913Z","iopub.execute_input":"2023-03-22T15:46:09.384303Z","iopub.status.idle":"2023-03-22T15:46:09.392516Z","shell.execute_reply.started":"2023-03-22T15:46:09.384267Z","shell.execute_reply":"2023-03-22T15:46:09.391201Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\nhealth_fact = load_dataset('health_fact')","metadata":{"execution":{"iopub.status.busy":"2023-03-22T15:46:12.875839Z","iopub.execute_input":"2023-03-22T15:46:12.876565Z","iopub.status.idle":"2023-03-22T15:46:26.169080Z","shell.execute_reply.started":"2023-03-22T15:46:12.876525Z","shell.execute_reply":"2023-03-22T15:46:26.167922Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d6b97cb74af4356b3c45c0b442a1bd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f35bc9e4b74b4139a2d8d015520baf36"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset health_fact/default (download: 23.74 MiB, generated: 64.34 MiB, post-processed: Unknown size, total: 88.08 MiB) to /root/.cache/huggingface/datasets/health_fact/default/1.1.0/99503637e4255bd805f84d57031c18fe4dd88298f00299d56c94fc59ed68ec19...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/24.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cd08c7693f947f7b8b499130c1628f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/9832 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1235 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1225 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset health_fact downloaded and prepared to /root/.cache/huggingface/datasets/health_fact/default/1.1.0/99503637e4255bd805f84d57031c18fe4dd88298f00299d56c94fc59ed68ec19. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cfdf2adf4a04ab6a663093b496ebdd4"}},"metadata":{}}]},{"cell_type":"code","source":"# Filter out instances with a -1 label\nhealth_fact['train'] = health_fact['train'].filter(lambda x: x['label'] != -1)\nhealth_fact['validation'] = health_fact['validation'].filter(lambda x: x['label'] != -1)\nhealth_fact['test'] = health_fact['test'].filter(lambda x: x['label'] != -1)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T15:46:26.171320Z","iopub.execute_input":"2023-03-22T15:46:26.172330Z","iopub.status.idle":"2023-03-22T15:46:26.569137Z","shell.execute_reply.started":"2023-03-22T15:46:26.172282Z","shell.execute_reply":"2023-03-22T15:46:26.567968Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"160beccea2c64033ab64b029b1e0dcee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed359509f16a4acdbd8870e0f1db274b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"660bc3e94e8741bcaba24b36e61e868a"}},"metadata":{}}]},{"cell_type":"code","source":"health_fact.set_format(type=\"pandas\")\ndf = health_fact[\"train\"][:]\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T15:46:28.061724Z","iopub.execute_input":"2023-03-22T15:46:28.062304Z","iopub.status.idle":"2023-03-22T15:46:28.319011Z","shell.execute_reply.started":"2023-03-22T15:46:28.062264Z","shell.execute_reply":"2023-03-22T15:46:28.317918Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"  claim_id                                              claim  \\\n0    15661  \"The money the Clinton Foundation took from fr...   \n1     9893    Annual Mammograms May Have More False-Positives   \n2    11358  SBRT Offers Prostate Cancer Patients High Canc...   \n3    10166  Study: Vaccine for Breast, Ovarian Cancer Has ...   \n4    11276  Some appendicitis cases may not require ’emerg...   \n\n       date_published                                        explanation  \\\n0      April 26, 2015  \"Gingrich said the Clinton Foundation \"\"took m...   \n1    October 18, 2011  This article reports on the results of a study...   \n2  September 28, 2016  This news release describes five-year outcomes...   \n3    November 8, 2011  While the story does many things well, the ove...   \n4  September 20, 2010  We really don’t understand why only a handful ...   \n\n                                       fact_checkers  \\\n0                                      Katie Sanders   \n1                                                      \n2  Mary Chris Jaklevic,Steven J. Atlas, MD, MPH,K...   \n3                                                      \n4                                                      \n\n                                           main_text  \\\n0  \"Hillary Clinton is in the political crosshair...   \n1  While the financial costs of screening mammogr...   \n2  The news release quotes lead researcher Robert...   \n3  The story does discuss costs, but the framing ...   \n4  \"Although the story didn’t cite the cost of ap...   \n\n                                             sources  label  \\\n0  https://www.wsj.com/articles/clinton-foundatio...      0   \n1                                                         1   \n2  https://www.healthnewsreview.org/wp-content/up...      1   \n3  http://clinicaltrials.gov/ct2/results?term=can...      2   \n4                                                         2   \n\n                                      subjects  \n0  Foreign Policy, PunditFact, Newt Gingrich,   \n1               Screening,WebMD,women's health  \n2      Association/Society news release,Cancer  \n3                  Cancer,WebMD,women's health  \n4                                               ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>claim_id</th>\n      <th>claim</th>\n      <th>date_published</th>\n      <th>explanation</th>\n      <th>fact_checkers</th>\n      <th>main_text</th>\n      <th>sources</th>\n      <th>label</th>\n      <th>subjects</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>15661</td>\n      <td>\"The money the Clinton Foundation took from fr...</td>\n      <td>April 26, 2015</td>\n      <td>\"Gingrich said the Clinton Foundation \"\"took m...</td>\n      <td>Katie Sanders</td>\n      <td>\"Hillary Clinton is in the political crosshair...</td>\n      <td>https://www.wsj.com/articles/clinton-foundatio...</td>\n      <td>0</td>\n      <td>Foreign Policy, PunditFact, Newt Gingrich,</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9893</td>\n      <td>Annual Mammograms May Have More False-Positives</td>\n      <td>October 18, 2011</td>\n      <td>This article reports on the results of a study...</td>\n      <td></td>\n      <td>While the financial costs of screening mammogr...</td>\n      <td></td>\n      <td>1</td>\n      <td>Screening,WebMD,women's health</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11358</td>\n      <td>SBRT Offers Prostate Cancer Patients High Canc...</td>\n      <td>September 28, 2016</td>\n      <td>This news release describes five-year outcomes...</td>\n      <td>Mary Chris Jaklevic,Steven J. Atlas, MD, MPH,K...</td>\n      <td>The news release quotes lead researcher Robert...</td>\n      <td>https://www.healthnewsreview.org/wp-content/up...</td>\n      <td>1</td>\n      <td>Association/Society news release,Cancer</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10166</td>\n      <td>Study: Vaccine for Breast, Ovarian Cancer Has ...</td>\n      <td>November 8, 2011</td>\n      <td>While the story does many things well, the ove...</td>\n      <td></td>\n      <td>The story does discuss costs, but the framing ...</td>\n      <td>http://clinicaltrials.gov/ct2/results?term=can...</td>\n      <td>2</td>\n      <td>Cancer,WebMD,women's health</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11276</td>\n      <td>Some appendicitis cases may not require ’emerg...</td>\n      <td>September 20, 2010</td>\n      <td>We really don’t understand why only a handful ...</td>\n      <td></td>\n      <td>\"Although the story didn’t cite the cost of ap...</td>\n      <td></td>\n      <td>2</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def label_int2str(row):\n    if row == -1:\n        return 'invalid'\n    return health_fact[\"train\"].features[\"label\"].int2str(row)\n\ndf[\"label_name\"] = df[\"label\"].apply(label_int2str)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T15:46:34.153466Z","iopub.execute_input":"2023-03-22T15:46:34.153875Z","iopub.status.idle":"2023-03-22T15:46:34.187946Z","shell.execute_reply.started":"2023-03-22T15:46:34.153840Z","shell.execute_reply":"2023-03-22T15:46:34.186971Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T15:46:37.209389Z","iopub.execute_input":"2023-03-22T15:46:37.209782Z","iopub.status.idle":"2023-03-22T15:46:37.226004Z","shell.execute_reply.started":"2023-03-22T15:46:37.209746Z","shell.execute_reply":"2023-03-22T15:46:37.224608Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"  claim_id                                              claim  \\\n0    15661  \"The money the Clinton Foundation took from fr...   \n1     9893    Annual Mammograms May Have More False-Positives   \n2    11358  SBRT Offers Prostate Cancer Patients High Canc...   \n3    10166  Study: Vaccine for Breast, Ovarian Cancer Has ...   \n4    11276  Some appendicitis cases may not require ’emerg...   \n\n       date_published                                        explanation  \\\n0      April 26, 2015  \"Gingrich said the Clinton Foundation \"\"took m...   \n1    October 18, 2011  This article reports on the results of a study...   \n2  September 28, 2016  This news release describes five-year outcomes...   \n3    November 8, 2011  While the story does many things well, the ove...   \n4  September 20, 2010  We really don’t understand why only a handful ...   \n\n                                       fact_checkers  \\\n0                                      Katie Sanders   \n1                                                      \n2  Mary Chris Jaklevic,Steven J. Atlas, MD, MPH,K...   \n3                                                      \n4                                                      \n\n                                           main_text  \\\n0  \"Hillary Clinton is in the political crosshair...   \n1  While the financial costs of screening mammogr...   \n2  The news release quotes lead researcher Robert...   \n3  The story does discuss costs, but the framing ...   \n4  \"Although the story didn’t cite the cost of ap...   \n\n                                             sources  label  \\\n0  https://www.wsj.com/articles/clinton-foundatio...      0   \n1                                                         1   \n2  https://www.healthnewsreview.org/wp-content/up...      1   \n3  http://clinicaltrials.gov/ct2/results?term=can...      2   \n4                                                         2   \n\n                                      subjects label_name  \n0  Foreign Policy, PunditFact, Newt Gingrich,       false  \n1               Screening,WebMD,women's health    mixture  \n2      Association/Society news release,Cancer    mixture  \n3                  Cancer,WebMD,women's health       true  \n4                                                    true  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>claim_id</th>\n      <th>claim</th>\n      <th>date_published</th>\n      <th>explanation</th>\n      <th>fact_checkers</th>\n      <th>main_text</th>\n      <th>sources</th>\n      <th>label</th>\n      <th>subjects</th>\n      <th>label_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>15661</td>\n      <td>\"The money the Clinton Foundation took from fr...</td>\n      <td>April 26, 2015</td>\n      <td>\"Gingrich said the Clinton Foundation \"\"took m...</td>\n      <td>Katie Sanders</td>\n      <td>\"Hillary Clinton is in the political crosshair...</td>\n      <td>https://www.wsj.com/articles/clinton-foundatio...</td>\n      <td>0</td>\n      <td>Foreign Policy, PunditFact, Newt Gingrich,</td>\n      <td>false</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9893</td>\n      <td>Annual Mammograms May Have More False-Positives</td>\n      <td>October 18, 2011</td>\n      <td>This article reports on the results of a study...</td>\n      <td></td>\n      <td>While the financial costs of screening mammogr...</td>\n      <td></td>\n      <td>1</td>\n      <td>Screening,WebMD,women's health</td>\n      <td>mixture</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11358</td>\n      <td>SBRT Offers Prostate Cancer Patients High Canc...</td>\n      <td>September 28, 2016</td>\n      <td>This news release describes five-year outcomes...</td>\n      <td>Mary Chris Jaklevic,Steven J. Atlas, MD, MPH,K...</td>\n      <td>The news release quotes lead researcher Robert...</td>\n      <td>https://www.healthnewsreview.org/wp-content/up...</td>\n      <td>1</td>\n      <td>Association/Society news release,Cancer</td>\n      <td>mixture</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10166</td>\n      <td>Study: Vaccine for Breast, Ovarian Cancer Has ...</td>\n      <td>November 8, 2011</td>\n      <td>While the story does many things well, the ove...</td>\n      <td></td>\n      <td>The story does discuss costs, but the framing ...</td>\n      <td>http://clinicaltrials.gov/ct2/results?term=can...</td>\n      <td>2</td>\n      <td>Cancer,WebMD,women's health</td>\n      <td>true</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11276</td>\n      <td>Some appendicitis cases may not require ’emerg...</td>\n      <td>September 20, 2010</td>\n      <td>We really don’t understand why only a handful ...</td>\n      <td></td>\n      <td>\"Although the story didn’t cite the cost of ap...</td>\n      <td></td>\n      <td>2</td>\n      <td></td>\n      <td>true</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# df[\"label_name\"].value_counts(ascending=True).plot.barh()\n\nax = sns.countplot(x='label_name', data=df, order = df['label_name'].value_counts().index)\nax.bar_label(ax.containers[0])\nplt.title(\"Frequency of Classes\")","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:59:45.088686Z","iopub.execute_input":"2023-03-21T09:59:45.089440Z","iopub.status.idle":"2023-03-21T09:59:45.346441Z","shell.execute_reply.started":"2023-03-21T09:59:45.089410Z","shell.execute_reply":"2023-03-21T09:59:45.345434Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Text(0.5, 1.0, 'Frequency of Classes')"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"df['Words Per Explanation'] = df['explanation'].str.split().apply(len)\nsns.boxplot(x=df['Words Per Explanation'], y=df['label_name'], data=df, order = df['label_name'].value_counts().index)\nplt.title(\"Words Per Explanation\")\nplt.xlabel(\"\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:59:45.348981Z","iopub.execute_input":"2023-03-21T09:59:45.349777Z","iopub.status.idle":"2023-03-21T09:59:45.870149Z","shell.execute_reply.started":"2023-03-21T09:59:45.349705Z","shell.execute_reply":"2023-03-21T09:59:45.868991Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"# checkpoint = \"bert-base-uncased\"\n# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# tokenized_explanations = tokenizer(health_fact[\"train\"][\"explanation\"], max_length=512, truncation=True, padding=True, return_tensors=\"pt\")","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:00:01.491145Z","iopub.execute_input":"2023-03-21T10:00:01.492211Z","iopub.status.idle":"2023-03-21T10:00:01.498071Z","shell.execute_reply.started":"2023-03-21T10:00:01.492175Z","shell.execute_reply":"2023-03-21T10:00:01.496825Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"health_fact.reset_format()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T15:46:54.451576Z","iopub.execute_input":"2023-03-22T15:46:54.452312Z","iopub.status.idle":"2023-03-22T15:46:54.460722Z","shell.execute_reply.started":"2023-03-22T15:46:54.452273Z","shell.execute_reply":"2023-03-22T15:46:54.456694Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# model_ckpt = 'distilbert-base-uncased'\nmodel_ckpt = 'allenai/longformer-base-4096'\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T16:45:20.664407Z","iopub.execute_input":"2023-03-22T16:45:20.664815Z","iopub.status.idle":"2023-03-22T16:45:26.507798Z","shell.execute_reply.started":"2023-03-22T16:45:20.664777Z","shell.execute_reply":"2023-03-22T16:45:26.506499Z"},"trusted":true},"execution_count":78,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e2fc30badf04a60a16385d7aafba810"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5afb56e5254846bea0c4027c265e6372"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63335b778de24239aa9a7a0cb9c84acb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d0dcf46d0584995839b2e210a6338e2"}},"metadata":{}}]},{"cell_type":"code","source":"\ndef new_column(example):\n    example[\"summary\"] = f\"SUBJECTS: {example['subjects']} - CLAIM: {example['claim']} - EXPLANATION: {example['explanation']}\"\n    return example\n\nhealth_fact = health_fact.map(new_column)\nhealth_fact['train'][0]","metadata":{"execution":{"iopub.status.busy":"2023-03-22T15:56:08.324809Z","iopub.execute_input":"2023-03-22T15:56:08.325394Z","iopub.status.idle":"2023-03-22T15:56:10.581755Z","shell.execute_reply.started":"2023-03-22T15:56:08.325348Z","shell.execute_reply":"2023-03-22T15:56:10.579824Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9804 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b018cd11d9ac4da58db2471d5fdf93ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1233 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3c0a993f5734d0bb25f38f27dbc2c69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1214 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1afbbda397c84727a8230da9dae413e2"}},"metadata":{}},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"{'claim_id': '15661',\n 'claim': '\"The money the Clinton Foundation took from from foreign governments while Hillary Clinton was secretary of state \"\"is clearly illegal. … The Constitution says you can’t take this stuff.\"',\n 'date_published': 'April 26, 2015',\n 'explanation': '\"Gingrich said the Clinton Foundation \"\"took money from from foreign governments while (Hillary Clinton) was secretary of state. It is clearly illegal. … The Constitution says you can’t take this stuff.\"\" A clause in the Constitution does prohibit U.S. officials such as former Secretary of State Hillary Clinton from receiving gifts, or emoluments, from foreign governments. But the gifts in this case were donations from foreign governments that went to the Clinton Foundation, not Hillary Clinton. She was not part of the foundation her husband founded while she was secretary of state. Does that violate the Constitution? Some libertarian-minded constitutional law experts say it very well could. Others are skeptical. What’s clear is there is room for ambiguity, and the donations are anything but \"\"clearly illegal.\"\" The reality is this a hazy part of U.S. constitutional\\xa0law. ',\n 'fact_checkers': 'Katie Sanders',\n 'main_text': '\"Hillary Clinton is in the political crosshairs as the author of a new book alleges improper financial ties between her public and personal life. At issue in conservative author Peter Schweizer’s forthcoming book Clinton Cash are donations from foreign governments to the Clinton Foundation during the four years she served as secretary of state. George Stephanopoulos used an interview with Schweizer on ABC This Week to point out what other nonpartisan journalists have found: There is no \"\"smoking gun\"\" showing that donations to the foundation influenced her foreign policy decisions. Still, former Republican House Speaker Newt Gingrich says the donations are \"\"clearly illegal\"\" under federal law. In his view, a donation by a foreign government to the Clinton Foundation while Clinton was secretary of state is the same as money sent directly to her, he said, even though she did not join the foundation’s board until she left her post. \"\"The Constitution of the United States says you cannot take money from foreign governments without explicit permission of the Congress. They wrote that in there because they knew the danger of corrupting our system by foreign money is enormous,\"\" Gingrich said. \"\"You had a sitting secretary of state whose husband radically increased his speech fees, you have a whole series of dots on the wall now where people gave millions of dollars — oh, by the way, they happen to get taken care of by the State Department.\"\" He continued, \"\"My point is they took money from foreign governments while she was secretary of State. That is clearly illegal.\"\" PunditFact wanted to know if a criminal case against Clinton is that open and shut. Is what happened \"\"clearly illegal\"\"? A spokesman for the Clinton Foundation certainly disagreed, calling Gingrich’s accusation \"\"a baseless leap\"\" because Clinton was not part of her husband’s foundation while serving as a senator or secretary of state. We did not hear from Gingrich by our deadline. Foundation basics Former President Clinton started the William J. Clinton Foundation in 2001, the year after Hillary Clinton won her first term as a New York senator. The foundation works with non-governmental organizations, the private sector and governments around the world on health, anti-poverty, HIV/AIDS and climate change initiatives. Spokesman Craig Minassian said it’s reasonable for the foundation to accept money from foreign governments because of the global scope of its programs, and the donations are usually in the form of tailored grants for specific missions. Hillary Clinton was not part of her husband’s foundation while she was a senator or\\xa0secretary of state. Her appointment to the latter post required Senate confirmation and came with an agreement between the White House and Clinton Foundation that the foundation would be more transparent about its donors. According to the 2008 memorandum of understanding, the foundation would release information behind new donations and could continue to collect donations from countries with which it had existing relationships or running grant programs. If countries with existing contributions significantly stepped up their contributions, or if a new foreign government wanted to donate, the State Department would have to approve. Clinton took an active role in fundraising when she left the State Department and the foundation became the Bill, Hillary & Chelsea Clinton Foundation in 2013. But she left the board when she announced her run for the presidency in April 2015. The Emoluments Clause So how does Gingrich come up with the claim that Clinton Foundation donations are \"\"clearly illegal\"\" and unconstitutional? The answer is something known as the Emoluments Clause. A few conservative websites have made similar arguments in recent days, including the Federalist blog. The Emoluments Clause, found in Article 1, Section 9 of the Constitution, reads in part: \"\"No Title of Nobility shall be granted by the United States: And no Person holding any Office of Profit or Trust under them, shall, without the Consent of the Congress, accept of any present, Emolument, Office, or Title, of any kind whatever, from any King, Prince, or foreign State.\"\" The framers came up with this clause to prevent the government and leaders from granting or receiving titles of nobility and to keep leaders free of external influence. (An emolument, per Merriam-Webster Dictionary, is \"\"the returns arising from office or employment usually in the form of compensation or perquisites.\"\") Lest you think the law is no longer relevant, the Pentagon ethics office in 2013 warned employees the \"\"little known provision\"\" applies to all federal employees and military retirees. There’s no mention of spouses in the memo. J. Peter Pham, director of the Atlantic Council’s Africa Center, said interpretation of the clause has evolved since its adoption at the Constitutional Convention, when the primary concern was about overseas diplomats not seeking gifts from foreign powers they were dealing with. The Defense Department memo, in his view, goes beyond what the framers envisioned for the part of the memo dealing with gifts. \"\"I think that, aside from the unambiguous parts, the burden would be on those invoking the clause to show actual causality that would be in violation of the clause,\"\" Pham said. Expert discussion We asked seven different constitutional law experts on whether the Clinton Foundation foreign donations were \"\"clearly illegal\"\" and a violation of the Emoluments Clause. We did not reach a consensus with their responses, though a majority thought the layers of separation between the foundation and Hillary Clinton work against Gingrich. The American system often distinguishes between public officers and private foundations, \"\"even if real life tends to blur some of those distinctions,\"\" said American University law professor Steve Vladeck. Vladeck added that the Emoluments Clause has never been enforced. \"\"I very much doubt that the first case in its history would be because a foreign government made charitable donations to a private foundation controlled by a government employee’s relative,\"\" he said. \"\"Gingrich may think that giving money to the Clinton Foundation and giving money to then-Secretary Clinton are the same thing. Unfortunately for him, for purposes of federal regulations, statutes, and the Constitution, they’re formally — and, thus, legally — distinct.\"\" Robert Delahunty, a University of St. Thomas constitutional law professor who worked in the Justice Department’s Office of Legal Counsel from 1989 to 2003, also called Gingrich’s link between Clinton and the foreign governments’ gifts to the Clinton Foundation as \"\"implausible, and in any case I don’t think we have the facts to support it.\"\" \"\"The truth is that we establish corporate bodies like the Clinton Foundation because the law endows these entities with a separate and distinct legal personhood,\"\" Delahunty said. John Harrison, University of Virginia law professor and former deputy assistant attorney general in the Office of Legal Counsel from 1990 to 1993, pointed to the Foreign Gifts Act, 5 U.S.C. 7432, which sets rules for how the Emoluments Clause should work in practice. The statute spells out the minimal value for acceptable gifts, and says it applies to spouses of the individuals covered, but \"\"it doesn’t say anything about receipt of foreign gifts by other entities such as the Clinton Foundation.\"\" \"\"I don’t know whether there’s any other provision of federal law that would treat a foreign gift to the foundation as having made to either of the Clintons personally,\"\" Harrison said, who added that agencies have their own supplemental rules for this section, and he did not know if the State Department addressed this. Other experts on the libertarian side of the scale thought Gingrich was more right in his assertion. Clinton violates the clause because of its intentionally broad phrasing about gifts of \"\"any kind whatever,\"\" which would cover indirect gifts via the foundation, said Dave Kopel, a constitutional law professor at Denver University and research director at the libertarian Independence Institute. Kopel also brought up bribery statutes, which would require that a gift had some influence in Clinton’s decision while secretary of state. Delahunty thought Kopel’s reasoning would have \"\"strange consequences,\"\" such as whether a state-owned airline flying Bill Clinton to a conference of former heads of state counted\\xa0as a gift to Hillary Clinton. Our ruling Gingrich said the Clinton Foundation \"\"took money from from foreign governments while (Hillary Clinton) was secretary of state. It is clearly illegal. … The Constitution says you can’t take this stuff.\"\" A clause in the Constitution does prohibit U.S. officials such as former Secretary of State Hillary Clinton from receiving gifts, or emoluments, from foreign governments. But the gifts in this case were donations from foreign governments that went to the Clinton Foundation, not Hillary Clinton. She was not part of the foundation her husband founded while she was secretary of state. Does that violate the Constitution? Some libertarian-minded constitutional law experts say it very well could. Others are skeptical. What’s clear is there is room for ambiguity, and the donations are anything but \"\"clearly illegal.\"\" The reality is this a hazy part of U.S. constitutional\\xa0law.',\n 'sources': 'https://www.wsj.com/articles/clinton-foundation-defends-acceptance-of-foreign-donations-1424302856, https://www.washingtonpost.com/politics/for-clintons-speech-income-shows-how-their-wealth-is-intertwined-with-charity/2015/04/22/12709ec0-dc8d-11e4-a500-1c5bb1d8ff6a_story.html?tid=pm_politics_pop_b, https://www.politifact.com/truth-o-meter/statements/2009/oct/29/ginny-brown-waite/does-president-need-permission-congress-accept-nob/, https://www.politifact.com/truth-o-meter/statements/2015/feb/26/american-crossroads/conservative-group-claims-hillary-clintons-foundat/, http://thefederalist.com/2015/03/02/the-u-s-constitution-actually-bans-hillarys-foreign-government-payola/, https://www.wsj.com/articles/foreign-government-gifts-to-clinton-foundation-on-the-rise-1424223031, https://www.washingtonpost.com/politics/foreign-governments-gave-millions-to-foundation-while-clinton-was-at-state-dept/2015/02/25/31937c1e-bc3f-11e4-8668-4e7ba8439ca6_story.html, https://www.politifact.com/truth-o-meter/statements/2014/apr/01/facebook-posts/meme-says-barack-obamas-acceptance-islamic-order-a/, https://www.nytimes.com/2015/04/20/us/politics/new-book-clinton-cash-questions-foreign-donations-to-foundation.html?&assetType=nyt_now',\n 'label': 0,\n 'subjects': 'Foreign Policy, PunditFact, Newt Gingrich, ',\n 'summary': 'SUBJECTS: Foreign Policy, PunditFact, Newt Gingrich,  - CLAIM: \"The money the Clinton Foundation took from from foreign governments while Hillary Clinton was secretary of state \"\"is clearly illegal. … The Constitution says you can’t take this stuff.\" - EXPLANATION: \"Gingrich said the Clinton Foundation \"\"took money from from foreign governments while (Hillary Clinton) was secretary of state. It is clearly illegal. … The Constitution says you can’t take this stuff.\"\" A clause in the Constitution does prohibit U.S. officials such as former Secretary of State Hillary Clinton from receiving gifts, or emoluments, from foreign governments. But the gifts in this case were donations from foreign governments that went to the Clinton Foundation, not Hillary Clinton. She was not part of the foundation her husband founded while she was secretary of state. Does that violate the Constitution? Some libertarian-minded constitutional law experts say it very well could. Others are skeptical. What’s clear is there is room for ambiguity, and the donations are anything but \"\"clearly illegal.\"\" The reality is this a hazy part of U.S. constitutional\\xa0law. '}"},"metadata":{}}]},{"cell_type":"code","source":"# def tokenize(batch):\n#     return tokenizer(batch['explanation'], truncation=True, padding=True)\n\ndef tokenize(batch):\n    return tokenizer(batch['summary'], truncation=True, padding=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T16:45:36.437322Z","iopub.execute_input":"2023-03-22T16:45:36.437687Z","iopub.status.idle":"2023-03-22T16:45:36.444771Z","shell.execute_reply.started":"2023-03-22T16:45:36.437655Z","shell.execute_reply":"2023-03-22T16:45:36.443533Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"health_fact_encoded = health_fact.map(tokenize, batched=True, batch_size=None)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T16:45:39.833658Z","iopub.execute_input":"2023-03-22T16:45:39.834409Z","iopub.status.idle":"2023-03-22T16:45:49.655713Z","shell.execute_reply.started":"2023-03-22T16:45:39.834367Z","shell.execute_reply":"2023-03-22T16:45:49.654411Z"},"trusted":true},"execution_count":80,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebb8668501304f5691e7fb1dbdcd070b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a087ea62489b47e5b75f77fc188ca143"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b9b64f24ea04d788bf911b1fe9facb8"}},"metadata":{}}]},{"cell_type":"code","source":"print(health_fact_encoded[\"train\"].column_names)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T15:56:49.569537Z","iopub.execute_input":"2023-03-22T15:56:49.570267Z","iopub.status.idle":"2023-03-22T15:56:49.576102Z","shell.execute_reply.started":"2023-03-22T15:56:49.570227Z","shell.execute_reply":"2023-03-22T15:56:49.574829Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"['claim_id', 'claim', 'date_published', 'explanation', 'fact_checkers', 'main_text', 'sources', 'label', 'subjects', 'summary', 'input_ids', 'attention_mask']\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModel\n\ndevice= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = AutoModel.from_pretrained(model_ckpt).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T16:46:41.935329Z","iopub.execute_input":"2023-03-22T16:46:41.935732Z","iopub.status.idle":"2023-03-22T16:46:47.367402Z","shell.execute_reply.started":"2023-03-22T16:46:41.935699Z","shell.execute_reply":"2023-03-22T16:46:47.366072Z"},"trusted":true},"execution_count":81,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/597M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4d607894c4e463f914f7fade9c617ea"}},"metadata":{}}]},{"cell_type":"code","source":"def extract_hidden_states(batch):\n    inputs = {k:v.to(device) for k,v in batch.items() if k in tokenizer.model_input_names}\n    with torch.no_grad():\n        last_hidden_state = model(**inputs).last_hidden_state\n    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}","metadata":{"execution":{"iopub.status.busy":"2023-03-22T16:46:50.273378Z","iopub.execute_input":"2023-03-22T16:46:50.274140Z","iopub.status.idle":"2023-03-22T16:46:50.282410Z","shell.execute_reply.started":"2023-03-22T16:46:50.274102Z","shell.execute_reply":"2023-03-22T16:46:50.281132Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"health_fact_encoded.set_format('torch', columns=[\"input_ids\", \"attention_mask\", \"label\"])","metadata":{"execution":{"iopub.status.busy":"2023-03-22T16:46:54.748271Z","iopub.execute_input":"2023-03-22T16:46:54.748655Z","iopub.status.idle":"2023-03-22T16:46:54.757840Z","shell.execute_reply.started":"2023-03-22T16:46:54.748619Z","shell.execute_reply":"2023-03-22T16:46:54.756915Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"from GPUtil import showUtilization as gpu_usage\ngpu_usage()  ","metadata":{"execution":{"iopub.status.busy":"2023-03-22T16:47:00.147400Z","iopub.execute_input":"2023-03-22T16:47:00.147784Z","iopub.status.idle":"2023-03-22T16:47:00.277456Z","shell.execute_reply.started":"2023-03-22T16:47:00.147751Z","shell.execute_reply":"2023-03-22T16:47:00.275859Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n| ID | GPU | MEM |\n------------------\n|  0 |  0% | 45% |\n","output_type":"stream"}]},{"cell_type":"code","source":"del df","metadata":{"execution":{"iopub.status.busy":"2023-03-22T16:47:03.868094Z","iopub.execute_input":"2023-03-22T16:47:03.869159Z","iopub.status.idle":"2023-03-22T16:47:03.876695Z","shell.execute_reply.started":"2023-03-22T16:47:03.869111Z","shell.execute_reply":"2023-03-22T16:47:03.875291Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T16:47:05.605830Z","iopub.execute_input":"2023-03-22T16:47:05.606986Z","iopub.status.idle":"2023-03-22T16:47:06.250849Z","shell.execute_reply.started":"2023-03-22T16:47:05.606931Z","shell.execute_reply":"2023-03-22T16:47:06.249519Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"from GPUtil import showUtilization as gpu_usage\ngpu_usage()  ","metadata":{"execution":{"iopub.status.busy":"2023-03-22T16:47:07.531419Z","iopub.execute_input":"2023-03-22T16:47:07.532355Z","iopub.status.idle":"2023-03-22T16:47:07.654229Z","shell.execute_reply.started":"2023-03-22T16:47:07.532315Z","shell.execute_reply":"2023-03-22T16:47:07.652573Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n| ID | GPU | MEM |\n------------------\n|  0 |  0% | 16% |\n","output_type":"stream"}]},{"cell_type":"code","source":"health_fact_hidden = health_fact_encoded.map(extract_hidden_states, batched=True, batch_size=100)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T16:47:11.809073Z","iopub.execute_input":"2023-03-22T16:47:11.810166Z","iopub.status.idle":"2023-03-22T16:57:10.655318Z","shell.execute_reply.started":"2023-03-22T16:47:11.810117Z","shell.execute_reply":"2023-03-22T16:57:10.653822Z"},"trusted":true},"execution_count":88,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/99 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac157e349e8643368b6821874e7f4cd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/13 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30336ef4618043f7871fcfd959be6afd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/13 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c6e4a291ce14225828daf453bfa833e"}},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\n\nX_train = np.array(health_fact_hidden[\"train\"][\"hidden_state\"])\nX_valid = np.array(health_fact_hidden[\"validation\"][\"hidden_state\"])\ny_train = np.array(health_fact_hidden[\"train\"][\"label\"])\ny_valid = np.array(health_fact_hidden[\"validation\"][\"label\"])\nX_train.shape, X_valid.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:00:39.673092Z","iopub.execute_input":"2023-03-22T17:00:39.674080Z","iopub.status.idle":"2023-03-22T17:00:39.807004Z","shell.execute_reply.started":"2023-03-22T17:00:39.674023Z","shell.execute_reply":"2023-03-22T17:00:39.805771Z"},"trusted":true},"execution_count":89,"outputs":[{"execution_count":89,"output_type":"execute_result","data":{"text/plain":"((9804, 768), (1214, 768))"},"metadata":{}}]},{"cell_type":"code","source":"from umap import UMAP\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Scale features to [0,1] range\nX_scaled = MinMaxScaler().fit_transform(X_train)\n# Initialize and fit UMAP\nmapper = UMAP(n_components=2, metric=\"cosine\").fit(X_scaled)\n# Create a DataFrame of 2D embeddings\ndf_emb = pd.DataFrame(mapper.embedding_, columns=[\"X\", \"Y\"])\ndf_emb[\"label\"] = y_train\ndf_emb.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:00:42.168715Z","iopub.execute_input":"2023-03-22T17:00:42.169432Z","iopub.status.idle":"2023-03-22T17:00:57.175822Z","shell.execute_reply.started":"2023-03-22T17:00:42.169381Z","shell.execute_reply":"2023-03-22T17:00:57.174819Z"},"trusted":true},"execution_count":90,"outputs":[{"execution_count":90,"output_type":"execute_result","data":{"text/plain":"          X         Y  label\n0  2.895852  0.785735      0\n1  8.520674  5.252949      1\n2  8.558901  6.864313      1\n3  9.121176  6.926792      2\n4  7.640649  5.638116      2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X</th>\n      <th>Y</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.895852</td>\n      <td>0.785735</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8.520674</td>\n      <td>5.252949</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8.558901</td>\n      <td>6.864313</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9.121176</td>\n      <td>6.926792</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.640649</td>\n      <td>5.638116</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize=(5,5))\naxes = axes.flatten()\ncmaps = [\"Greys\", \"Blues\", \"Oranges\", \"Reds\", \"Greens\"]\nlabels = health_fact[\"train\"].features[\"label\"].names\n\nfor i, (label, cmap) in enumerate(zip(labels, cmaps)):\n    df_emb_sub = df_emb.query(f\"label == {i}\")\n    axes[i].hexbin(df_emb_sub[\"X\"], df_emb_sub[\"Y\"], cmap=cmap,\n                   gridsize=20, linewidths=(0,))\n    axes[i].set_title(label)\n    axes[i].set_xticks([]), axes[i].set_yticks([])\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:01:43.601338Z","iopub.execute_input":"2023-03-22T17:01:43.602163Z","iopub.status.idle":"2023-03-22T17:01:43.926857Z","shell.execute_reply.started":"2023-03-22T17:01:43.602123Z","shell.execute_reply":"2023-03-22T17:01:43.925222Z"},"trusted":true},"execution_count":91,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 500x500 with 4 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr_clf = LogisticRegression(max_iter=3000)\nlr_clf.fit(X_train, y_train)\nlr_clf.score(X_valid, y_valid)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:01:57.480838Z","iopub.execute_input":"2023-03-22T17:01:57.481550Z","iopub.status.idle":"2023-03-22T17:02:59.378445Z","shell.execute_reply.started":"2023-03-22T17:01:57.481512Z","shell.execute_reply":"2023-03-22T17:02:59.373103Z"},"trusted":true},"execution_count":92,"outputs":[{"execution_count":92,"output_type":"execute_result","data":{"text/plain":"0.7018121911037891"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.dummy import DummyClassifier\n\ndummy_clf = DummyClassifier(strategy=\"most_frequent\")\ndummy_clf.fit(X_train, y_train)\ndummy_clf.score(X_valid, y_valid)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T16:02:10.723028Z","iopub.execute_input":"2023-03-22T16:02:10.723407Z","iopub.status.idle":"2023-03-22T16:02:10.737983Z","shell.execute_reply.started":"2023-03-22T16:02:10.723374Z","shell.execute_reply":"2023-03-22T16:02:10.736959Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"0.5181219110378913"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n\ndef plot_confusion_matrix(y_preds, y_true, labels):\n    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n    fig, ax = plt.subplots(figsize=(6, 6))\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n    plt.title(\"Normalized confusion matrix\")\n    plt.show()\n\ny_preds = lr_clf.predict(X_valid)\nplot_confusion_matrix(y_preds, y_valid, labels)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:03:32.688539Z","iopub.execute_input":"2023-03-22T17:03:32.689279Z","iopub.status.idle":"2023-03-22T17:03:32.994018Z","shell.execute_reply.started":"2023-03-22T17:03:32.689238Z","shell.execute_reply":"2023-03-22T17:03:32.992589Z"},"trusted":true},"execution_count":93,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 600x600 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nnum_labels = 4\nmodel = (AutoModelForSequenceClassification\n         .from_pretrained(model_ckpt, num_labels=num_labels)\n         .to(device))\n","metadata":{"execution":{"iopub.status.busy":"2023-03-22T19:21:25.277615Z","iopub.execute_input":"2023-03-22T19:21:25.278004Z","iopub.status.idle":"2023-03-22T19:21:32.235473Z","shell.execute_reply.started":"2023-03-22T19:21:25.277967Z","shell.execute_reply":"2023-03-22T19:21:32.233963Z"},"trusted":true},"execution_count":145,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    f1 = f1_score(labels, preds, average=\"weighted\")\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc, \"f1\": f1}\n","metadata":{"execution":{"iopub.status.busy":"2023-03-22T17:03:42.052164Z","iopub.execute_input":"2023-03-22T17:03:42.057817Z","iopub.status.idle":"2023-03-22T17:03:42.066119Z","shell.execute_reply.started":"2023-03-22T17:03:42.057775Z","shell.execute_reply":"2023-03-22T17:03:42.065017Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\nbatch_size = 2\nlogging_steps = len(health_fact_encoded[\"train\"]) // batch_size\nmodel_name = f\"{model_ckpt}-finetuned-health-fact\"\ntraining_args = TrainingArguments(output_dir=model_name,\n                                  num_train_epochs=4,\n                                  learning_rate=1e-5,\n                                  per_device_train_batch_size=batch_size,\n                                  per_device_eval_batch_size=batch_size,\n                                  weight_decay=0.0,\n                                  warmup_steps=200,\n                                  evaluation_strategy=\"epoch\",\n                                  save_strategy=\"epoch\",\n                                  disable_tqdm=False,\n                                  logging_steps=logging_steps,\n                                  push_to_hub=False,\n                                  load_best_model_at_end = True,\n                                  log_level=\"error\",)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-22T19:19:53.886803Z","iopub.execute_input":"2023-03-22T19:19:53.887539Z","iopub.status.idle":"2023-03-22T19:19:53.900330Z","shell.execute_reply.started":"2023-03-22T19:19:53.887494Z","shell.execute_reply":"2023-03-22T19:19:53.899275Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":"del model\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T19:20:58.915328Z","iopub.execute_input":"2023-03-22T19:20:58.915709Z","iopub.status.idle":"2023-03-22T19:20:59.397339Z","shell.execute_reply.started":"2023-03-22T19:20:58.915674Z","shell.execute_reply":"2023-03-22T19:20:59.396054Z"},"trusted":true},"execution_count":142,"outputs":[]},{"cell_type":"code","source":"gpu_usage()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T19:21:01.340800Z","iopub.execute_input":"2023-03-22T19:21:01.341806Z","iopub.status.idle":"2023-03-22T19:21:01.485527Z","shell.execute_reply.started":"2023-03-22T19:21:01.341765Z","shell.execute_reply":"2023-03-22T19:21:01.484003Z"},"trusted":true},"execution_count":143,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n| ID | GPU | MEM |\n------------------\n|  0 |  0% | 28% |\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch import nn\nfrom transformers import Trainer\n\n# ['false', 'mixture', 'true', 'unproven']\ncounts = [3001, 1434, 5078, 291]\nweights = [ 1 - (count/sum(counts))for count in counts]\n\nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n#         print(inputs)\n        labels = inputs.get(\"labels\")\n        inputs = {k: v.to(self.args.device) for k, v in inputs.items()}\n        \n#         print(labels)\n        # forward pass\n        outputs = model(**inputs)\n#         print(outputs)\n        logits = outputs.get(\"logits\")\n        # compute custom loss (for 4 labels with different weights)\n        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(weights).to(device))\n        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss\n\nweights","metadata":{"execution":{"iopub.status.busy":"2023-03-22T12:30:30.261847Z","iopub.execute_input":"2023-03-22T12:30:30.262891Z","iopub.status.idle":"2023-03-22T12:30:30.281239Z","shell.execute_reply.started":"2023-03-22T12:30:30.262848Z","shell.execute_reply":"2023-03-22T12:30:30.280207Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"[0.6939004487964096,\n 0.8537331701346389,\n 0.48204814361485104,\n 0.9703182374541004]"},"metadata":{}}]},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()\n\nfrom transformers import Trainer\nfrom transformers import EarlyStoppingCallback\n\nclass CustomEarlyStoppingCallback(EarlyStoppingCallback):\n    def __init__(self, early_stopping_patience=3):\n        super().__init__(early_stopping_patience=early_stopping_patience)\n\nearly_stopping_callback = CustomEarlyStoppingCallback(early_stopping_patience=1)\n        \ntrainer = Trainer(model=model, args=training_args,\n                  compute_metrics=compute_metrics,\n                  train_dataset=health_fact_encoded[\"train\"],\n                  eval_dataset=health_fact_encoded[\"validation\"],\n                  tokenizer=tokenizer,\n                  callbacks=[early_stopping_callback],)\ntrainer.train();","metadata":{"execution":{"iopub.status.busy":"2023-03-22T19:21:32.238129Z","iopub.execute_input":"2023-03-22T19:21:32.238973Z","iopub.status.idle":"2023-03-22T20:56:15.206452Z","shell.execute_reply.started":"2023-03-22T19:21:32.238923Z","shell.execute_reply":"2023-03-22T20:56:15.205539Z"},"trusted":true},"execution_count":146,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9804' max='19608' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 9804/19608 1:34:41 < 1:34:43, 1.73 it/s, Epoch 2/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.879400</td>\n      <td>0.756939</td>\n      <td>0.760297</td>\n      <td>0.738893</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.779400</td>\n      <td>0.967213</td>\n      <td>0.772652</td>\n      <td>0.758761</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"preds_output = trainer.predict(health_fact_encoded[\"validation\"])","metadata":{"execution":{"iopub.status.busy":"2023-03-22T16:32:25.144121Z","iopub.execute_input":"2023-03-22T16:32:25.145051Z","iopub.status.idle":"2023-03-22T16:32:36.175281Z","shell.execute_reply.started":"2023-03-22T16:32:25.144998Z","shell.execute_reply":"2023-03-22T16:32:36.173976Z"},"trusted":true},"execution_count":53,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"preds_output.metrics","metadata":{"execution":{"iopub.status.busy":"2023-03-22T16:32:43.563461Z","iopub.execute_input":"2023-03-22T16:32:43.564699Z","iopub.status.idle":"2023-03-22T16:32:43.574683Z","shell.execute_reply.started":"2023-03-22T16:32:43.564653Z","shell.execute_reply":"2023-03-22T16:32:43.573507Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"{'test_loss': 0.5714957118034363,\n 'test_accuracy': 0.7528830313014827,\n 'test_f1': 0.7379914743143585,\n 'test_runtime': 11.0134,\n 'test_samples_per_second': 110.229,\n 'test_steps_per_second': 6.901}"},"metadata":{}}]},{"cell_type":"code","source":"y_preds = np.argmax(preds_output.predictions, axis=1) ","metadata":{"execution":{"iopub.status.busy":"2023-03-22T16:35:01.454835Z","iopub.execute_input":"2023-03-22T16:35:01.455944Z","iopub.status.idle":"2023-03-22T16:35:01.463485Z","shell.execute_reply.started":"2023-03-22T16:35:01.455868Z","shell.execute_reply":"2023-03-22T16:35:01.462117Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(y_preds, y_valid, labels)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T16:35:14.244021Z","iopub.execute_input":"2023-03-22T16:35:14.244484Z","iopub.status.idle":"2023-03-22T16:35:14.625769Z","shell.execute_reply.started":"2023-03-22T16:35:14.244449Z","shell.execute_reply":"2023-03-22T16:35:14.623975Z"},"trusted":true},"execution_count":56,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 600x600 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAkEAAAIhCAYAAABJ8G73AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABu5klEQVR4nO3dd1QUVxsG8GfpvVcFaSpgodjBAtgL9sSGBTXm0xhrjL3EHqOJJYnGaAQTe2IXSyyIYsOGFTuIBUHqUkSBne8P4urCopBQlHl+53COe+ed2Xt3WHy4c2eRCIIggIiIiEhkVCq6A0REREQVgSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiojAUHB0MikUBLSwsPHz4stN3X1xd16tSpgJ6VjsDAQNjb2yu02dvbIzAwsFz7ERMTA4lEguDg4HJ93pL48ccfUb16dWhoaEAikSA1NbVUj//6ey0mJqZUj/shuXnzJr755psSj9HX1xe+vr5l0if6eKlVdAeIxOLly5eYPn06/vjjj4ruSpnbuXMnDAwMKrobH5TIyEiMHj0an332GQYNGgQ1NTXo6+uX6nN06tQJZ86cgbW1dake90Ny8+ZNzJ49G76+voXC97usXLmy7DpFHy2GIKJy0r59e2zatAkTJkyAu7t7mT3PixcvoK2tXWbHLw5PT88Kff4P0Y0bNwAAw4YNQ6NGjcrkOczNzWFubl4mx/5YZWVlQUdHB7Vq1arortAHiJfDiMrJxIkTYWpqikmTJr23Njs7G1OmTIGDgwM0NDRQtWpVjBw5stDlE3t7e/j7+2PHjh3w9PSElpYWZs+ejePHj0MikWDTpk2YNGkSrK2toaenh86dOyM+Ph7p6en4/PPPYWZmBjMzMwwePBgZGRkKx/7555/RokULWFhYQFdXF3Xr1sV3332HnJyc9/a/4OUwX19fSCQSpV9vX7569uwZ/ve//8HGxgYaGhpwcHDA7NmzkZubq3D8p0+folevXtDX14ehoSF69+6NZ8+evbdfrz158gSff/45bG1toaGhgSpVquCTTz5BfHy8vCY2Nhb9+/eHhYUFNDU14erqiu+//x4ymUxe8/oS3JIlS/DDDz/AwcEBenp68PLywtmzZxXG379/fwBA48aNIZFI5K9PUZcOC16+kclkmDdvHpydnaGtrQ0jIyO4ublh+fLl8pqiLoetW7cO7u7u0NLSgomJCbp3746oqCiFmsDAQOjp6eHevXvo2LEj9PT0YGtri6+++govX75872v6+ntx37598PT0hLa2NlxdXbFv3z5531xdXaGrq4tGjRrhwoULCvtfuHABffr0gb29PbS1tWFvb4++ffsqXEIODg7Gp59+CgDw8/Mr9D30+tLyiRMn4O3tDR0dHQwZMkTp6/ntt99CRUUFe/fuLfQ66Ojo4Nq1a+8dM338OBNEVE709fUxffp0jBkzBseOHUPLli2V1gmCgG7duuHo0aOYMmUKmjdvjqtXr2LWrFk4c+YMzpw5A01NTXn9pUuXEBUVhenTp8PBwQG6urrIzMwEAEydOhV+fn4IDg5GTEwMJkyYgL59+0JNTQ3u7u7YvHkzLl++jKlTp0JfXx8rVqyQH/f+/fvo16+fPIhduXIF8+fPx61bt7Bu3boSjX3lypWQSqUKbTNmzEBoaCicnZ0B5AegRo0aQUVFBTNnzoSTkxPOnDmDefPmISYmBkFBQQDyZ7pat26Np0+fYuHChahZsyZCQkLQu3fvYvXlyZMnaNiwIXJycjB16lS4ubkhKSkJhw4dQkpKCiwtLfH8+XN4e3vj1atXmDt3Luzt7bFv3z5MmDAB9+/fL3Rp5eeff4aLiwuWLVsmH1vHjh0RHR0NQ0NDrFy5Eps3b8a8efMQFBQEFxeXEs/YfPfdd/jmm28wffp0tGjRAjk5Obh169Z71xUtXLgQU6dORd++fbFw4UIkJSXhm2++gZeXF86fP48aNWrIa3NyctClSxcMHToUX331FU6cOIG5c+fC0NAQM2fOfG8fr1y5gilTpmDatGkwNDTE7Nmz0aNHD0yZMgVHjx7FggULIJFIMGnSJPj7+yM6Olo+axkTEwNnZ2f06dMHJiYmiIuLw6pVq9CwYUPcvHkTZmZm6NSpExYsWICpU6fi559/Rr169QAATk5O8j7ExcWhf//+mDhxIhYsWAAVFeW/60+aNAknT57EoEGDcPnyZdjZ2SEoKAjr16/H2rVrUbdu3feOlyoBgYjKVFBQkABAOH/+vPDy5UvB0dFRaNCggSCTyQRBEAQfHx+hdu3a8vqDBw8KAITvvvtO4Thbt24VAAi//vqrvM3Ozk5QVVUVbt++rVAbGhoqABA6d+6s0D527FgBgDB69GiF9m7dugkmJiZFjiEvL0/IyckRfv/9d0FVVVVITk6Wbxs0aJBgZ2enUG9nZycMGjSoyOMtXry40Fj+97//CXp6esLDhw8VapcsWSIAEG7cuCEIgiCsWrVKACDs3r1boW7YsGECACEoKKjI5xUEQRgyZIigrq4u3Lx5s8iayZMnCwCEc+fOKbSPGDFCkEgk8tc7OjpaACDUrVtXyM3NlddFREQIAITNmzfL297+PnhbUa+Vj4+P4OPjI3/s7+8veHh4vHNsr58jOjpaEARBSElJEbS1tYWOHTsq1MXGxgqamppCv3795G2DBg0SAAjbtm1TqO3YsaPg7Oz8zud9PQ5tbW3h8ePH8rbIyEgBgGBtbS1kZmbK23ft2iUAEPbs2VPk8XJzc4WMjAxBV1dXWL58ubz9zz//FAAIoaGhhfbx8fERAAhHjx5Vuu3t11MQBCExMVGwsbERGjVqJFy6dEnQ0dER+vfv/96xUuXBy2FE5UhDQwPz5s3DhQsXsG3bNqU1x44dA4BCl0g+/fRT6Orq4ujRowrtbm5uqFmzptJj+fv7Kzx2dXUFkL+AtmB7cnKywiWxy5cvo0uXLjA1NYWqqirU1dUxcOBA5OXl4c6dO+8fbBE2b96MiRMnYvr06Rg2bJi8fd++ffDz80OVKlWQm5sr/+rQoQMAICwsDAAQGhoKfX19dOnSReG4/fr1K9bzHzhwAH5+fvLXQpljx46hVq1ahdbuBAYGQhAE+Tl6rVOnTlBVVZU/dnNzAwCldwP+W40aNcKVK1fwxRdf4NChQ4Vm1pQ5c+YMXrx4Ueh7ydbWFi1btiz0vSSRSNC5c2eFNjc3t2KPw8PDA1WrVpU/fv0a+/r6QkdHp1D728fNyMjApEmTUL16daipqUFNTQ16enrIzMwsdOnuXYyNjYucZS3I1NQUW7duxaVLl+Dt7Y1q1arhl19+KfZz0cePIYionPXp0wf16tXDtGnTlK6vSUpKgpqaWqHLJRKJBFZWVkhKSlJof9edQCYmJgqPNTQ03tmenZ0NIH89TPPmzfHkyRMsX74cJ0+exPnz5/Hzzz8DyL8k9W+EhoYiMDAQAwcOxNy5cxW2xcfHY+/evVBXV1f4ql27NgAgMTERQP7rY2lpWejYVlZWxerD8+fPYWNj886apKQkpa9rlSpV5NvfZmpqqvD49eXKf/s6KTNlyhQsWbIEZ8+eRYcOHWBqaopWrVoVWlvzttf9LGosBceho6MDLS0thTZNTU3598X7/NvvNyA/xP7000/47LPPcOjQIUREROD8+fMwNzcv0etY0jvjGjdujNq1ayM7OxsjRoyArq5uifanjxvXBBGVM4lEgkWLFqFNmzb49ddfC203NTVFbm4unj9/rhCEBEHAs2fP0LBhw0LHK227du1CZmYmduzYATs7O3l7ZGTkvz7m1atX0a1bN/j4+GDNmjWFtpuZmcHNzQ3z589Xuv/rAGJqaoqIiIhC24u7MNrc3ByPHz9+Z42pqSni4uIKtT99+lTe19KipaWldOFxYmKiwvOoqalh/PjxGD9+PFJTU3HkyBFMnToV7dq1w6NHjxRmWt4eB4Aix1Ka4/gv0tLSsG/fPsyaNQuTJ0+Wt798+RLJycklOlZJ3w+zZs3CtWvXUL9+fcycORP+/v5wdHQs0THo48WZIKIK0Lp1a7Rp0wZz5swpdFdWq1atAAAbNmxQaN++fTsyMzPl28vS6/9I3l6ALQiC0vBSHLGxsejQoQMcHR2xfft2qKurF6rx9/fH9evX4eTkhAYNGhT6eh2C/Pz8kJ6ejj179ijsv2nTpmL1pUOHDggNDcXt27eLrGnVqhVu3ryJS5cuKbT//vvvkEgk8PPzK9ZzFYe9vT2uXr2q0Hbnzp139s/IyAiffPIJRo4cieTk5CI/ONDLywva2tqFvpceP36MY8eOlcv3UnFIJBIIgqDw/QYAa9euRV5enkJbac6yHT58GAsXLsT06dNx+PBh+Z2Gr169+s/Hpo8DZ4KIKsiiRYtQv359JCQkyC/5AECbNm3Qrl07TJo0CVKpFE2bNpXfHebp6YkBAwaUed/atGkDDQ0N9O3bFxMnTkR2djZWrVqFlJSUf3W8Dh06IDU1FT/99JP883Jec3Jygrm5OebMmYPDhw/D29sbo0ePhrOzM7KzsxETE4P9+/fjl19+gY2NDQYOHIilS5di4MCBmD9/PmrUqIH9+/fj0KFDxerLnDlzcODAAbRo0QJTp05F3bp1kZqaioMHD2L8+PFwcXHBuHHj8Pvvv6NTp06YM2cO7OzsEBISgpUrV2LEiBFFrsH6NwYMGID+/fvjiy++QM+ePfHw4UN89913hS6Hdu7cGXXq1EGDBg1gbm6Ohw8fYtmyZbCzs1O4w+ttRkZGmDFjBqZOnYqBAweib9++SEpKwuzZs6GlpYVZs2aV2jj+CwMDA7Ro0QKLFy+GmZkZ7O3tERYWht9++w1GRkYKta8/Xf3XX3+Fvr4+tLS04ODgUOiS5Pu8vovMx8cHs2bNgoqKCrZu3YoWLVpg4sSJ8jv9qHLjTBBRBfH09ETfvn0LtUskEuzatQvjx49HUFAQOnbsiCVLlmDAgAE4duxYod+Wy4KLiwu2b9+OlJQU9OjRA6NGjYKHh4fCLfQlcfPmTWRlZaFHjx7w8vJS+AoJCQGQv5bjwoULaNu2LRYvXoz27dtjwIABWLduHTw8PGBsbAwgf93KsWPH0Lp1a0yePBmffPIJHj9+jC1bthSrL1WrVkVERAT8/f3x7bffon379hg1ahTS0tLka1fMzc1x+vRptGzZElOmTIG/vz8OHTqE7777Dj/++OO/eg2K0q9fP3z33Xc4dOgQ/P39sWrVKqxatapQ0PLz88OJEycwfPhwtGnTBtOnT0erVq0QFhamdGbttSlTpmDt2rW4cuUKunXrhi+//BK1a9fG6dOniwxPFWHTpk3w8/PDxIkT0aNHD1y4cEE+O/M2BwcHLFu2DFeuXIGvry8aNmxY6LN+3icvLw99+/aVf5bW69vomzRpggULFmD58uXYtWtXaQ2NPmASQRCEiu4EERERUXnjTBARERGJEkMQERERiRJDEBEREYkSQxARERGJEkMQERERiRJDEBEREYkSPyzxAyeTyfD06VPo6+uXyZ9HICIiqkwEQUB6ejqqVKki/wyoojAEfeCePn0KW1vbiu4GERHRR+XRo0fv/WPJDEEfOH19fQCAUc8fIVHXruDe0H91elHXiu4ClaK0LP6NqcrC1rTwH6Clj1N6uhQuTnby/z/fhSHoA/f6EphEXRsSDb5JP3b6BgYV3QUqRXmqDEGVhYEBf75WNsVZQsKF0URERCRKDEFEREQkSgxBREREJEoMQURERCRKDEFEREQkSgxBREREJEoMQURERCRKDEFEREQkSgxBREREJEoMQURERCRKDEFEREQkSgxBREREJEoMQURERCRKDEFEREQkSgxBREREJEoMQURERCRKDEFEREQkSgxBREREJEoMQURERCRKDEFEREQkSgxBREREJEoMQURERCRKDEFEREQkSgxBREREJEoMQURERCRKDEFEREQkSgxBREREJEoMQURERCRKDEFEREQkSgxBREREJEoMQURERCRKDEFEREQkSgxBREREJEoMQURERCRKDEFEREQkSgxBREREJEoMQURERCRKDEFEREQkSgxBREREJEoMQURERCRKDEFEREQkSgxBREREJEoMQURERCRKDEFEREQkSgxBREREJEoMQURERCRKDEFEREQkSgxBREREJEoMQURERCRKahXdAaqcBvlVx/B2rrAw0sadJ2mYteUSIu4+V1q7dEhj9GrqWKj99pM0tJy5v1B7l0bVsOp/TXHw8mMM/elkqfedCtuw6xTWbg1FQpIUNeytMP3LbmjoVvicvXYu8h4WrNyDuzHPYGlmgGF9WqJfF2+FmqC/wrBpz2k8jU+BsaEe2vu44ethnaCpoV7WwxG1P0PO4I8dJ5CYnA7Hapb4apg/POs4KK1NTJZi6W8hiLr3BI+eJqFPZ2989XnnQnXpGS+w8o9DOHb6BtIzXqCKpTHGDu2EZg1dyno4orfur5P4eeNRxCdJ4exghXnjesLLw6nI+lOX7mLm8p24Hf0MVmaG+LJ/KwT2aCbf/seu09h6IAK3HsQBANydbTFtRGfUq21X5mOpCKKfCRIEAZ9//jlMTEwgkUgQGRn5zvqYmJhi1YlZl4bV8E2felgRcgPtZh9ExN3n2DDWB1VMdJTWz9x8CR7jdsq/GkzYhZSMl9h3IbZQbVVTHcz81BNn7ySU9TDoHyHHLmP+z7swon9r7FnzFRq6OWDopF/xND5Faf2juCR8NmUtGro5YM+arzA8oDXm/rgTB8OuyGt2H76Ixb+GYNTAtji0fjIWft0b+0MjsXhNSHkNS5T+PnEF36/ZhyG9/LBxxWh41rbH6G+C8CwhVWn9q5xcGBvoYkgvP9RwsFJak5OTi5EzfsPT+BQsmhKA7au/wvRRPWFhalCGIyEA2Hn4EqYv24GxgW1xbP1ENPFwQp9xq/D4WbLS+odPk9Bv/Go08XDCsfUTMWZQG0z9YTv2HouU15y6dBc92tTHzp9H4cCa8ahqZYxPx6xEXBHfIx870YeggwcPIjg4GPv27UNcXBzq1KlT0V366A1r64wtJx9g88kHuBcnxawtl/A0OQsDfWsorU9/kYPn0mz5l5u9CQx1NLD11AOFOhWJBD8N88aS3dcQ+zyjPIZCANb9GYZPOzZG705NUN3OEtO/7A5rCyNs3HNKaf3mPadRxcII07/sjup2lujdqQk+6dAIa7cdl9dcvhmD+nUc0KV1fdhYmaB5Q2f4t/TE9duPymlU4rRxVzi6tmmAbu0awcHWAl993hmWZob4a/9ZpfVVLE0w4X9d4N+qPvR0tJTW7D58AWnpWfh++kB41LKHtYUxPGrbo6ZjlbIcCgH4ZXMoAjo3wYCu3qjpYIX543qiqoUxgnaEK61fvyMcVS2NMX9cT9R0sMKArt7o17kJVm469uaYcwZhyCfNUbemDWrYW2LplL6QyWQ4ceFOeQ2rXIk+BN2/fx/W1tbw9vaGlZUV1NR4hfC/UFdVgZudCcJuPFNoD7v5DA2qmxXrGH2bOeFk1DM8ScpSaB/XpTaS0rOxJfxBEXtSaXuVk4vrdx6jWYOaCu3NGjjj0vUYpftcvvkQzRo4K7Q1b+iM67cfISc3DwDQoK4jrt95hCtRDwEAsU+TEHYuCr5NapX+IAhA/ozNrXtP0MRT8ZeRJp41cPXWw3993BPnouDmUg2LVu1G2/7z0OuLpVi3LRR5ebL/2mV6h1c5ubhy+xF8GytecvRt7ILz16KV7nP+ekyher/GLoiMipW/Nwt6kf0KuXkyGBson8n/2Ik6BAUGBmLUqFGIjY2FRCKBvb09Dh48iGbNmsHIyAimpqbw9/fH/fv3izxGSkoKAgICYG5uDm1tbdSoUQNBQUHy7U+ePEHv3r1hbGwMU1NTdO3aFTExMUUe7+XLl5BKpQpfHxMTfU2oqaogUZqt0J6Ylg0LQ+W/Sb7NwlALfnWtsfmE4mveoLoZ+jZzwtfrI0q1v/RuKWmZyJPJYGasr9BuaqyPxJR0pfs8T06HaYF6M2N95ObJkJKWCQDwb+mJcUM6oM/on+DSegJaBsxHY4/qGN6vVdkMhJAqzUKeTAaTAufG5B3nsjiexCfj6KnryJPJsPybQAzt3RIbd57Eum3H3r8z/WvJqZnIy5PB3ETxfJqb6CMhSfn5TEiSKq3PzZMhKVX57PqclXtgZW6IFg2dlW7/2Ik6BC1fvhxz5syBjY0N4uLicP78eWRmZmL8+PE4f/48jh49ChUVFXTv3h0ymfLfambMmIGbN2/iwIEDiIqKwqpVq2Bmlj/jkZWVBT8/P+jp6eHEiRMIDw+Hnp4e2rdvj1evXik93sKFC2FoaCj/srW1LbPxlyUBgsJjiQQQhCKK39KrqSOkWTk4ePmJvE1XSw0/fuaFr9dHICVD+etGZUwiKdAgQIKCbUWXvz73r9vPRt7Dyg1H8M3Yntj963isnBOI0LM38dPvf5den0mpQmdSECApdH6LT5AJMDbSxbQve8C1ug3a+bhjcC8//LX/3H/rKBVLwXOXfz7fVa/4+M17s/BOP/5xBDsPX0LwwqHQ0qycNyyI+tqPoaEh9PX1oaqqCiur/EV/PXv2VKj57bffYGFhgZs3bypdLxQbGwtPT080aNAAAGBvby/ftmXLFqioqGDt2rXyb7CgoCAYGRnh+PHjaNu2baHjTZkyBePHj5c/lkqlH1UQSk5/idw8GcwNtBXaTQ208LzA7JAyfZo5YvuZaOS8NZVub66HauZ6CB7dQt6m8s/r+fDX3mgxLQQPuUaoTBgb6kJVRQWJyYozkkkpGTA11lO6j7mJPhKTFX8TTUpNh5qqCowMdAEAy9YdQLe29dG7UxMAgLNjFWRlv8L07//EF/1bQ0VF1L+flQkjAx2oqqggqcCsT0pqBkyNlJ/L4jAz0YeaqipUVd+cMwdbCySlpCMnJxfq6qL+b6bMmBjpQlVVBQlJiu/NxJSMQrM9r1mYGhSaJUpMyX9vmhjqKrT/vPEolq0/jO0/jkTtGlVLt/MfEP6kKeD+/fvo168fHB0dYWBgAAeH/FtHY2ML36kEACNGjMCWLVvg4eGBiRMn4vTp0/JtFy9exL1796Cvrw89PT3o6enBxMQE2dnZRV5i09TUhIGBgcLXxyQnT4arD5PRorbinSQtalnhwr3Ed+7r5WwBB0t9bC6w5udenBQtZ+5H29kH5V9/X3mC07fj0Xb2QTxNziriiPRfaairoU5NG4QXWBQZfvEO6tWxV7qPZy07hF8sUH/hDuo420JdTRUA8CI7Rx5kX1NVUYEgCMWaMaSSU1dXg0v1qjgXeU+h/VzkPbi5/Pvbn91d7fAoLlFhtjz2yXOYmegzAJUhDXU1uDvbIizitkJ7WMQtNKyr/CMPGtaxR1jELYW24+duwcO1mvy9CQA/bTiK79cdwtZlw+HhWq30O/8BYQgqoHPnzkhKSsKaNWtw7tw5nDuXP6Vb1OWrDh064OHDhxg7diyePn2KVq1aYcKECQAAmUyG+vXrIzIyUuHrzp076NevX7mNqbyt+fs2+jZ3RO9mjqhubYBvenuiqokO/gi7CwCY3MMdy4c2KbRf3+aOuHQ/EbefpCm0v8yV4faTNIUvadYrZGTn4vaTNIVZIyp9Qz71wZ/7z+HP/edw72E85v28C3HxKejXOf9zfxav2YcJCzbJ6/t28cbT+BTM/3k37j2Ml+/7WS9feU1L71rYuOc09h27jEdxSQi/cBtL1x1AK+86CjMKVLoCujXDrr/PY/ff5xH9KAHfr9mLZ89T0bNjYwDAT8EHMfP7rQr73H7wFLcfPMWL7FdIScvE7QdP8SA2Xr69Z8cmSEvPwpJf9+Lhk+cIP38LQX8ex6edvMp1bGI0vK8fNuw5g417z+BO9DNMX7YDj+NTENg9/3N/5q7cg5Gz/5DXD+rRDI+fpWDGsh24E/0MG/eewca9Z/FFv5bymh//OIKFq/dh+bR+sLU2RXySFPFJUmRkvSz38ZUHxvS3JCUlISoqCqtXr0bz5s0BAOHhym81fJu5uTkCAwMRGBiI5s2b4+uvv8aSJUtQr149bN26FRYWFh/djM5/sed8LIz1NDCuc21YGGrj9pM0DFgeJr/by9JIq9BnBulrq6NjPVvM3HKpIrpM79CppSdSpFn46fe/kZAsRU17a6z9dhiqWpkAAJ4npeNpwpvPDLK1NsXahZ9h/srd2LA7HJamhpgxqjva+7jLa0YOaAOJRIIfftuP+MQ0mBjpoaVXbXz1WcdyH5+YtG3hjrT0LKzdchSJyelwsrPC8m8CYW1hDABITJHi2fNUhX0CRq+Q/zvq3hMcDIuEtYUR9q6bDACwMjfCT3OG4oe1+9D3y+UwNzVAny5NMainT7mNS6y6t6mHlLRMfP/bIcQnpcHF0RqbfxgOW+v892Z8ohSPn715b9pVMcWmH/6HGct2Yt32k7AyM8SC8T3RuaWHvCZoezhe5eRhyNR1Cs/19dD2mDis8r0/JYIg7snnZcuWYdmyZYiJiYFMJoOFhQU6dOiAWbNmITY2FpMnT8b58+exc+dOdOvWDTExMXBwcMDly5fh4eGBmTNnon79+qhduzZevnyJyZMnIyEhAefOnUNWVhY8PDxQtWpV+QLs2NhY7NixA19//TVsbGze2z+pVApDQ0MY91kLiUblvEVRTK4u7/n+IvpopGZyoX5lYWfGn6+VhVQqRVULY6Slpb13AoLzzm9RUVHBli1bcPHiRdSpUwfjxo3D4sWL37mPhoYGpkyZAjc3N7Ro0QKqqqrYsmULAEBHRwcnTpxAtWrV0KNHD7i6umLIkCF48eKFqGaGiIiIPkSinwn60HEmqHLhTFDlwpmgyoMzQZUHZ4KIiIiI3oMhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiERJraI7QMWz/Mvm0NHTr+hu0H90PjaportApehuclZFd4FKyXBTh4ruApUSQSh+LWeCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJTUKroDZen48ePw8/NDSkoKjIyMKro7onL46AXsP3AWqakZqFrVHP37tYGLczWltbfvPMKWbccQF5eEl69yYGZqiJZ+nujQrrFCXcT5W/hrZxgSElJgYWGMT3v6oGF9l/IYjuj9ffQC9u0/i9S0DNhUMcfAgKLPZ8SFWzh87BIexsYjNycXNlXN0bN7c7jXdZLX5ObmYfe+0zgRfhUpqemwtjJF314t4eHmpPSYVHoiwq8gPPQCMqSZMLcyRYduPrB3slFa+/DBE/y99yQSE1KQk5MDI2MDNPByg7dvPYW602GXcP7UVaSlSqGjq43abjXQ2r8Z1NUr9X8xH4R1209i5cajiE+SwtnBCvPG9kQTj6LfR6cv3cXMFTtxO/oZLM0M8WVAKwT2aCbf/sfu09h2IAK3HsQBANycbTFteGfUq21X5mOpCJX6O9Tb2xtxcXEwNDQsVn1MTAwcHBxw+fJleHh4lG3nKrGz525iw6bDCBzYHjVr2OJY6CUs/mELFi34H8xMC58LTU11tGndANVsLaCpoY7bdx8hKPgANDXV0fKfH7Z37z3GT6t24JMePmhQzxkXLt3GTyt3YsbUgajuVLW8hygqZ87dxO8bD2PIwPZwrmmLI6GX8O33W7BkofLzGXU7FnVrO6DPJ77Q0dFC2MkrWLx0G+bOGgwHOysAwLbtYQg/fQ3DhnRCFWtTXL32AD+s+AuzZwyS11Dpu3b5Ng7sOg7/T1qimkMVnD99DRt+3YUvJw+EkbFBoXoNDXU0bu4BK2szqGuqI/bBU+z58wg0NNTQwNsNAHDlYhSO7AtHtz5tYetgjaSEVOzcfAgA0KG7bzmOTnx2HbmEGct2YNHXn6KRmyN+33kKfcavQvimqbCxMilU//BpEvp9tRr9u3hh5TcDEXH1ASYt/hOmxnro7OcBADh16S66t6mPhnUdoKWhjp82HEGvsStxcuMUWFsYle8Ay0GlvhymoaEBKysrSCSScn/uV69elftzfigOHDoH3xYe8PPxRNUqZhgQ0BamJgY4euyS0np7Oyt4N6kNm6rmMDc3QjPvuqhb1xG3bz+S1xz8OwJ1ajugi39TVKlihi7+TVHL1R4H/44or2GJVsjBc/Br4YGWvvnnc9A/5/PwUeXnc1BAW3Tp5AUnxyqwtjJBn0/9YGVpgkuX78prTp6+hm6dm8LTvTosLYzRplV9uNd1RMiBc+U1LFE6ffwS6jWug/pN6sLc0hQdu/vCwEgf509dVVpvbWMBt3ousLA2g7GJIdwbuKK6sz0ePngir3kUEwdbhypwq+8CYxNDVHexQ916znjyKL6cRiVev2wORb/OTdC/izdq2lth3rieqGphjOAd4Urr1+8MR1VLY8wb1xM17a3Qv4s3+vo3wcpNx94cc/YgDOnZHHVr2qCGvSV+mNIXMpkMJy7cKa9hlauPKgT5+vpi1KhRGDt2LIyNjWFpaYlff/0VmZmZGDx4MPT19eHk5IQDBw4AyL8cJpFIkJqaCgAYMmQI3Nzc8PLlSwBATk4O6tevj4CAAACAg4MDAMDT0xMSiQS+vr7y5x07dqxCX7p164bAwED5Y3t7e8ybNw+BgYEwNDTEsGHDAACnT59GixYtoK2tDVtbW4wePRqZmZll9ApVvNzcPETHxKFOHQeF9jp1HHH33uNiHSPm4TPcvfsYLi5vLrfcu/cEdes4KtS51S3+MenfeX0+3QqcT7c6jrhTzNdeJhOQnf0Kerpab46bk1foUom6uhpu331UcHcqJbm5eYh7HA8nZ8XLGtWdqyE25mmxjhH3OAGPYp7Cvvqby2d2DlUR9ygBjx8+AwAkJ6bizs0Y1KzlUNRhqBS8ysnFlduP4NtIcUmAb2MXnL8WrXSfC9dj4NtYsd6vsQuuRMUiJzdP6T4vsl8hN1cGYwOd0un4B+ajuxy2fv16TJw4EREREdi6dStGjBiBXbt2oXv37pg6dSqWLl2KAQMGIDY2ttC+K1asgLu7OyZPnoylS5dixowZSExMxMqVKwEAERERaNSoEY4cOYLatWtDQ0OjRH1bvHgxZsyYgenTpwMArl27hnbt2mHu3Ln47bff8Pz5c3z55Zf48ssvERQUpPQYL1++lIc0AJBKpSXqQ0VLT8+CTCbA0EBPod3QQBepaRnv3HfUuBVIT89CXp4MPbo1h5+Pp3xbaloGDA10Cx0zLa3yBsoPgfT1+TQscD4NdZH2nvP5WsjBs3j5MgdNGteSt7nVdUTIwXNwca4GSwtjXL8ZjYuX70AmE0q1//RGVuYLyGQC9PQV/zPT1ddFhvThO/dd8s0aZGa8gEwmg1/7JqjfpK58W916zsjMyMJvP26FIAAymQwNm7qhRetGZTIOypecmom8PBnMTfQV2s2N9ZGQnK50n4QkKcyNC9Sb6CM3T4bk1AxYmhW+vD135R5YmRuiRUPn0uv8B+SjC0Hu7u7ykDFlyhR8++23MDMzk8+8zJw5E6tWrcLVq4Wnd/X09LBhwwb4+PhAX18f33//PY4ePSpfM2Rubg4AMDU1hZVVydcltGzZEhMmTJA/HjhwIPr16yefRapRowZWrFgBHx8frFq1ClpaWoWOsXDhQsyePbvEz/2hKXQFUhAgwbsvS86YOhAvs1/h3v0n2PpnKCwtTeDdpHaRBxUEvOeIVGoKvNCCIBTrMvOpMzewfedJfDX2U4UQOyigDdYE7cdXk3+BRAJYWhjDp7k7wk5eKe2eU0GFTptQ+P1awNBRvfDqZQ4ePYzD4X3hMDEzglu9/BmF6HuPcOJIBPw/aQmbatZISkzFgZ3HcdzgLHzbNimTIdAbBd+HAoR3/lwseK4FoYgNAH7ccAQ7D1/CzpWjoKWp/t86+oH66EKQm5ub/N+qqqowNTVF3bpvfiuxtLQEACQkJMDAoPBCPy8vL0yYMAFz587FpEmT0KJFi1LrW4MGDRQeX7x4Effu3cPGjRvlbYIgQCaTITo6Gq6uroWOMWXKFIwfP17+WCqVwtbWttT6WNb09XWgoiIpNOuTlp4FQ0PdIvbKZ2FuBACwtbVAmjQTO3adkIcgI0O9QjMP0vRMGLznmPTfGPxzPtNSC7z20iwYGLz7tT9z7iZ+XbcPY0b2QN3aipdGDAx08dWYT/HqVS4yMrJgbKyPzdtCYW5mVNpDoH/o6GpDRUWCDGmWQntmehZ09d99qcP4nwXwllXMkJGehdCDZ+Uh6Oj+03Bv4CqfHbKsYoacVznYs+0IWrRuDBUV/qpSFkyMdKGqqoKEJMWrBYkpGYVmh16zMDUoNEuUmJIONVUVmBT4WfrzxqNYvv4w/loxErWrV96bTz6qNUEAoK6umEYlEolC2+tULJPJlO4vk8lw6tQpqKqq4u7du0prClJRUYEgKE7T5+TkFKrT1VX8JpLJZPjf//6HyMhI+deVK1dw9+5dODkpv4VRU1MTBgYGCl8fEzU1VTjYW+P6DcVr0tdvRKNGdeW34SojCAJyc95co65evWqhY167/qBEx6SSe30+rxZ87W9Eo+Y7XvtTZ25g1Zq9+HJ4N9TzqFFknYaGGkxMDJCXJ0PEhVtoUK9mqfWdFKmpqcLaxhL37yhe+rp/JxbV7KuU4EgC8t5aP5KTk1toNkKiIoEAAQAvb5YVDXU1uDvbIuz8bYX2sIhbaFhX+XqsBnXsERZxS6HteMQtuLtWg7qaqrztpw1H8UPQIWxZOhwerso/CqOy+OhC0H+1ePFiREVFISwsDIcOHVJYm/N6DVBenuICMXNzc8TFxckf5+Xl4fr16+99rnr16uHGjRuoXr16oa+Srjf6mHRo1xjHwyIRdiIST54mYsOmw0hKSkMrv/zb3bf+GYpfft0jrz985AIuXb6DZ8+S8exZMsJOXsH+g+fQ1LuOvKZdm0a4dv0B9oacxtOnidgbcho3bsagfVuuOyhrndo3RmhYJEL/OZ+/bzyMxKQ0tG6Zfz43bwvFytVvzmd+ANqD/n1boYZTVaSmZiA1NQNZWdnymnv3nyDiwi3EJ6Tg1u1YfPv9FgiCgM4dvcp9fGLi7VsPl85ex6Vz1/E8PgkHdh5HWko6Gv5zu/vhfeHYvvGgvP5ceCRuXb+PpOcpSHqegkvnbuBU6EW4NXizuNa5tiPOn7qKa5duIyUpDfduP8SxA6fhUtsJKiqi+y+mXA3v64eNe85g094zuBPzDDOW7cDj+BQM6p7/uT/zVu7ByNl/yOsHdW+Gx89SMGP5DtyJeYZNe89g096z+KJfS3nNjxuO4Ntf92HZtH6wtTZFfJIU8UlSZGS9LPT8lcFHdznsv4iMjMTMmTPx119/oWnTpli+fDnGjBkDHx8fODo6wsLCAtra2jh48CBsbGygpaUFQ0NDtGzZEuPHj0dISAicnJywdOlS+R1n7zJp0iQ0adIEI0eOxLBhw6Crq4uoqCgcPnwYP/74Y9kPuII0aVwL6RlZ2Lk7PP/D9aqa4+vxfWD2z6K71NQMJCalyesFQcC2v47j+fNUqKiqwMLCCL0/9ZN/RhAA1Kxhgy9HdMef28Pw144wWFoY48sR3fkZQeXA65/zuWN3OFJTM2Bb1RyTxveB+evzmZaBxOQ35/Po8UvIy5Mh6PdDCPr9kLy9RTM3jBjWGUD+nS3btoch4XkKNDU14OlWHV983gW6uoXXyVHpqevpjBeZ2Th+6BzSpZmwsDZF/8+7wcgkf8Y5XZqJtJQ3l0sEmYAjIaeQkpwGFRUVmJgaoY1/MzTwerMswadNY0gAHD1wCtK0DOjq6sC5tiNadfIu7+GJTrfW9ZCclonv1x1CfFIaXBytsfn74bC1zv+MoPgkKZ7Ep8jr7aqYYtP3/8OM5TsRtP0krMwMMX9cT/lnBAFA8PZwvMrJw9Cp6xSea8LQ9pj4WcdyGVd5kggFr/N8wHx9feHh4YFly5bJ2+zt7TF27FiFW9glEgl27twJIyMj+SdGa2lpoX79+mjWrBlWr14tr+3Rowfi4+Nx4sQJqKqqYu3atZgzZw6ePHmC5s2b4/jx48jJycGYMWOwdetWqKmpYdy4cTh79iyMjIwQHBxcZD8A4Pz585g2bRrOnDkDQRDg5OSE3r17Y+rUqcUas1QqhaGhIX4Pvw0dPeXXeenjocrlEZXK3eSs9xfRR2F4E97SX1lIpVLYWBojLS3tvUtKPqoQJEYMQZULQ1DlwhBUeTAEVR4lCUG8YEtERESixBBEREREosQQRERERKLEEERERESixBBEREREosQQRERERKLEEERERESixBBEREREosQQRERERKLEEERERESixBBEREREosQQRERERKLEEERERESixBBEREREosQQRERERKLEEERERESixBBEREREosQQRERERKLEEERERESipFacohUrVhT7gKNHj/7XnSEiIiIqL8UKQUuXLi3WwSQSCUMQERERfRSKFYKio6PLuh9ERERE5epfrwl69eoVbt++jdzc3NLsDxEREVG5KHEIysrKwtChQ6Gjo4PatWsjNjYWQP5aoG+//bbUO0hERERUFkocgqZMmYIrV67g+PHj0NLSkre3bt0aW7duLdXOEREREZWVYq0JetuuXbuwdetWNGnSBBKJRN5eq1Yt3L9/v1Q7R0RERFRWSjwT9Pz5c1hYWBRqz8zMVAhFRERERB+yEoeghg0bIiQkRP74dfBZs2YNvLy8Sq9nRERERGWoxJfDFi5ciPbt2+PmzZvIzc3F8uXLcePGDZw5cwZhYWFl0UciIiKiUlfimSBvb2+cOnUKWVlZcHJywt9//w1LS0ucOXMG9evXL4s+EhEREZW6Es8EAUDdunWxfv360u4LERERUbn5VyEoLy8PO3fuRFRUFCQSCVxdXdG1a1eoqf2rwxERERGVuxKnluvXr6Nr16549uwZnJ2dAQB37tyBubk59uzZg7p165Z6J4mIiIhKW4nXBH322WeoXbs2Hj9+jEuXLuHSpUt49OgR3Nzc8Pnnn5dFH4mIiIhKXYlngq5cuYILFy7A2NhY3mZsbIz58+ejYcOGpdo5IiIiorJS4pkgZ2dnxMfHF2pPSEhA9erVS6VTRERERGWtWCFIKpXKvxYsWIDRo0fjr7/+wuPHj/H48WP89ddfGDt2LBYtWlTW/SUiIiIqFcW6HGZkZKTwJzEEQUCvXr3kbYIgAAA6d+6MvLy8MugmERERUekqVggKDQ0t634QERERlatihSAfH5+y7gcRERFRufrXn26YlZWF2NhYvHr1SqHdzc3tP3eKiIiIqKyVOAQ9f/4cgwcPxoEDB5Ru55ogIiIi+hiU+Bb5sWPHIiUlBWfPnoW2tjYOHjyI9evXo0aNGtizZ09Z9JGIiIio1JV4JujYsWPYvXs3GjZsCBUVFdjZ2aFNmzYwMDDAwoUL0alTp7LoJxEREVGpKvFMUGZmJiwsLAAAJiYmeP78OYD8vyx/6dKl0u0dERERURn5V58Yffv2bQCAh4cHVq9ejSdPnuCXX36BtbV1qXeQiIiIqCyU+HLY2LFjERcXBwCYNWsW2rVrh40bN0JDQwPBwcGl3T8iIiKiMlHiEBQQECD/t6enJ2JiYnDr1i1Uq1YNZmZmpdo5IiIiorLyrz8n6DUdHR3Uq1evNPpCREREVG6KFYLGjx9f7AP+8MMP/7ozREREROWlWCHo8uXLxTrY239klUqXl50p9A0MKrob9B9pa6hWdBeoFPX1Gl3RXaBSMur8TxXdBSol6mrFv+eLf0CViIiIRKnEt8gTERERVQYMQURERCRKDEFEREQkSgxBREREJEoMQURERCRK/yoE/fHHH2jatCmqVKmChw8fAgCWLVuG3bt3l2rniIiIiMpKiUPQqlWrMH78eHTs2BGpqanIy8sDABgZGWHZsmWl3T8iIiKiMlHiEPTjjz9izZo1mDZtGlRV33zwW4MGDXDt2rVS7RwRERFRWSlxCIqOjoanp2ehdk1NTWRmZpZKp4iIiIjKWolDkIODAyIjIwu1HzhwALVq1SqNPhERERGVuRL/Ffmvv/4aI0eORHZ2NgRBQEREBDZv3oyFCxdi7dq1ZdFHIiIiolJX4hA0ePBg5ObmYuLEicjKykK/fv1QtWpVLF++HH369CmLPhIRERGVuhKHIAAYNmwYhg0bhsTERMhkMlhYWJR2v4iIiIjK1L8KQa+ZmZmVVj+IiIiIylWJQ5CDgwMkEkmR2x88ePCfOkRERERUHkocgsaOHavwOCcnB5cvX8bBgwfx9ddfl1a/iIiIiMpUiUPQmDFjlLb//PPPuHDhwn/uEBEREVF5KLU/oNqhQwds3769tA5HREREVKZKLQT99ddfMDExKa3DEREREZWpEl8O8/T0VFgYLQgCnj17hufPn2PlypWl2jkiIiKislLiENStWzeFxyoqKjA3N4evry9cXFxKq19EREREZapEISg3Nxf29vZo164drKysyqpPRERERGWuRGuC1NTUMGLECLx8+bKs+kNERERULkq8MLpx48a4fPlyWfSFiIiIqNyUeE3QF198ga+++gqPHz9G/fr1oaurq7Ddzc2t1DpHREREVFaKHYKGDBmCZcuWoXfv3gCA0aNHy7dJJBIIggCJRIK8vLzS7yURERFRKSt2CFq/fj2+/fZbREdHl2V/iIiIiMpFsUOQIAgAADs7uzLrDBEREVF5KdHC6Hf99XgiIiKij0mJFkbXrFnzvUEoOTn5P3WIiIiIqDyUKATNnj0bhoaGZdUXIiIionJTohDUp08fWFhYlFVfiIiIiMpNsdcEcT0QERERVSbFDkGv7w4jIiIiqgyKfTlMJpOVZT+IiIiIylWJ/3YYERERUWXAEERERESixBBEREREosQQRERERKLEEERERESixBBEREREosQQRERERKLEEERERESixBBEREREosQQRERERKLEEERERESixBBEREREosQQRERERKLEEERERESixBBEREREosQQRERERKKkVtEdoMrpj53hWL0lFAnJUtS0t8LML7uhkbtTkfVnI+9h3s+7cSfmGSxNDfC/vi3Rv2tT+fbeY37Cucj7hfbza+KKoEWfl8kY6I2g7Sfx88ajSEiSwtnBCnPH9kQTj6LP5+lLdzFrxU7cjn4GSzNDfBnQCoN6NJNvDzl+BcvX/43ox4nIyc2Do605RvT1w6cdGpXHcERt6CfNMap/K1iaGeLWgzhM/WE7zih5b7322act8NmnLVDN2gSP41Pw/bpD2Lo/QqGms58Hpg7vBAcbM0Q/TsS8VXsRcvxqWQ+FAKz98wR+3HAU8YlpcHG0xoLxPeHtWb3I+lMX72Lash249SAOVmaGGD2wNYb0bK5Qs+fYZSz4JQTRjxPhYGOG6SM6w9/PvayHUiE4E0Slbu+xy5jz0y58OaAN9q+ZgIZujgic9CuexKcorX8Ul4TBk9agoZsj9q+ZgJH922D2ip04EHZFXrN67mBE7Jgt//o7eCJUVVXQ0dejnEYlXruOXMKMZTswNrAtjqyfiMbuTug7fhUeP0tWWv/waRL6fbUajd2dcGT9RIwZ1AbTlm7HvtBIeY2RgQ7GDmqLkDXjcPyPSejTqTHGzN+E0LNR5TQqcereph4WjO+J74MOwaf/tzgTeR/bln8BG0tjpfVDejbDjC86Y9Ga/fDqMx/frt6PxRN7oX3zOvKahnUdsG7BYGw7cB7N+32LbQfOI2jhUNSvbVdewxKtHX9fxNQftuOrwe0QtmEyvDyc0GvMSjwq6r35JBG9xq6Cl4cTwjZMxvjB7TB5yV/Yc+yyvCbi6gMMmRqEXh0a4uSmyejVoSEGT/kNF67HlNOoyhdDUAG+vr4YO3ZsRXfjo7Z223H06tgYffyboLq9JWaN6g5rcyNs2H1Kaf2G3adRxcIIs0Z1R3V7S/Txb4JPOzbCr1tC5TVGBrqwMDWQf528cAfamuro5Fs5fzv5kPyyORT9OjdB/y7eqGlvhXnjeqKqhTGCd4Qrrf99ZzhsLI0xb1xP1LS3Qv8u3ujr3wQrNx2T1zStVwMdfd1R094K9jbm+Ly3L2o5VcG5Kw/Ka1ii9EW/ltiw+wz+2H0Gd2LiMfWH7XgSn4IhnzRXWt+7YyOs33kKOw9fwsMnSdhx+CI27DmDMQPbyGuG9/XF8YhbWBr8N+4+jMfS4L8Rdv42RvT1K69hidbKTcfQv6sXBnbzhrODFRZ+9QmqWhpj3V8nldav2xEOGytjLPzqEzg7WGFgN28EdGmCnzYcldf8svk4fBu5YPzgdqhpb4Xxg9vBp6EzVm0OVXrMjx1DUAkJgoDc3NyK7sYH61VOLq7feYzmDZ0V2ps3dMbFIn6TuHwjplB9i4YuuHb7EXJy85Tusy3kHDq39ISOtmap9JuUe5WTi6u3H8G3kYtCu09jF1y4Fq10nwvXY+DTWLHer7ELrkTFKj2fgiDgxPnbuBebAC/Poi+x0X+jrqYKDxdbHDunONsWei4KjdwclO6joa6G7Fc5Cm3ZL3NQr7Yd1FTz//toVNcBx87eUqg5diYKjdwcS7H3VNCrnFxE3nqElo1dFdr9Grsi4qry9+b5a9HwK1DfqkktXL755r0ZcS0aLZsovn9berki4mrl/AWFIegtgYGBCAsLw/LlyyGRSCCRSBAcHAyJRIJDhw6hQYMG0NTUxMmTJxEYGIhu3bop7D927Fj4+vrKHwuCgO+++w6Ojo7Q1taGu7s7/vrrr3f24eXLl5BKpQpfH5OUtEzk5clgbqKv0G5urI/EZOVjeZ6cDnPjAvUm+sjNkyElLaNQfWTUQ9yOjkNv/yal13FSKjm16POZkJyudJ+EJGmR5zM59c35lGa8gEPLCbBpPg79J6zGgvE94VMgbFHpMTXSg5qaKp4XOG/Pk9JhYWqgdJ9jZ6MwoKs33F1sAQAertUQ0LkJNNTVYGqkBwCwMDUofMzkdFiY6hc6HpWepNQM5e9NU30kJCn/WZuQJIW5qfL3ZtI/782EJGnhY5roIyFJ+fv9Y8eF0W9Zvnw57ty5gzp16mDOnDkAgBs3bgAAJk6ciCVLlsDR0RFGRkbFOt706dOxY8cOrFq1CjVq1MCJEyfQv39/mJubw8fHR+k+CxcuxOzZs0tlPBVLovBIAACJRGkllGwTBEHpcQBga8g5ODtYw8OVaw7KTcHzA0HJmSmyHK9Pp+StDXo6mji2fhIyX7zEyQt3MGvFLthVNUPTejVKqdOkjPyt9Q+JRPLW+03R4t8OwsLUAIeDJkACICE5HZv3ncOYQW2QJ5O9dUzF/SWSws9DZaPwe01QeJ8Vqi/wWMj/6QzJW1sK7i8I7/7x/TFjCHqLoaEhNDQ0oKOjAysrKwDArVv507xz5sxBmzZt3rW7gszMTPzwww84duwYvLy8AACOjo4IDw/H6tWriwxBU6ZMwfjx4+WPpVIpbG1t/+2Qyp2xoS5UVVXwvMCsT2JKOsyMlf9maG6ir6Q+A2qqKjA21FVof5H9CvuOXca4Ie1Lt+OklInRP+czqfD5Kfjb4msWpgaFZokSU9ILnU8VFRU42JoDAOrUtMGdmGdY8fthhqAykpSagdzcvEIzNGYmeoVmcl7LfpmDUXM3YtyCzbAwNcCzxDQEdm8KacYLJKVmAsifOSg4k2RmrF/kMal0mBrpQVVVpdAMTWLye96bSurVVFVgYqT7Vk3hn99FHfNjx8thxdSgQYMS1d+8eRPZ2dlo06YN9PT05F+///477t8v+nZUTU1NGBgYKHx9TDTU1VCnpg3CL9xRaA+/cAf169gr3ceztn2h+pPnb6Ousy3U1VQV2veFRuJlTi66tynZ+aB/R0NdDW7Otgg7f1uh/UTELTSoq3wdSYM69jgRobhG5HjELbi7Vit0Pt8mCMCrV1xvV1ZycvMQeesR/Aqs1/Jt5FLkGpLXcvNkeJqQCplMQI+29fF3+A357E/EtehCx2zZxKXSriH5UGioq8HDxRah5wq/14pa49WwrgOOF3hvHjsXBc9ab96bjeo6FDrmsbO3Ku0aL4agYtLVVZyRUFFRKTQFnJPzZgGh7J+p4pCQEERGRsq/bt68+d51QR+7z3r5YmvIWWwLOYd7MfGY89NOPE1IQUAXbwDAol/3Yfz8jfL6/l298SQ+BXN/2oV7MfHYFnIO2/afw+d9Ct9dsi3kLNo2q1tohojKzvC+fti45ww27T2DOzHPMGPZDjyOT8Gg7vmf+zNv5R58OfsPef3A7s3w6FkKZi7fgTsxz7Bp7xls2nsWX/RrKa9Zvv5vhEXcQsyTRNyNiccvm4/hzwMR6Nme4bYsrdx0DAO6eiOgcxPUtLfE/HE9YGNlgqDt+XcTzRzZBau+GSCvd6pmgV4dGsLR1hz1atnht/mD4epYBXNW7pHXrN5yHH6NXTBmYGvUsLPEmIGt4dPIpdLeTfQh+aJfS/yx+zQ27DmD29HPMPWH7Xj8LBmD//ncn9k/7cbwWb/L64f0aIZHccmYtnQ7bkc/w4Y9Z7Bh9xl82b+VvOZ/fXwReu4Wlq0/jDsxz7Bs/WGERdyqtHf78XJYARoaGsjLU35H0tvMzc1x/fp1hbbIyEioq6sDAGrVqgVNTU3ExsYWeemrsurc0hOpaZlY/vshPE+SoqaDNYIWfQ4bKxMA+dPnTxLefGaQrbUpghYNw9yfduGPXeGwMDXErNHd0cFH8fb3B48ScP5aNP5YMrxcxyN23VrXQ0paJn5YdwjxSfkfyLbp++GwtX7rfL71GVB2VUyx6fv/YebynQjafhKWZoaYP64n/P085DVZ2a8wafGfiEtIhZamOqrbWeDnbwaiW+t65T08Udl5+BJMDHUx8bMOsDQzQNT9OPQeuxKPnuWfP0szA/n7FABUVSQYGdAS1e0skZubh5MX7qDdZ9/jUdybz6GJuBqNodOCMG2EP6YO90f040QMmboOF288LPfxiU2PtvWRnJaJ79YeQHyiFK5O1ti67AtU++e9GZ8oVfg8L7uqZti2bASmLt2OtX+ehJW5Ib6d8Am6tPSU1zR2d8Rv8wdj/qp9WPDLPjjYmGHdgiFoUMRM/sdOIhS1Ik6kPv/8c0RGRmLbtm3Q09PD1atX0apVK6SkpCgsiD506BA6dOiA4OBgeHl5YcOGDVi2bBk8PT1x/PhxAPkLo3/55Rd8//33aNasGaRSKU6fPg09PT0MGjSoWP2RSqUwNDTE3UeJ0P/ILo1RYdoaRV8Ooo+Ppdfoiu4ClZKU8z9VdBeolEilUliaGiItLe29S0p4OayACRMmQFVVFbVq1YK5uTliY2OV1rVr1w4zZszAxIkT0bBhQ6Snp2PgwIEKNXPnzsXMmTOxcOFCuLq6ol27dti7dy8cHJRfryUiIqLyw5mgDxxngioXzgRVLpwJqjw4E1R5cCaIiIiI6D0YgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiUGIKIiIhIlBiCiIiISJQYgoiIiEiU1Cq6A1Q884/eg4aOXkV3g/6jJZ1dK7oLVIquH1pc0V2gUrLsxP2K7gKVkuzM9GLXciaIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiIiIREmtojtAlVMzB2O0rGEGAy01PJO+xI5rz/AgKeu9+zmYaGNUcwfESbOxOPSBwjZtdRV0qmUJtyr60FFXRVJWDnZfe4ab8RllNQz6x7rtJ7Fy41HEJ0nh7GCFeWN7oomHU5H1py/dxcwVO3E7+hkszQzxZUArBPZoJt/+x+7T2HYgArcexAEA3JxtMW14Z9SrbVfmYxG7LXtPI+jP43ienI7qdpaYNLwL6td1VFr7PEmKxb/uxc17T/DwSSICujbF5BFdFWr+2n8Oe45cxL2HzwAAtapXxZjBHVDXpVqZj4WA86eu4PTxC0iXZsLCyhTtuvrAztFGaW3sgyc4EnISiQkpyHmVA0NjA9T3coOXTz15TV5eHsKPnseVCzchTcuAmbkxWvs3R3UX+3IaUfniTBCVOs+qBujuZoW/bz/H4tD7uJ+UheHe1WCsrf7O/bTUVNC/vg3uPM8stE1VIsEXTe1hoqOOoHOPMP/wPWy9/BSpL3LKahj0j11HLmHGsh0YG9gWR9dPRBN3J/QZvwqPnyUrrX/4NAn9vlqNJu5OOLp+IsYOaoNpS7djb2ikvObUpbvo3qY+dvw0Cvt/HQ8bS2P0GrsScQmp5TMokTpwPBLf/rIHw/q2wp8rx6JeHQcMn/4b4hJSlNa/ysmFsZEehvVpCWdHa6U156/eR0c/D6z77n/YsPRLWFkY4/OpaxCfmFaWQyEA1y/fxsHdx9G8VSP8b3wAqjlUxcY1u5CWIlVar66hjoZNPRA48lOMnDQILdo0RujBU7h45qq85tiB07h45io6dPfDyIkDUd/bDVuD9iDucUJ5DatciS4E5eXlQSaTVXQ3KjXf6qY4G5OKsw9TEZ/+CjuvPUPKi1w0dTB+5369Pavg4uM0xCQXnjFqYmcEHXVVrD0bi+jkF0h5kYMHSVl4Kn1ZVsOgf/yyORT9OjdB/y7eqGlvhXnjeqKqhTGCd4QrrV+/MxxVLY0xb1xP1LS3Qv8u3ujr3wQrNx17c8zZgzCkZ3PUrWmDGvaW+GFKX8hkMpy4cKe8hiVKv+84gR7tGuKTDo3hVM0Sk0d0hZW5EbbsO6O0vqqVCaaM6IqubRpAT1dLac2iyf3Qp7M3XJyqwrGaBWaP/QQyQcDZy3fLcigE4OyJS/BsVAf1mtSFuaUp2nfzhaGRPs6fvqq03trGAnXrucDCygxGJoZwq+8KJ2d7xEY/kddcvRiFZq0aoYarA4xNjdDQ2x1OzvY4E3axvIZVrio0BNnb22PZsmUKbR4eHvjmm28AABKJBGvXrkX37t2ho6ODGjVqYM+ePfLa48ePQyKRICQkBO7u7tDS0kLjxo1x7do1eU1wcDCMjIywb98+1KpVC5qamnj48CFSUlIwcOBAGBsbQ0dHBx06dMDdu/lv2rS0NGhra+PgwYMKfduxYwd0dXWRkZF/+eXJkyfo3bs3jI2NYWpqiq5duyImJkZeHxgYiG7dumHJkiWwtraGqakpRo4ciZycyjt7oSqRwNZIG7cTFC9R3Y7PgIOpTpH7Na5mBDNdDRy8pfy3jTrW+ohJzsKn7taY18EZk1s5oU1NM0hKtfdU0KucXFy5/Qi+jVwU2n0bu+D8tWil+1y4HgPfxor1fo1dcCUqFjm5eUr3eZH9Crm5MhgbFP09Qv9NTk4ubt59Au/6NRXavevXxJWbD0vtebJfvkJubh4M9Xkuy1Jebh6ePo6Hk7PiJWRH52p4HPO0WMeIe5yARzFPFS6f5eXmQU1dcaWMuroaYqOLd8yPzQc/EzR79mz06tULV69eRceOHREQEIDkZMVp+K+//hpLlizB+fPnYWFhgS5duigEjaysLCxcuBBr167FjRs3YGFhgcDAQFy4cAF79uzBmTNnIAgCOnbsiJycHBgaGqJTp07YuHGjwvNs2rQJXbt2hZ6eHrKysuDn5wc9PT2cOHEC4eHh0NPTQ/v27fHq1Sv5PqGhobh//z5CQ0Oxfv16BAcHIzg4uMjxvnz5ElKpVOHrY6KrqQpVFQmkL3MV2tNf5kJfU/kSNHNdDXSubYnfLzyGTFB+XFNdDbhXNYCKRIJfzjzEoVvP4VfDFG2dzUt7CPSW5NRM5OXJYG6ir9BubqyPhOR0pfskJElhblyg3kQfuXkyJKcqX781d+UeWJkbokVD59LpOBWSIs1EnkwGUyPFc2NqpIfEFOXn8t9Yum4/LEwN4VWvRqkdkwrLynwBQSZAT08xbOrp6SIj/d3rL3+YswbzJq7AmmWb0LCpO+o1qSvf5uRsh7NhF5H0PAWCTMD92w9x68Z9ZEgLL1OoDD74hdGBgYHo27cvAGDBggX48ccfERERgfbt28trZs2ahTZt2gAA1q9fDxsbG+zcuRO9evUCAOTk5GDlypVwd3cHANy9exd79uzBqVOn4O3tDQDYuHEjbG1tsWvXLnz66acICAjAwIEDkZWVBR0dHUilUoSEhGD79u0AgC1btkBFRQVr166FRJI/HxEUFAQjIyMcP34cbdu2BQAYGxvjp59+gqqqKlxcXNCpUyccPXoUw4YNUzrehQsXYvbs2aX9Mn6wJAAGNrTBgagEPM94VXSdBMh4mYstl59CAPA4NRuG2upoWcMUh24/L7f+itXr7/HXBAjvnIUrUA5BKGIDgB83HMHOw5ewc+UoaGm+e90Y/XfKzk1pzaiu2xaK/aGRCFo8HJoaPJflouD5RBG/Sb5l8MheePUqB48fxuFoSDhMTI1Qt17+7G37br7Yu+0Ifl60HpAAJqZG8GhYG5Hnb5RF7yvcBx+C3Nzc5P/W1dWFvr4+EhIUL5l4eXnJ/21iYgJnZ2dERUXJ2zQ0NBSOExUVBTU1NTRu3FjeZmpqqrBfp06doKamhj179qBPnz7Yvn079PX15eHm4sWLuHfvHvT1FX+rys7Oxv379+WPa9euDVVVVflja2trhct1BU2ZMgXjx4+XP5ZKpbC1tS2y/kOT+TIPeTIBBgVmffQ11ZBeYHYIALTUVVDNWBtVDbXQ0z1/4aVEAqhIJPihay2sOvUQdxMzIc3ORZ5M8e0dn/4ShlrqUJVIkCe8/41PJWdipAtVVRUkJCnOSCamZBSaHXrNwtSg0CxRYko61FRVYGKoq9D+88ajWL7+MP5aMRK1q1ct3c6TAmMDXaiqqBSa9UlOy4CpsfJzWRJBfx7Hmi3HsObbz+HsWOU/H4/eTUdXGxIVSaFZn8yMLOi951KksakhAMDS2gyZ6VkI+/usPATp6umgz5AuyM3JRVZWNvQNdHEkJBzGJgZlM5AKVqEhSEVFBUKB/7wKrpdRV1f8bUIikRRrYfPbv7lqa2srPC74nG+3v67T0NDAJ598gk2bNqFPnz7YtGkTevfuDTW1/JdMJpOhfv36hS6ZAYC5+ZtLNCXtv6amJjQ1Nd87vg9VniDgUeoLOFvo4Wrcmx+2zha6uBZXeMo9O0eGb4/cU2hr5miCGua6CDr3CElZ+bND0UlZqGdjCAkgD0IWehpIe5HDAFSGNNTV4O5si7Dzt9HJ113eHhZxC+2b11W6T4M69vg7/LpC2/GIW3B3rQZ1tTe/EPy04SiWBh/C1mUj4OHK26nLmrq6GmrVqIozl+6iddM35+7MpTvw86r9n4697s/j+HXTUaxe8Bnq1Px4fmn7mKmqqaKKjSUe3HkI17rV5e0P7sTCuXbRH19RkAABuUrW6qmpq8HAUA95eXmIunoXtT1qKtn741ehIcjc3BxxcXHyx1KpFNHRyhdbvsvZs2dRrVr+D9GUlBTcuXMHLi4uRdbXqlULubm5OHfunPxyWFJSEu7cuQNXV1d5XUBAANq2bYsbN24gNDQUc+fOlW+rV68etm7dCgsLCxgYVM6E/G8dv5eE/g2qIjb1BWKSs+BtbwJjHXWcis6/Dde/lgUMtdWx8eITCADi0hXv8Ep/mYucPJlCe3h0Mpo7mqCHmxVOPEiGua4G2tQ0R9j9pPIcmigN7+uHkbP/gIeLLRrUdcAfu07jcXwKBnXP/9yfeSv3IO55Gn6eNQAAMKh7M6z76yRmLN+BAV29ceFaNDbtPYtf5gySH/PHDUew6NcQrJo9CLbWpoj/Z6ZJV1sTejof7y8BH7qBPVpgyuItqF3TBu6udvhr/znEJaSid6f82fSl6/YjITENCyf2le9z637+nUNZL14hJS0Tt+4/gbqaGpzsLAHkXwL78fdD+G5SP1S1NEZicv651NHWhI42z2VZatKiHnZuPogqNpawsbfGxbPXkJaSjgZe+Vc+joSEIz0tA9375S8fiQiPhKGxPswsTAAAsdFPceb4RTRq5iE/5uOHcUhPy4BVVXNI0zIQdugsBEFAU78G5T6+8lChIahly5YIDg5G586dYWxsjBkzZihcOiquOXPmwNTUFJaWlpg2bRrMzMzQrVu3Iutr1KiBrl27YtiwYVi9ejX09fUxefJkVK1aFV27vvkgMB8fH1haWiIgIAD29vZo0qSJfFtAQAAWL16Mrl27Ys6cObCxsUFsbCx27NiBr7/+GjY2yj+sSgwuP5FCV0MV7ZzNYailhjjpS6w+HYuUfz7Tx0BL7b2fGVRQ6otcrDr9EN3rWmFSSyekvchF2P0kHLmTWBZDoLd0a10PyWmZ+H7dIcQnpcHF0Rqbvx8OW+v8H6TxSVI8iX/zOTN2VUyx6fv/YcbynQjafhJWZoaYP64nOvt5yGuCt4fjVU4ehk5dp/BcE4a2x8TPOpbLuMSog68H0tKz8MvGI3ieLEUNOyusmjcUVSzzP74iMVmKuOepCvt88sUy+b9v3n2MkNDLqGJpjL9/nwoA2LLvDHJy8jBu3h8K+43o3wYjB7Qt0/GIXR1PZ7zIykbY4XPIkGbCwtoUAZ91g9E/l64ypJlIS30zAy8IAo7uP4XU5DSoqKjA2NQIrTo1Q4Mmb5aL5Obm4djB00hJSoOGhjpquDqge7/20NJW/hEJH7sKDUFTpkzBgwcP4O/vD0NDQ8ydO/dfzQR9++23GDNmDO7evQt3d3fs2bMHGhoa79wnKCgIY8aMgb+/P169eoUWLVpg//79CpevJBIJ+vbti8WLF2PmzJkK++vo6ODEiROYNGkSevTogfT0dFStWhWtWrXizBCA8OgUhEcr/wC2TZfefavlwVvPcfBW4cXOMckvsDSs5N8f9N8N6dkcQ3o2V7rtxxn9C7V516uBo+snFnm8izu/Ka2uUQn16eyNPp29lW6bP6FPobbrhxa/83ivwxBVjIZN3dGwqbvSbd36tlN43Li5Jxo393zn8eydbDBy4qB31lQmEqGoBTIfgePHj8PPzw8pKSkwMjKq6O6UCalUCkNDQwQGn4WGjl5Fd4f+oyWdXd9fRB+NBH5YZ6Xx5/XK+Tk4YpSdmY7ZXTyRlpb23kmJD/5zgoiIiIjKAkMQERERidIH/zlB7+Lr61vk7e5ERERE78KZICIiIhIlhiAiIiISJYYgIiIiEiWGICIiIhIlhiAiIiISJYYgIiIiEiWGICIiIhIlhiAiIiISJYYgIiIiEiWGICIiIhIlhiAiIiISJYYgIiIiEiWGICIiIhIlhiAiIiISJYYgIiIiEiWGICIiIhIlhiAiIiISJYYgIiIiEiWGICIiIhIlhiAiIiISJYYgIiIiEiWGICIiIhIlhiAiIiISJYYgIiIiEiWGICIiIhIlhiAiIiISJYYgIiIiEiWGICIiIhIlhiAiIiISJYYgIiIiEiWGICIiIhIlhiAiIiISJYYgIiIiEiWGICIiIhIlhiAiIiISJYYgIiIiEiWGICIiIhIlhiAiIiISJYYgIiIiEiWGICIiIhIlhiAiIiISJYYgIiIiEiWGICIiIhIlhiAiIiISJYYgIiIiEiWGICIiIhIlhiAiIiISJbWK7gC9myAIAIBXLzIruCdUGqRSaUV3gUpRevrLiu4ClZLszPSK7gKVkpdZGQDe/P/5LhKhOFVUYR4/fgxbW9uK7gYREdFH5dGjR7CxsXlnDUPQB04mk+Hp06fQ19eHRCKp6O6UGalUCltbWzx69AgGBgYV3R36D3guKxeez8pDLOdSEASkp6ejSpUqUFF596ofXg77wKmoqLw3yVYmBgYGlfrNKSY8l5ULz2flIYZzaWhoWKw6LowmIiIiUWIIIiIiIlFiCKIPgqamJmbNmgVNTc2K7gr9RzyXlQvPZ+XBc1kYF0YTERGRKHEmiIiIiESJIYiIiIhEiSGIiIiIRIkhiEqVIAj4/PPPYWJiAolEgsjIyHfWx8TEFKuOKs7x48chkUiQmppa0V0hIipVDEFUqg4ePIjg4GDs27cPcXFxqFOnTkV3if4jb29vxMXFFfvDxxhsP2y+vr4YO3ZsRXeD6IPAT4ymUnX//n1YW1vD29u7ortCpURDQwNWVlYV8tyvXr2ChoZGhTy3WAmCgLy8PKip8b+HyiQvLw8SieS9f0ZCbPhqUKkJDAzEqFGjEBsbC4lEAnt7exw8eBDNmjWDkZERTE1N4e/vj/v37xd5jJSUFAQEBMDc3Bza2tqoUaMGgoKC5NufPHmC3r17w9jYGKampujatStiYmLKYXSVh6+vL0aNGoWxY8fC2NgYlpaW+PXXX5GZmYnBgwdDX18fTk5OOHDgAIDCl8OGDBkCNzc3vHyZ/xfUc3JyUL9+fQQEBAAAHBwcAACenp6QSCTw9fWVP2/BGYhu3bohMDBQ/tje3h7z5s1DYGAgDA0NMWzYMADA6dOn0aJFC2hra8PW1hajR49GZmZmGb1ClVdgYCDCwsKwfPlySCQSSCQSBAcHQyKR4NChQ2jQoAE0NTVx8uRJBAYGolu3bgr7jx07Vn4+gfzA9N1338HR0RHa2tpwd3fHX3/9Vb6D+gjZ29tj2bJlCm0eHh745ptvAAASiQRr165F9+7doaOjgxo1amDPnj3y2tfvyZCQELi7u0NLSwuNGzfGtWvX5DXBwcEwMjLCvn37UKtWLWhqauLhw4dISUnBwIEDYWxsDB0dHXTo0AF3794FAKSlpUFbWxsHDx5U6NuOHTugq6uLjIz8v87+vp/Dr793lixZAmtra5iammLkyJHIyckpxVexdDAEUalZvnw55syZAxsbG8TFxeH8+fPIzMzE+PHjcf78eRw9ehQqKiro3r07ZDKZ0mPMmDEDN2/exIEDBxAVFYVVq1bBzMwMAJCVlQU/Pz/o6enhxIkTCA8Ph56eHtq3b49Xr16V51A/euvXr4eZmRkiIiIwatQojBgxAp9++im8vb1x6dIltGvXDgMGDEBWVlahfVesWIHMzExMnjwZQP45S0xMxMqVKwEAERERAIAjR44gLi4OO3bsKFHfFi9ejDp16uDixYuYMWMGrl27hnbt2qFHjx64evUqtm7divDwcHz55Zf/8VUQn+XLl8PLywvDhg1DXFwc4uLiYGtrCwCYOHEiFi5ciKioKLi5uRXreNOnT0dQUBBWrVqFGzduYNy4cejfvz/CwsLKchiiMHv2bPTq1QtXr15Fx44dERAQgOTkZIWar7/+GkuWLMH58+dhYWGBLl26KASNrKwsLFy4EGvXrsWNGzdgYWGBwMBAXLhwAXv27MGZM2cgCAI6duyInJwcGBoaolOnTti4caPC82zatAldu3aFnp5esX8Oh4aG4v79+wgNDcX69esRHByM4ODgMn3N/hWBqBQtXbpUsLOzK3J7QkKCAEC4du2aIAiCEB0dLQAQLl++LAiCIHTu3FkYPHiw0n1/++03wdnZWZDJZPK2ly9fCtra2sKhQ4dKbQyVnY+Pj9CsWTP549zcXEFXV1cYMGCAvC0uLk4AIJw5c0YIDQ0VAAgpKSny7adPnxbU1dWFGTNmCGpqakJYWJh8W8Fz+vbzjhkzRqGta9euwqBBg+SP7ezshG7duinUDBgwQPj8888V2k6ePCmoqKgIL168KOHoqeB5eH1+d+3apVA3aNAgoWvXrgptY8aMEXx8fARBEISMjAxBS0tLOH36tELN0KFDhb59+5ZF1ysNOzs7YenSpQpt7u7uwqxZswRBEAQAwvTp0+XbMjIyBIlEIhw4cEAQhDfnbMuWLfKapKQkQVtbW9i6dasgCIIQFBQkABAiIyPlNXfu3BEACKdOnZK3JSYmCtra2sK2bdsEQRCEHTt2CHp6ekJmZqYgCIKQlpYmaGlpCSEhIYIgFO/n8KBBgwQ7OzshNzdXXvPpp58KvXv3/ncvWBniTBCVqfv376Nfv35wdHSEgYGB/FJJbGys0voRI0Zgy5Yt8PDwwMSJE3H69Gn5tosXL+LevXvQ19eHnp4e9PT0YGJiguzs7HdeYqPC3v5NX1VVFaampqhbt668zdLSEgCQkJCgdH8vLy9MmDABc+fOxVdffYUWLVqUWt8aNGig8PjixYsIDg6Wn3M9PT20a9cOMpkM0dHRpfa8YlfwdX+fmzdvIjs7G23atFE4N7///jvfj6Xg7feorq4u9PX1C70fvby85P82MTGBs7MzoqKi5G0aGhoKx4mKioKamhoaN24sbzM1NVXYr1OnTlBTU5Nfftu+fTv09fXRtm1bAMX/OVy7dm2oqqrKH1tbWxf586QiceUblanOnTvD1tYWa9asQZUqVSCTyVCnTp0iL1916NABDx8+REhICI4cOYJWrVph5MiRWLJkCWQyGerXr19oqhYAzM3Ny3oolYq6urrCY4lEotAmkUgAoMjLljKZDKdOnYKqqqp8PcH7qKioQCjwV3qUrRHQ1dUt9Fz/+9//MHr06EK11apVK9Zz0/sVfN3fd75ef2+EhISgatWqCnX821TvVpz3grL3aFHvx4J1r2lrays8Lvicb7e/rtPQ0MAnn3yCTZs2oU+fPti0aRN69+4tXyhf3J/D/7b/5Y0hiMpMUlISoqKisHr1ajRv3hwAEB4e/t79zM3NERgYiMDAQDRv3lx+3btevXrYunUrLCwsYGBgUNbdp3dYvHgxoqKiEBYWhnbt2iEoKAiDBw8GAPndXHl5eQr7mJubIy4uTv44Ly8P169fh5+f3zufq169erhx4waqV69eyqMQJw0NjULnRhlzc3Ncv35doS0yMlL+n9vrxbaxsbHw8fEpk75WVgXfC1Kp9F/Nap49e1b+i0BKSgru3LkDFxeXIutr1aqF3NxcnDt3Tn4Hb1JSEu7cuQNXV1d5XUBAANq2bYsbN24gNDQUc+fOlW+rbD+HeTmMyszrOwd+/fVX3Lt3D8eOHcP48ePfuc/MmTOxe/du3Lt3Dzdu3MC+ffvkb86AgACYmZmha9euOHnyJKKjoxEWFoYxY8bg8ePH5TEkQv5/hDNnzsRvv/2Gpk2bYvny5RgzZgwePHgAALCwsJDfYRIfH4+0tDQAQMuWLRESEoKQkBDcunULX3zxRbE+gHHSpEk4c+YMRo4cicjISNy9exd79uzBqFGjynKYlZa9vT3OnTuHmJgYJCYmFvnbecuWLXHhwgX8/vvvuHv3LmbNmqUQivT19TFhwgSMGzcO69evx/3793H58mX8/PPPWL9+fXkN56PUsmVL/PHHHzh58iSuX7+OQYMGKVw6Kq45c+bg6NGjuH79OgIDA2FmZlbojr631ahRA127dsWwYcMQHh6OK1euoH///qhatSq6du0qr/Px8YGlpSUCAgJgb2+PJk2ayLdVtp/DDEFUZlRUVLBlyxZcvHgRderUwbhx47B48eJ37qOhoYEpU6bAzc0NLVq0gKqqKrZs2QIA0NHRwYkTJ1CtWjX06NEDrq6uGDJkCF68eFEpfiP5GGRnZyMgIACBgYHo3LkzAGDo0KFo3bo1BgwYIP98mRUrVmD16tWoUqWK/IfrkCFDMGjQIAwcOBA+Pj5wcHB47ywQkL82IiwsDHfv3kXz5s3h6emJGTNmwNraukzHWllNmDABqqqqqFWrFszNzYtcn9euXTvMmDEDEydORMOGDZGeno6BAwcq1MydOxczZ87EwoUL4erqinbt2mHv3r3ytX+k3JQpU9CiRQv4+/ujY8eO6NatG5ycnEp8nG+//RZjxoxB/fr1ERcXhz179rz3c7WCgoJQv359+Pv7w8vLC4IgYP/+/YUuh/ft2xdXrlyRf/TFa5Xt57BEKOoiIREREX1wjh8/Dj8/P6SkpMDIyKiiu/NR40wQERERiRJDEBEREYkSL4cRERGRKHEmiIiIiESJIYiIiIhEiSGIiIiIRIkhiIiIiESJIYiIiIhEiSGIiCqtb775Bh4eHvLHgYGB7/yzAmUlJiYGEokEkZGRRdbY29tj2bJlxT5mcHBwqXxQnkQiwa5du/7zcYg+RgxBRFSuAgMDIZFI5H+53tHRERMmTEBmZmaZP/fy5csRHBxcrNriBBci+rjxr8gTUblr3749goKCkJOTg5MnT+Kzzz5DZmYmVq1aVag2JydH4e8a/ReGhoalchwiqhw4E0RE5U5TUxNWVlawtbVFv379EBAQIL8k8/oS1rp16+Do6AhNTU0IgoC0tDR8/vnnsLCwgIGBAVq2bIkrV64oHPfbb7+FpaUl9PX1MXToUGRnZytsL3g5TCaTYdGiRahevTo0NTVRrVo1zJ8/HwDkfwTU09MTEokEvr6+8v2CgoLg6uoKLS0tuLi4YOXKlQrPExERAU9PT2hpaaFBgwa4fPlyiV+jH374AXXr1oWuri5sbW3xxRdfICMjo1Ddrl27ULNmTWhpaaFNmzZ49OiRwva9e/eifv360NLSgqOjI2bPno3c3NwS94eoMmIIIqIKp62tjZycHPnje/fuYdu2bdi+fbv8clSnTp3w7Nkz7N+/HxcvXkS9evXQqlUrJCcnAwC2bduGWbNmYf78+bhw4QKsra0LhZOCpkyZgkWLFmHGjBm4efMmNm3aBEtLSwD5QQYAjhw5gri4OOzYsQMAsGbNGkybNg3z589HVFQUFixYgBkzZmD9+vUAgMzMTPj7+8PZ2RkXL17EN998gwkTJpT4NVFRUcGKFStw/fp1rF+/HseOHcPEiRMVarKysjB//nysX78ep06dglQqRZ8+feTbDx06hP79+2P06NG4efMmVq9ejeDgYHnQIxI9gYioHA0aNEjo2rWr/PG5c+cEU1NToVevXoIgCMKsWbMEdXV1ISEhQV5z9OhRwcDAQMjOzlY4lpOTk7B69WpBEATBy8tLGD58uML2xo0bC+7u7kqfWyqVCpqamsKaNWuU9jM6OloAIFy+fFmh3dbWVti0aZNC29y5cwUvLy9BEARh9erVgomJiZCZmSnfvmrVKqXHepudnZ2wdOnSIrdv27ZNMDU1lT8OCgoSAAhnz56Vt0VFRQkAhHPnzgmCIAjNmzcXFixYoHCcP/74Q7C2tpY/BiDs3LmzyOclqsy4JoiIyt2+ffugp6eH3Nxc5OTkoGvXrvjxxx/l2+3s7GBubi5/fPHiRWRkZMDU1FThOC9evMD9+/cBAFFRURg+fLjCdi8vL4SGhirtQ1RUFF6+fIlWrVoVu9/Pnz/Ho0ePMHToUAwbNkzenpubK19vFBUVBXd3d+jo6Cj0o6RCQ0OxYMEC3Lx5E1KpFLm5ucjOzkZmZiZ0dXUBAGpqamjQoIF8HxcXFxgZGSEqKgqNGjXCxYsXcf78eYWZn7y8PGRnZyMrK0uhj0RixBBEROXOz88Pq1atgrq6OqpUqVJo4fPr/+Rfk8lksLa2xvHjxwsd69/eJq6trV3ifWQyGYD8S2KNGzdW2KaqqgoAEErhb1I/fPgQHTt2xPDhwzF37lyYmJggPDwcQ4cOVbhsCOTf4l7Q6zaZTIbZs2ejR48ehWq0tLT+cz+JPnYMQURU7nR1dVG9evVi19erVw/Pnj2Dmpoa7O3tlda4urri7NmzGDhwoLzt7NmzRR6zRo0a0NbWxtGjR/HZZ58V2q6hoQEgf+bkNUtLS1StWhUPHjxAQECA0uPWqlULf/zxB168eCEPWu/qhzIXLlxAbm4uvv/+e6io5C/d3LZtW6G63NxcXLhwAY0aNQIA3L59G6mpqXBxcQGQ/7rdvn27RK81kZgwBBHRB69169bw8vJCt27dsGjRIjg7O+Pp06fYv38/unXrhgYNGmDMmDEYNGgQGjRogGbNmmHjxo24ceMGHB0dlR5TS0sLkyZNwsSJE6GhoYGmTZvi+fPnuHHjBoYOHQoLCwtoa2vj4MGDsLGxgZaWFgwNDfHNN99g9OjRMDAwQIcOHfDy5UtcuHABKSkpGD9+PPr164dp06Zh6NChmD59OmJiYrBkyZISjdfJyQm5ubn48ccf0blzZ5w6dQq//PJLoTp1dXWMGjUKK1asgLq6Or788ks0adJEHopmzpwJf39/2Nra4tNPP4WKigquXr2Ka9euYd68eSU/EUSVDO8OI6IPnkQiwf79+9GiRQsMGTIENWvWRJ8+fRATEyO/m6t3796YOXMmJk2ahPr16+Phw4cYMWLEO487Y8YMfPXVV5g5cyZcXV3Ru3dvJCQkAMhfb7NixQqsXr0aVapUQdeuXQEAn332GdauXYvg4GDUrVsXPj4+CA4Olt9Sr6enh7179+LmzZvw9PTEtGnTsGjRohKN18PDAz/88AMWLVqEOnXqYOPGjVi4cGGhOh0dHUyaNAn9+vWDl5cXtLW1sWXLFvn2du3aYd++fTh8+DAaNmyIJk2a4IcffoCdnV2J+kNUWUmE0riATURERPSR4UwQERERiRJDEBEREYkSQxARERGJEkMQERERiRJDEBEREYkSQxARERGJEkMQERERiRJDEBEREYkSQxARERGJEkMQERERiRJDEBEREYnS/wGXCGFCX/8UsQAAAABJRU5ErkJggg==\n"},"metadata":{}}]},{"cell_type":"code","source":"from torch.nn.functional import cross_entropy\n\ndef forward_pass_with_label(batch):\n    # Place all input tensors on the same device as the model\n    inputs = {k:v.to(device) for k,v in batch.items()\n              if k in tokenizer.model_input_names}\n\n    with torch.no_grad():\n        output = model(**inputs)\n        pred_label = torch.argmax(output.logits, axis=-1)\n        loss = cross_entropy(output.logits, batch[\"label\"].to(device),\n                             reduction=\"none\")\n    # Place outputs on CPU for compatibility with other dataset columns\n    return {\"loss\": loss.cpu().numpy(),\n            \"predicted_label\": pred_label.cpu().numpy()}","metadata":{"execution":{"iopub.status.busy":"2023-03-22T16:35:36.300587Z","iopub.execute_input":"2023-03-22T16:35:36.300988Z","iopub.status.idle":"2023-03-22T16:35:36.314666Z","shell.execute_reply.started":"2023-03-22T16:35:36.300953Z","shell.execute_reply":"2023-03-22T16:35:36.313383Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"# Convert our dataset back to PyTorch tensors\nhealth_fact_encoded.set_format(\"torch\",\n                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\n# Compute loss values\nhealth_fact_encoded[\"validation\"] = health_fact_encoded[\"validation\"].map(\n    forward_pass_with_label, batched=True, batch_size=16)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T16:37:32.899608Z","iopub.execute_input":"2023-03-22T16:37:32.900016Z","iopub.status.idle":"2023-03-22T16:37:44.305464Z","shell.execute_reply.started":"2023-03-22T16:37:32.899981Z","shell.execute_reply":"2023-03-22T16:37:44.304197Z"},"trusted":true},"execution_count":62,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/76 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d1f937cb4584de3969e02bc11e2de7b"}},"metadata":{}}]},{"cell_type":"code","source":"health_fact_encoded.set_format(\"pandas\")\ncols = [\"summary\", \"label\", \"predicted_label\", \"loss\"]\ndf_test = health_fact_encoded[\"validation\"][:][cols]\ndf_test[\"label\"] = df_test[\"label\"].apply(label_int2str)\ndf_test[\"predicted_label\"] = (df_test[\"predicted_label\"]\n                              .apply(label_int2str))","metadata":{"execution":{"iopub.status.busy":"2023-03-22T16:38:11.277092Z","iopub.execute_input":"2023-03-22T16:38:11.277464Z","iopub.status.idle":"2023-03-22T16:38:11.385340Z","shell.execute_reply.started":"2023-03-22T16:38:11.277431Z","shell.execute_reply":"2023-03-22T16:38:11.384063Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"with pd.option_context('display.max_colwidth', 2048):\n    display(df_test.sort_values(\"loss\", ascending=False).head(10))","metadata":{"execution":{"iopub.status.busy":"2023-03-22T16:42:35.368558Z","iopub.execute_input":"2023-03-22T16:42:35.369391Z","iopub.status.idle":"2023-03-22T16:42:35.393103Z","shell.execute_reply.started":"2023-03-22T16:42:35.369342Z","shell.execute_reply":"2023-03-22T16:42:35.390548Z"},"trusted":true},"execution_count":77,"outputs":[{"output_type":"display_data","data":{"text/plain":"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             summary  \\\n492                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              SUBJECTS: Viral Phenomena, conservative post, qpolitical, reddit - CLAIM: A step-dad refused to pay for his daughter's wedding at the last minute because she allowed her biological father back into her life. - EXPLANATION: As to whether a 3 November 2015 wedding was canceled as related in the narrative, that claim was provably false. The story was lifted in its entirety from a 3 June 2013 post to Reddit’s r/offmychest titled “My step-daughter wants her “Real Dad” to give her away,” but it was subsequently reposted in subreddits devoted to misogyny and revenge tales. In the original version the wedding was scheduled for 3 August 2013 (not 3 November 2015), and it appeared the dates were revised to make the tale sound new. The user who claimed credit for originating it racked up a grand total of only four posts and was not an established Reddit user. That user returned to update the original post twice, but the story and its subsequent repetitions were still based on a single, unverified Reddit thread from 2013 (which perhaps would have more aptly been posted to this subreddit). While the narrative certainly resonated with readers, there’s no evidence it occurred outside the imagination of the individual who wrote it.   \n903                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        SUBJECTS: New Jersey, Public Health, Florida, Facebook Fact-checks, New York, Coronavirus, Facebook posts,  - CLAIM: “Florida is doing over five times better” than New Jersey and New York in COVID-19 deaths per million people. - EXPLANATION: The claim is accurate on one measure — the number of COVID-19 deaths per capita for the duration of the outbreak. But the claim is that Florida is doing well now, when in fact deaths are on the rise.   \n631                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  SUBJECTS: Facebook Fact-checks, Coronavirus, Viral image,  - CLAIM: Viral image Says Harvard scientists say the coronavirus is “spreading so fast that it will infect 70% of humanity this year.” - EXPLANATION: An epidemiologist at Harvard University projects 40%-70% of adults could catch coronavirus in the coming year.   \n209  SUBJECTS: P-Cure,proton therapy - CLAIM: FDA Clears P-Cure Upright Imaging Solution  for Proton Therapy Enhancement - EXPLANATION: P-Cure Ltd. device We’re fully prepared to believe that there are ways to more accurately administer proton beam therapy with fewer side effects and lower costs. This release makes just those claims and yet provides nothing to back the claims up. There is no quantification of benefits. No true exploration of risks. No explanation of the quality of the evidence. And no true comparison of alternatives. The release muddies the water on what kind of nod it got from the US’s regulatory agency. The company received FDA “clearance” to market the device but not an “approval.” (See the discussion under the Quality of Evidence criteria.) Proton beam therapy represents an alternative form of radiotherapy compared to conventional x-rays. Interest in this form of therapy has grown based upon its ability to deliver higher doses of radiation to a smaller field. This may improve the benefit and reduce the harm for patients with cancer who need radiotherapy. Although not a focus in this release, the proven outcomes of proton vs. conventional radiotherapy in terms of benefits (increased survival) and harms (less damage to surrounding normal tissue) are less clear than the proponents of proton therapy would like to admit. For many cancers, conventional therapy may be adequate. Here the question is whether the delivery of proton beam therapy to patients who are getting it can be enhanced by delivering the treatment to the patient in a seated compared to a flat position. One can imagine for some patients that may lead to a more comfortable treatment, but it isn’t clear how many would derive a measurable benefit. The release focuses on decreases in the discomfort of treatment and implies that some who may not otherwise tolerate this therapy could now have it. All interesting, but no information is provided to support the release’s claims. Finally, the release focuses on the potential benefits to the pr...   \n627                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               SUBJECTS: National, Candidate Biography, Message Machine 2010, Democratic Senatorial Campaign Committee,  - CLAIM: As CEO of WWE, Linda McMahon \"was caught tipping off a ringside physician\" about a federal investigation into illegally distributing steroids to wrestlers. - EXPLANATION: DSCC claims Linda McMahon tipped off ringside physician about steroids investigation   \n776                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       SUBJECTS: National, Civil Rights, Terrorism, Bob Barr,  - CLAIM: The legislation ... that Senator McCain supports would provide the authority for the federal government to surveille American citizens in their own country without any suspicion whatsoever that they're engaging in discussions with terrorists or about criminal activity. - EXPLANATION: Carmakers and policymakers in Europe are staking their futures on a race to electric vehicles. But the vast charging network needed to sustain their vision is patchy, and it’s not clear who’ll pay for it.   \n773                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           SUBJECTS: Public Health, Facebook Fact-checks, Coronavirus, Facebook posts,  - CLAIM: 2019 coronavirus can live for \"up to 3 hours in the air, up to 4 hours on copper, up to 24 hours on cardboard up to 3 days on plastic and stainless steel.” - EXPLANATION: Preliminary research indicates that COVID-19 can live for up to three hours in the air, four hours on copper, 24 hours on cardboard and three days on plastic and stainless steel. Health officials advise people to wash their hands regularly, avoid touching their faces and disinfect their homes daily to prevent the spread of the coronavirus.   \n503                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 SUBJECTS: Language, Offensive Origins - CLAIM: Jimmies, the sprinkles used on confections, are so named as a reference to Jim Crow. - EXPLANATION: It may be the case that among those who refer to dark brown or chocolate sprinkles as “jimmies” and other colors simply as “sprinkles,”  someone simply assumed a potentially racist connection was at work and retroactively invented an explanation for it.   \n797                                                                                                                                                                         SUBJECTS: George Institute for Global Health,High blood pressure - CLAIM: Innovative triple pill significantly lowers blood pressure, study finds - EXPLANATION: This news release does a nice job of explaining the basic design of a study investigating whether a once-daily “triple pill” (that combines 3 different blood pressure-lowering medications) is more effective in lowering blood pressure than doctors’ usual approach. But there’s plenty of room for improvement here including: presenting the findings in a more meaningful context for patients, including the substantial adverse outcomes noted by the researchers, and using much more toned down language for results that are neither novel or all that significant. Notably, there is a significant financial conflict of interest involving the authors. This should have been made clear to readers. The global importance of poorly controlled high blood pressure is well-stated by the authors in the introduction to their study published in JAMA (paraphrased): High blood pressure is the leading cause of mortality and cardiovascular disease globally with most of the disease burden occurring in low- and middle-income countries. The availability and affordability of medications is a critical issue. About one-third of individuals with high blood pressure in these settings receive treatment. Only about half of patients treated in high-income countries and about 1-in-4 patients in low- and middle-income countries achieve blood pressure control. A fixed low-dose combination therapy with inexpensive blood pressure-lowering drugs has the potential to address several barriers to improve blood pressure control worldwide. Indeed, there’s significant need for a simple-to-take, effective, and affordable medication to address this issue. If successful, the benefits to patients, and the profits for developers, would be massive.   \n8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        SUBJECTS:   - CLAIM: Raw Milk Straight from the Cow - EXPLANATION: This story was about a concerning trend in consumer use of unpasteurized milk. As with other potentially functional foods, there are health claims made about the product which have no supporting evidence. The real issue which should have been reported on is the potential for harm associated with consumption of raw milk. The story does not do this justice, and would have been substantially strengthened by a deeper discussion (with experts) about the health risks of unpasteurized milk.   \n\n        label predicted_label      loss  \n492  unproven           false  4.574901  \n903     false            true  4.286610  \n631   mixture           false  4.084052  \n209   mixture           false  3.982743  \n627      true           false  3.832539  \n776      true           false  3.699471  \n773      true           false  3.652257  \n503  unproven           false  3.588532  \n797     false            true  3.469921  \n8        true           false  3.467692  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>summary</th>\n      <th>label</th>\n      <th>predicted_label</th>\n      <th>loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>492</th>\n      <td>SUBJECTS: Viral Phenomena, conservative post, qpolitical, reddit - CLAIM: A step-dad refused to pay for his daughter's wedding at the last minute because she allowed her biological father back into her life. - EXPLANATION: As to whether a 3 November 2015 wedding was canceled as related in the narrative, that claim was provably false. The story was lifted in its entirety from a 3 June 2013 post to Reddit’s r/offmychest titled “My step-daughter wants her “Real Dad” to give her away,” but it was subsequently reposted in subreddits devoted to misogyny and revenge tales. In the original version the wedding was scheduled for 3 August 2013 (not 3 November 2015), and it appeared the dates were revised to make the tale sound new. The user who claimed credit for originating it racked up a grand total of only four posts and was not an established Reddit user. That user returned to update the original post twice, but the story and its subsequent repetitions were still based on a single, unverified Reddit thread from 2013 (which perhaps would have more aptly been posted to this subreddit). While the narrative certainly resonated with readers, there’s no evidence it occurred outside the imagination of the individual who wrote it.</td>\n      <td>unproven</td>\n      <td>false</td>\n      <td>4.574901</td>\n    </tr>\n    <tr>\n      <th>903</th>\n      <td>SUBJECTS: New Jersey, Public Health, Florida, Facebook Fact-checks, New York, Coronavirus, Facebook posts,  - CLAIM: “Florida is doing over five times better” than New Jersey and New York in COVID-19 deaths per million people. - EXPLANATION: The claim is accurate on one measure — the number of COVID-19 deaths per capita for the duration of the outbreak. But the claim is that Florida is doing well now, when in fact deaths are on the rise.</td>\n      <td>false</td>\n      <td>true</td>\n      <td>4.286610</td>\n    </tr>\n    <tr>\n      <th>631</th>\n      <td>SUBJECTS: Facebook Fact-checks, Coronavirus, Viral image,  - CLAIM: Viral image Says Harvard scientists say the coronavirus is “spreading so fast that it will infect 70% of humanity this year.” - EXPLANATION: An epidemiologist at Harvard University projects 40%-70% of adults could catch coronavirus in the coming year.</td>\n      <td>mixture</td>\n      <td>false</td>\n      <td>4.084052</td>\n    </tr>\n    <tr>\n      <th>209</th>\n      <td>SUBJECTS: P-Cure,proton therapy - CLAIM: FDA Clears P-Cure Upright Imaging Solution  for Proton Therapy Enhancement - EXPLANATION: P-Cure Ltd. device We’re fully prepared to believe that there are ways to more accurately administer proton beam therapy with fewer side effects and lower costs. This release makes just those claims and yet provides nothing to back the claims up. There is no quantification of benefits. No true exploration of risks. No explanation of the quality of the evidence. And no true comparison of alternatives. The release muddies the water on what kind of nod it got from the US’s regulatory agency. The company received FDA “clearance” to market the device but not an “approval.” (See the discussion under the Quality of Evidence criteria.) Proton beam therapy represents an alternative form of radiotherapy compared to conventional x-rays. Interest in this form of therapy has grown based upon its ability to deliver higher doses of radiation to a smaller field. This may improve the benefit and reduce the harm for patients with cancer who need radiotherapy. Although not a focus in this release, the proven outcomes of proton vs. conventional radiotherapy in terms of benefits (increased survival) and harms (less damage to surrounding normal tissue) are less clear than the proponents of proton therapy would like to admit. For many cancers, conventional therapy may be adequate. Here the question is whether the delivery of proton beam therapy to patients who are getting it can be enhanced by delivering the treatment to the patient in a seated compared to a flat position. One can imagine for some patients that may lead to a more comfortable treatment, but it isn’t clear how many would derive a measurable benefit. The release focuses on decreases in the discomfort of treatment and implies that some who may not otherwise tolerate this therapy could now have it. All interesting, but no information is provided to support the release’s claims. Finally, the release focuses on the potential benefits to the pr...</td>\n      <td>mixture</td>\n      <td>false</td>\n      <td>3.982743</td>\n    </tr>\n    <tr>\n      <th>627</th>\n      <td>SUBJECTS: National, Candidate Biography, Message Machine 2010, Democratic Senatorial Campaign Committee,  - CLAIM: As CEO of WWE, Linda McMahon \"was caught tipping off a ringside physician\" about a federal investigation into illegally distributing steroids to wrestlers. - EXPLANATION: DSCC claims Linda McMahon tipped off ringside physician about steroids investigation</td>\n      <td>true</td>\n      <td>false</td>\n      <td>3.832539</td>\n    </tr>\n    <tr>\n      <th>776</th>\n      <td>SUBJECTS: National, Civil Rights, Terrorism, Bob Barr,  - CLAIM: The legislation ... that Senator McCain supports would provide the authority for the federal government to surveille American citizens in their own country without any suspicion whatsoever that they're engaging in discussions with terrorists or about criminal activity. - EXPLANATION: Carmakers and policymakers in Europe are staking their futures on a race to electric vehicles. But the vast charging network needed to sustain their vision is patchy, and it’s not clear who’ll pay for it.</td>\n      <td>true</td>\n      <td>false</td>\n      <td>3.699471</td>\n    </tr>\n    <tr>\n      <th>773</th>\n      <td>SUBJECTS: Public Health, Facebook Fact-checks, Coronavirus, Facebook posts,  - CLAIM: 2019 coronavirus can live for \"up to 3 hours in the air, up to 4 hours on copper, up to 24 hours on cardboard up to 3 days on plastic and stainless steel.” - EXPLANATION: Preliminary research indicates that COVID-19 can live for up to three hours in the air, four hours on copper, 24 hours on cardboard and three days on plastic and stainless steel. Health officials advise people to wash their hands regularly, avoid touching their faces and disinfect their homes daily to prevent the spread of the coronavirus.</td>\n      <td>true</td>\n      <td>false</td>\n      <td>3.652257</td>\n    </tr>\n    <tr>\n      <th>503</th>\n      <td>SUBJECTS: Language, Offensive Origins - CLAIM: Jimmies, the sprinkles used on confections, are so named as a reference to Jim Crow. - EXPLANATION: It may be the case that among those who refer to dark brown or chocolate sprinkles as “jimmies” and other colors simply as “sprinkles,”  someone simply assumed a potentially racist connection was at work and retroactively invented an explanation for it.</td>\n      <td>unproven</td>\n      <td>false</td>\n      <td>3.588532</td>\n    </tr>\n    <tr>\n      <th>797</th>\n      <td>SUBJECTS: George Institute for Global Health,High blood pressure - CLAIM: Innovative triple pill significantly lowers blood pressure, study finds - EXPLANATION: This news release does a nice job of explaining the basic design of a study investigating whether a once-daily “triple pill” (that combines 3 different blood pressure-lowering medications) is more effective in lowering blood pressure than doctors’ usual approach. But there’s plenty of room for improvement here including: presenting the findings in a more meaningful context for patients, including the substantial adverse outcomes noted by the researchers, and using much more toned down language for results that are neither novel or all that significant. Notably, there is a significant financial conflict of interest involving the authors. This should have been made clear to readers. The global importance of poorly controlled high blood pressure is well-stated by the authors in the introduction to their study published in JAMA (paraphrased): High blood pressure is the leading cause of mortality and cardiovascular disease globally with most of the disease burden occurring in low- and middle-income countries. The availability and affordability of medications is a critical issue. About one-third of individuals with high blood pressure in these settings receive treatment. Only about half of patients treated in high-income countries and about 1-in-4 patients in low- and middle-income countries achieve blood pressure control. A fixed low-dose combination therapy with inexpensive blood pressure-lowering drugs has the potential to address several barriers to improve blood pressure control worldwide. Indeed, there’s significant need for a simple-to-take, effective, and affordable medication to address this issue. If successful, the benefits to patients, and the profits for developers, would be massive.</td>\n      <td>false</td>\n      <td>true</td>\n      <td>3.469921</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>SUBJECTS:   - CLAIM: Raw Milk Straight from the Cow - EXPLANATION: This story was about a concerning trend in consumer use of unpasteurized milk. As with other potentially functional foods, there are health claims made about the product which have no supporting evidence. The real issue which should have been reported on is the potential for harm associated with consumption of raw milk. The story does not do this justice, and would have been substantially strengthened by a deeper discussion (with experts) about the health risks of unpasteurized milk.</td>\n      <td>true</td>\n      <td>false</td>\n      <td>3.467692</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"with pd.option_context('display.max_colwidth', 512):\n    display(df_test.sort_values(\"loss\", ascending=True).head(10))","metadata":{"execution":{"iopub.status.busy":"2023-03-22T16:41:30.167105Z","iopub.execute_input":"2023-03-22T16:41:30.167553Z","iopub.status.idle":"2023-03-22T16:41:30.188801Z","shell.execute_reply.started":"2023-03-22T16:41:30.167500Z","shell.execute_reply":"2023-03-22T16:41:30.187378Z"},"trusted":true},"execution_count":75,"outputs":[{"output_type":"display_data","data":{"text/plain":"                                                                                                                                                                                                                                                                                                                                                                                 summary  \\\n121                                                                                                            SUBJECTS: Health News - CLAIM: Tom Hanks, wife Rita Wilson test positive for coronavirus in Australia. - EXPLANATION: Oscar-winning actor Tom Hanks and his wife, actress Rita Wilson, have both tested positive for coronavirus in Australia, the actor said on Twitter.   \n595   SUBJECTS: Health News - CLAIM: UK to use firefighters to deliver food, collect bodies in coronavirus crisis. - EXPLANATION: The United Kingdom will use firefighters to help deliver food, retrieve dead bodies and drive ambulances as it braces for the looming peak of the coronavirus outbreak that has already claimed the lives of more than 22,000 people across the world.   \n879                                                                                                                                     SUBJECTS: Health News - CLAIM: Morning-after pill sold over the counter in Canada. - EXPLANATION: The so-called “morning after” pill Plan  B has received full over-the-counter status in Canada, drug  maker Paladin Labs Inc said on Thursday.   \n1204                                                                                                                                                    SUBJECTS: Health News - CLAIM: 'Sesame Street' to welcome first autistic Muppet. - EXPLANATION: Long-running children’s television show “Sesame Street” is welcoming a new kid to the block - a Muppet with autism called Julia.   \n315                                                            SUBJECTS: Health News - CLAIM: Disney closes controversial fat-fighting exhibit. - EXPLANATION: Obesity experts on Friday applauded Walt Disney World for shuttering a new attraction that drew fierce criticism for its potential to shame overweight children and misrepresent the causes of the global obesity crisis.   \n177                                     SUBJECTS: Health News - CLAIM: Methamphetamine use soars in Iran as lifestyles speed up. - EXPLANATION: Women in headscarves and men in tatty clothes puff on a glass pipe as smoke swirls around their faces. The pictures published by Iranian media and blogs in recent months are a sign of a new drug epidemic: shishe, or methamphetamine.   \n766                                                                     SUBJECTS: Health News - CLAIM: Battle ropes become popular go-to fitness tools in U.S. gyms. - EXPLANATION: Battle ropes, the thick and heavy ropes that look as if they could tether a ship to shore, have become go-to fitness tools in gyms for people seeking a tough workout that is also engaging and fun.   \n524                               SUBJECTS: Health News - CLAIM: AstraZeneca's Imfinzi gets speedy FDA review for small cell lung cancer. - EXPLANATION: British drugmaker AstraZeneca Plc said on Friday its immunotherapy cancer treatment Imfinzi has been granted a speedy review by the U.S. medicines watchdog for the treatment of a particularly aggressive type of lung cancer.   \n38                                                                                                SUBJECTS: Health News - CLAIM: China detains man for spreading 'panic' with bird flu rumors. - EXPLANATION: Police in central China have detained a man who spread “panic” with a graphic rumor about the arrival of bird flu in his home province, state media reported on Wednesday.   \n229                          SUBJECTS: Health News - CLAIM: Cabin crew swap aircraft aisles for hospitals in UK's coronavirus fight. - EXPLANATION: Britain’s temporary hospitals are seeking volunteers from airlines, calling on cabin crew members who are currently grounded to use their first-aid skills and calm manner to help get the new Nightingale hospitals up and running.   \n\n     label predicted_label      loss  \n121   true            true  0.001138  \n595   true            true  0.001159  \n879   true            true  0.001162  \n1204  true            true  0.001164  \n315   true            true  0.001166  \n177   true            true  0.001170  \n766   true            true  0.001176  \n524   true            true  0.001176  \n38    true            true  0.001178  \n229   true            true  0.001179  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>summary</th>\n      <th>label</th>\n      <th>predicted_label</th>\n      <th>loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>121</th>\n      <td>SUBJECTS: Health News - CLAIM: Tom Hanks, wife Rita Wilson test positive for coronavirus in Australia. - EXPLANATION: Oscar-winning actor Tom Hanks and his wife, actress Rita Wilson, have both tested positive for coronavirus in Australia, the actor said on Twitter.</td>\n      <td>true</td>\n      <td>true</td>\n      <td>0.001138</td>\n    </tr>\n    <tr>\n      <th>595</th>\n      <td>SUBJECTS: Health News - CLAIM: UK to use firefighters to deliver food, collect bodies in coronavirus crisis. - EXPLANATION: The United Kingdom will use firefighters to help deliver food, retrieve dead bodies and drive ambulances as it braces for the looming peak of the coronavirus outbreak that has already claimed the lives of more than 22,000 people across the world.</td>\n      <td>true</td>\n      <td>true</td>\n      <td>0.001159</td>\n    </tr>\n    <tr>\n      <th>879</th>\n      <td>SUBJECTS: Health News - CLAIM: Morning-after pill sold over the counter in Canada. - EXPLANATION: The so-called “morning after” pill Plan  B has received full over-the-counter status in Canada, drug  maker Paladin Labs Inc said on Thursday.</td>\n      <td>true</td>\n      <td>true</td>\n      <td>0.001162</td>\n    </tr>\n    <tr>\n      <th>1204</th>\n      <td>SUBJECTS: Health News - CLAIM: 'Sesame Street' to welcome first autistic Muppet. - EXPLANATION: Long-running children’s television show “Sesame Street” is welcoming a new kid to the block - a Muppet with autism called Julia.</td>\n      <td>true</td>\n      <td>true</td>\n      <td>0.001164</td>\n    </tr>\n    <tr>\n      <th>315</th>\n      <td>SUBJECTS: Health News - CLAIM: Disney closes controversial fat-fighting exhibit. - EXPLANATION: Obesity experts on Friday applauded Walt Disney World for shuttering a new attraction that drew fierce criticism for its potential to shame overweight children and misrepresent the causes of the global obesity crisis.</td>\n      <td>true</td>\n      <td>true</td>\n      <td>0.001166</td>\n    </tr>\n    <tr>\n      <th>177</th>\n      <td>SUBJECTS: Health News - CLAIM: Methamphetamine use soars in Iran as lifestyles speed up. - EXPLANATION: Women in headscarves and men in tatty clothes puff on a glass pipe as smoke swirls around their faces. The pictures published by Iranian media and blogs in recent months are a sign of a new drug epidemic: shishe, or methamphetamine.</td>\n      <td>true</td>\n      <td>true</td>\n      <td>0.001170</td>\n    </tr>\n    <tr>\n      <th>766</th>\n      <td>SUBJECTS: Health News - CLAIM: Battle ropes become popular go-to fitness tools in U.S. gyms. - EXPLANATION: Battle ropes, the thick and heavy ropes that look as if they could tether a ship to shore, have become go-to fitness tools in gyms for people seeking a tough workout that is also engaging and fun.</td>\n      <td>true</td>\n      <td>true</td>\n      <td>0.001176</td>\n    </tr>\n    <tr>\n      <th>524</th>\n      <td>SUBJECTS: Health News - CLAIM: AstraZeneca's Imfinzi gets speedy FDA review for small cell lung cancer. - EXPLANATION: British drugmaker AstraZeneca Plc said on Friday its immunotherapy cancer treatment Imfinzi has been granted a speedy review by the U.S. medicines watchdog for the treatment of a particularly aggressive type of lung cancer.</td>\n      <td>true</td>\n      <td>true</td>\n      <td>0.001176</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>SUBJECTS: Health News - CLAIM: China detains man for spreading 'panic' with bird flu rumors. - EXPLANATION: Police in central China have detained a man who spread “panic” with a graphic rumor about the arrival of bird flu in his home province, state media reported on Wednesday.</td>\n      <td>true</td>\n      <td>true</td>\n      <td>0.001178</td>\n    </tr>\n    <tr>\n      <th>229</th>\n      <td>SUBJECTS: Health News - CLAIM: Cabin crew swap aircraft aisles for hospitals in UK's coronavirus fight. - EXPLANATION: Britain’s temporary hospitals are seeking volunteers from airlines, calling on cabin crew members who are currently grounded to use their first-aid skills and calm manner to help get the new Nightingale hospitals up and running.</td>\n      <td>true</td>\n      <td>true</td>\n      <td>0.001179</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import wandb\nwandb.init(project=\"pubhealth-hyperparameter-search\")","metadata":{"execution":{"iopub.status.busy":"2023-03-22T12:17:32.489935Z","iopub.execute_input":"2023-03-22T12:17:32.490718Z","iopub.status.idle":"2023-03-22T12:18:17.525347Z","shell.execute_reply.started":"2023-03-22T12:17:32.490680Z","shell.execute_reply":"2023-03-22T12:18:17.524271Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.14.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.13.10"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230322_121746-inlz973z</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/tansaku/pubhealth-hyperparameter-search/runs/inlz973z' target=\"_blank\">driven-feather-12</a></strong> to <a href='https://wandb.ai/tansaku/pubhealth-hyperparameter-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/tansaku/pubhealth-hyperparameter-search' target=\"_blank\">https://wandb.ai/tansaku/pubhealth-hyperparameter-search</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/tansaku/pubhealth-hyperparameter-search/runs/inlz973z' target=\"_blank\">https://wandb.ai/tansaku/pubhealth-hyperparameter-search/runs/inlz973z</a>"},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tansaku/pubhealth-hyperparameter-search/runs/inlz973z?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7f65d17d7e50>"},"metadata":{}}]},{"cell_type":"code","source":"learning_rates = [2e-5, 3e-5, 5e-5]\nbatch_sizes = [8, 16]\nweight_decays = [0.0, 0.01]\nwarmup_steps = [0, 100, 200]\nidx = 1\nfor learning_rate in learning_rates:\n    for batch_size in batch_sizes:\n        for weight_decay in weight_decays:\n            for warmup_step in warmup_steps:\n                print(idx, learning_rate, batch_size, weight_decay, warmup_step)\n                idx += 1","metadata":{"execution":{"iopub.status.busy":"2023-03-22T12:44:10.314098Z","iopub.execute_input":"2023-03-22T12:44:10.314806Z","iopub.status.idle":"2023-03-22T12:44:10.363642Z","shell.execute_reply.started":"2023-03-22T12:44:10.314769Z","shell.execute_reply":"2023-03-22T12:44:10.360419Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"1 2e-05 8 0.0 0\n2 2e-05 8 0.0 100\n3 2e-05 8 0.0 200\n4 2e-05 8 0.01 0\n5 2e-05 8 0.01 100\n6 2e-05 8 0.01 200\n7 2e-05 16 0.0 0\n8 2e-05 16 0.0 100\n9 2e-05 16 0.0 200\n10 2e-05 16 0.01 0\n11 2e-05 16 0.01 100\n12 2e-05 16 0.01 200\n13 3e-05 8 0.0 0\n14 3e-05 8 0.0 100\n15 3e-05 8 0.0 200\n16 3e-05 8 0.01 0\n17 3e-05 8 0.01 100\n18 3e-05 8 0.01 200\n19 3e-05 16 0.0 0\n20 3e-05 16 0.0 100\n21 3e-05 16 0.0 200\n22 3e-05 16 0.01 0\n23 3e-05 16 0.01 100\n24 3e-05 16 0.01 200\n25 5e-05 8 0.0 0\n26 5e-05 8 0.0 100\n27 5e-05 8 0.0 200\n28 5e-05 8 0.01 0\n29 5e-05 8 0.01 100\n30 5e-05 8 0.01 200\n31 5e-05 16 0.0 0\n32 5e-05 16 0.0 100\n33 5e-05 16 0.0 200\n34 5e-05 16 0.01 0\n35 5e-05 16 0.01 100\n36 5e-05 16 0.01 200\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nnum_labels=4\n# Define the model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\n# Function to train and evaluate the model with given hyperparameters\ndef train_evaluate_model(learning_rate, batch_size, weight_decay, warmup_step):\n    model = (AutoModelForSequenceClassification\n         .from_pretrained(model_ckpt, num_labels=num_labels)\n         .to(device))\n\n    training_args = TrainingArguments(\n        output_dir='./results',\n        num_train_epochs=4,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        learning_rate=learning_rate,\n        weight_decay=weight_decay,\n        warmup_steps=warmup_step,\n        logging_dir='./logs',\n        logging_steps=100,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        save_total_limit=1,\n        report_to=\"wandb\",\n        load_best_model_at_end=True,\n        metric_for_best_model=\"accuracy\",\n        greater_is_better=True,\n    )\n\n    trainer = Trainer(\n        model=model, \n        args=training_args,\n        compute_metrics=compute_metrics,\n        train_dataset=health_fact_encoded[\"train\"],\n        eval_dataset=health_fact_encoded[\"validation\"],\n        tokenizer=tokenizer,\n        callbacks=[CustomEarlyStoppingCallback(early_stopping_patience=1)],\n    )\n\n    # Train the model\n    trainer.train()\n    \n    # would be good to run the below up front to check that this actually works ...\n\n    # Evaluate the model on the validation set\n    results = trainer.evaluate(health_fact_encoded[\"validation\"])\n    \n    summary = {\n        \"learning_rate\": learning_rate,\n        \"batch_size\": batch_size,\n        \"weight_decay\": weight_decay,\n        \"warmup_step\": warmup_step,\n        \"accuracy\": results[\"eval_accuracy\"],\n    }\n    print(summary)\n    # Log the hyperparameters and results in wandb\n    wandb.log(summary)\n\n# Perform hyperparameter search\nlearning_rates = [2e-5, 3e-5, 5e-5]\nbatch_sizes = [8, 16]\nweight_decays = [0.0, 0.01]\nwarmup_steps = [0, 100, 200]\n\nfor learning_rate in learning_rates:\n    for batch_size in batch_sizes:\n        for weight_decay in weight_decays:\n            for warmup_step in warmup_steps:\n                train_evaluate_model(learning_rate, batch_size, weight_decay, warmup_step)\n\n\n# Close wandb\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:12:57.064776Z","iopub.execute_input":"2023-03-21T10:12:57.065531Z","iopub.status.idle":"2023-03-21T20:28:01.965027Z","shell.execute_reply.started":"2023-03-21T10:12:57.065491Z","shell.execute_reply":"2023-03-21T20:28:01.964109Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 1.0252, 'learning_rate': 1.959216965742251e-05, 'epoch': 0.08}\n{'loss': 0.8932, 'learning_rate': 1.9184339314845025e-05, 'epoch': 0.16}\n{'loss': 0.801, 'learning_rate': 1.877650897226754e-05, 'epoch': 0.24}\n{'loss': 0.7737, 'learning_rate': 1.8368678629690052e-05, 'epoch': 0.33}\n{'loss': 0.7832, 'learning_rate': 1.7960848287112562e-05, 'epoch': 0.41}\n{'loss': 0.7944, 'learning_rate': 1.7553017944535075e-05, 'epoch': 0.49}\n{'loss': 0.7521, 'learning_rate': 1.714518760195759e-05, 'epoch': 0.57}\n{'loss': 0.7588, 'learning_rate': 1.67373572593801e-05, 'epoch': 0.65}\n{'loss': 0.7484, 'learning_rate': 1.6329526916802612e-05, 'epoch': 0.73}\n{'loss': 0.7169, 'learning_rate': 1.5921696574225122e-05, 'epoch': 0.82}\n{'loss': 0.6943, 'learning_rate': 1.5513866231647635e-05, 'epoch': 0.9}\n{'loss': 0.6838, 'learning_rate': 1.5106035889070147e-05, 'epoch': 0.98}\n","output_type":"stream"},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\n","output_type":"stream"},{"name":"stdout","text":"{'eval_loss': 0.7055772542953491, 'eval_accuracy': 0.71334431630972, 'eval_f1': 0.6903688677860969, 'eval_runtime': 11.1119, 'eval_samples_per_second': 109.252, 'eval_steps_per_second': 13.679, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"Model weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.6305, 'learning_rate': 1.469820554649266e-05, 'epoch': 1.06}\n{'loss': 0.6263, 'learning_rate': 1.4290375203915172e-05, 'epoch': 1.14}\n{'loss': 0.5928, 'learning_rate': 1.3882544861337685e-05, 'epoch': 1.22}\n{'loss': 0.6054, 'learning_rate': 1.3474714518760197e-05, 'epoch': 1.31}\n{'loss': 0.6606, 'learning_rate': 1.3066884176182709e-05, 'epoch': 1.39}\n{'loss': 0.5922, 'learning_rate': 1.2659053833605222e-05, 'epoch': 1.47}\n{'loss': 0.6103, 'learning_rate': 1.2251223491027732e-05, 'epoch': 1.55}\n{'loss': 0.57, 'learning_rate': 1.1843393148450246e-05, 'epoch': 1.63}\n{'loss': 0.6311, 'learning_rate': 1.1435562805872757e-05, 'epoch': 1.71}\n{'loss': 0.5665, 'learning_rate': 1.102773246329527e-05, 'epoch': 1.79}\n{'loss': 0.5433, 'learning_rate': 1.0619902120717782e-05, 'epoch': 1.88}\n{'loss': 0.6221, 'learning_rate': 1.0212071778140294e-05, 'epoch': 1.96}\n","output_type":"stream"},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\n","output_type":"stream"},{"name":"stdout","text":"{'eval_loss': 0.6940823197364807, 'eval_accuracy': 0.7397034596375618, 'eval_f1': 0.724394130116743, 'eval_runtime': 11.1378, 'eval_samples_per_second': 108.999, 'eval_steps_per_second': 13.647, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"Model weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 0.5241, 'learning_rate': 9.804241435562807e-06, 'epoch': 2.04}\n{'loss': 0.4754, 'learning_rate': 9.396411092985319e-06, 'epoch': 2.12}\n{'loss': 0.4245, 'learning_rate': 8.98858075040783e-06, 'epoch': 2.2}\n{'loss': 0.4892, 'learning_rate': 8.580750407830342e-06, 'epoch': 2.28}\n{'loss': 0.4707, 'learning_rate': 8.172920065252856e-06, 'epoch': 2.37}\n{'loss': 0.4692, 'learning_rate': 7.765089722675368e-06, 'epoch': 2.45}\n{'loss': 0.4223, 'learning_rate': 7.35725938009788e-06, 'epoch': 2.53}\n{'loss': 0.4077, 'learning_rate': 6.949429037520392e-06, 'epoch': 2.61}\n{'loss': 0.4449, 'learning_rate': 6.541598694942904e-06, 'epoch': 2.69}\n{'loss': 0.4781, 'learning_rate': 6.133768352365417e-06, 'epoch': 2.77}\n{'loss': 0.4735, 'learning_rate': 5.725938009787929e-06, 'epoch': 2.85}\n{'loss': 0.3973, 'learning_rate': 5.31810766721044e-06, 'epoch': 2.94}\n","output_type":"stream"},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\n","output_type":"stream"},{"name":"stdout","text":"{'eval_loss': 0.7572960257530212, 'eval_accuracy': 0.7397034596375618, 'eval_f1': 0.735014788109258, 'eval_runtime': 11.0975, 'eval_samples_per_second': 109.394, 'eval_steps_per_second': 13.697, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"Model weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7397034596375618).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\n","output_type":"stream"},{"name":"stdout","text":"{'train_runtime': 863.3584, 'train_samples_per_second': 45.423, 'train_steps_per_second': 5.68, 'train_loss': 0.6134814085553301, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"{'eval_loss': 0.6940823197364807, 'eval_accuracy': 0.7397034596375618, 'eval_f1': 0.724394130116743, 'eval_runtime': 11.1364, 'eval_samples_per_second': 109.011, 'eval_steps_per_second': 13.649, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3678' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3678/4904 14:39 < 04:53, 4.18 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.702300</td>\n      <td>0.695786</td>\n      <td>0.712521</td>\n      <td>0.688563</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.616100</td>\n      <td>0.680612</td>\n      <td>0.744646</td>\n      <td>0.729268</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.405800</td>\n      <td>0.747778</td>\n      <td>0.742175</td>\n      <td>0.737500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7446457990115322).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3678' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3678/4904 14:41 < 04:53, 4.17 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.698400</td>\n      <td>0.695706</td>\n      <td>0.711697</td>\n      <td>0.686455</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.619600</td>\n      <td>0.669680</td>\n      <td>0.750412</td>\n      <td>0.737101</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.410500</td>\n      <td>0.737341</td>\n      <td>0.747941</td>\n      <td>0.744122</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7504118616144976).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3678' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3678/4904 14:42 < 04:54, 4.17 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.692100</td>\n      <td>0.694163</td>\n      <td>0.712521</td>\n      <td>0.688047</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.608800</td>\n      <td>0.687121</td>\n      <td>0.745470</td>\n      <td>0.732108</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.394100</td>\n      <td>0.748222</td>\n      <td>0.745470</td>\n      <td>0.745778</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7454695222405272).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3678' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3678/4904 14:42 < 04:54, 4.17 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.703100</td>\n      <td>0.695265</td>\n      <td>0.712521</td>\n      <td>0.688829</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.616300</td>\n      <td>0.682636</td>\n      <td>0.745470</td>\n      <td>0.731350</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.405400</td>\n      <td>0.751938</td>\n      <td>0.744646</td>\n      <td>0.739652</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7454695222405272).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3678' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3678/4904 14:43 < 04:54, 4.16 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.698600</td>\n      <td>0.695440</td>\n      <td>0.716639</td>\n      <td>0.692249</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.620200</td>\n      <td>0.669181</td>\n      <td>0.750412</td>\n      <td>0.736323</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.408800</td>\n      <td>0.737615</td>\n      <td>0.747941</td>\n      <td>0.744051</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7504118616144976).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1226' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1226/2452 09:09 < 09:10, 2.23 it/s, Epoch 2/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.695400</td>\n      <td>0.677611</td>\n      <td>0.726524</td>\n      <td>0.713957</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.593400</td>\n      <td>0.674695</td>\n      <td>0.724876</td>\n      <td>0.704755</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-613 (score: 0.7265238879736409).\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.697800</td>\n      <td>0.677509</td>\n      <td>0.723229</td>\n      <td>0.713332</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.595200</td>\n      <td>0.665070</td>\n      <td>0.737232</td>\n      <td>0.718229</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.476000</td>\n      <td>0.674750</td>\n      <td>0.742998</td>\n      <td>0.738985</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.395300</td>\n      <td>0.725540</td>\n      <td>0.739703</td>\n      <td>0.731357</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.742998352553542).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:20, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.703700</td>\n      <td>0.680429</td>\n      <td>0.727348</td>\n      <td>0.714810</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.598200</td>\n      <td>0.665102</td>\n      <td>0.737232</td>\n      <td>0.717386</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.468500</td>\n      <td>0.668433</td>\n      <td>0.741351</td>\n      <td>0.738134</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.378600</td>\n      <td>0.720160</td>\n      <td>0.741351</td>\n      <td>0.735116</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.7413509060955519).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.694500</td>\n      <td>0.677555</td>\n      <td>0.726524</td>\n      <td>0.714023</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.587900</td>\n      <td>0.674347</td>\n      <td>0.734761</td>\n      <td>0.714436</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.467100</td>\n      <td>0.669087</td>\n      <td>0.749588</td>\n      <td>0.744347</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.388400</td>\n      <td>0.716362</td>\n      <td>0.748764</td>\n      <td>0.740981</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.7495881383855024).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.696900</td>\n      <td>0.674794</td>\n      <td>0.725700</td>\n      <td>0.713512</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.589600</td>\n      <td>0.672211</td>\n      <td>0.733937</td>\n      <td>0.714470</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.466000</td>\n      <td>0.669973</td>\n      <td>0.747941</td>\n      <td>0.744986</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.380900</td>\n      <td>0.711991</td>\n      <td>0.754530</td>\n      <td>0.748496</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7545304777594728).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.703200</td>\n      <td>0.679749</td>\n      <td>0.725700</td>\n      <td>0.712406</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.597800</td>\n      <td>0.665815</td>\n      <td>0.734761</td>\n      <td>0.714839</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.469200</td>\n      <td>0.670259</td>\n      <td>0.742998</td>\n      <td>0.739851</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.378600</td>\n      <td>0.721529</td>\n      <td>0.741351</td>\n      <td>0.735104</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.742998352553542).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:35, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.702500</td>\n      <td>0.681688</td>\n      <td>0.723229</td>\n      <td>0.696683</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.577900</td>\n      <td>0.728081</td>\n      <td>0.734761</td>\n      <td>0.716302</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.322500</td>\n      <td>0.861972</td>\n      <td>0.738880</td>\n      <td>0.735830</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.300600</td>\n      <td>1.084361</td>\n      <td>0.736409</td>\n      <td>0.733986</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-3678 (score: 0.7388797364085667).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3678' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3678/4904 14:43 < 04:54, 4.16 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.702500</td>\n      <td>0.679674</td>\n      <td>0.722405</td>\n      <td>0.697467</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.597700</td>\n      <td>0.683762</td>\n      <td>0.744646</td>\n      <td>0.729278</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.356100</td>\n      <td>0.835116</td>\n      <td>0.736409</td>\n      <td>0.732184</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7446457990115322).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:39, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.714200</td>\n      <td>0.693460</td>\n      <td>0.715815</td>\n      <td>0.688587</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.611000</td>\n      <td>0.705205</td>\n      <td>0.733114</td>\n      <td>0.717944</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.368500</td>\n      <td>0.779808</td>\n      <td>0.733937</td>\n      <td>0.728725</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.307300</td>\n      <td>0.966724</td>\n      <td>0.738056</td>\n      <td>0.732295</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-4904 (score: 0.7380560131795717).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:41, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.702800</td>\n      <td>0.678670</td>\n      <td>0.726524</td>\n      <td>0.700565</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.574800</td>\n      <td>0.699367</td>\n      <td>0.736409</td>\n      <td>0.720355</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.327300</td>\n      <td>0.843683</td>\n      <td>0.742998</td>\n      <td>0.739571</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.304100</td>\n      <td>1.053005</td>\n      <td>0.743822</td>\n      <td>0.740185</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-4904 (score: 0.7438220757825371).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:41, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.704500</td>\n      <td>0.681482</td>\n      <td>0.716639</td>\n      <td>0.691288</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.600900</td>\n      <td>0.692552</td>\n      <td>0.739703</td>\n      <td>0.724360</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.357300</td>\n      <td>0.821473</td>\n      <td>0.740527</td>\n      <td>0.736732</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.277900</td>\n      <td>1.027645</td>\n      <td>0.740527</td>\n      <td>0.738213</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-3678 (score: 0.7405271828665568).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:41, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.703000</td>\n      <td>0.692957</td>\n      <td>0.708402</td>\n      <td>0.680576</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.605900</td>\n      <td>0.690434</td>\n      <td>0.734761</td>\n      <td>0.718889</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.354000</td>\n      <td>0.809770</td>\n      <td>0.741351</td>\n      <td>0.736511</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.312200</td>\n      <td>1.023119</td>\n      <td>0.742998</td>\n      <td>0.739305</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-4904 (score: 0.742998352553542).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1839' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1839/2452 13:46 < 04:35, 2.22 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.692800</td>\n      <td>0.666872</td>\n      <td>0.728995</td>\n      <td>0.718880</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.565600</td>\n      <td>0.668586</td>\n      <td>0.747941</td>\n      <td>0.731736</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.404400</td>\n      <td>0.705550</td>\n      <td>0.740527</td>\n      <td>0.738255</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1226 (score: 0.7479406919275123).\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1226' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1226/2452 09:10 < 09:11, 2.22 it/s, Epoch 2/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.693900</td>\n      <td>0.676170</td>\n      <td>0.728171</td>\n      <td>0.718216</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.576900</td>\n      <td>0.676756</td>\n      <td>0.727348</td>\n      <td>0.710966</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-613 (score: 0.728171334431631).\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1839' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1839/2452 13:45 < 04:35, 2.23 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.690600</td>\n      <td>0.679991</td>\n      <td>0.724053</td>\n      <td>0.712898</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.584600</td>\n      <td>0.659111</td>\n      <td>0.744646</td>\n      <td>0.728230</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.417000</td>\n      <td>0.717380</td>\n      <td>0.738056</td>\n      <td>0.732364</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1226 (score: 0.7446457990115322).\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.687400</td>\n      <td>0.679228</td>\n      <td>0.726524</td>\n      <td>0.710899</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.566300</td>\n      <td>0.667801</td>\n      <td>0.735585</td>\n      <td>0.717613</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.409200</td>\n      <td>0.706129</td>\n      <td>0.738056</td>\n      <td>0.734821</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.300500</td>\n      <td>0.779258</td>\n      <td>0.742998</td>\n      <td>0.737677</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.742998352553542).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:22, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.697300</td>\n      <td>0.667620</td>\n      <td>0.730643</td>\n      <td>0.716925</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.570000</td>\n      <td>0.665627</td>\n      <td>0.734761</td>\n      <td>0.719291</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.407400</td>\n      <td>0.696674</td>\n      <td>0.738056</td>\n      <td>0.738038</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.301000</td>\n      <td>0.788392</td>\n      <td>0.736409</td>\n      <td>0.730842</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.7380560131795717).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1839' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1839/2452 13:46 < 04:35, 2.22 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.696800</td>\n      <td>0.665594</td>\n      <td>0.731466</td>\n      <td>0.715351</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.577300</td>\n      <td>0.667241</td>\n      <td>0.739703</td>\n      <td>0.721338</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.399200</td>\n      <td>0.691265</td>\n      <td>0.738056</td>\n      <td>0.736906</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1226 (score: 0.7397034596375618).\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3678' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3678/4904 14:43 < 04:54, 4.16 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.737600</td>\n      <td>0.689678</td>\n      <td>0.716639</td>\n      <td>0.693725</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.609600</td>\n      <td>0.725373</td>\n      <td>0.738880</td>\n      <td>0.724048</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.367900</td>\n      <td>0.893925</td>\n      <td>0.735585</td>\n      <td>0.725919</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7388797364085667).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:40, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.744600</td>\n      <td>0.707010</td>\n      <td>0.718287</td>\n      <td>0.689123</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.609200</td>\n      <td>0.738522</td>\n      <td>0.742175</td>\n      <td>0.725627</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.305400</td>\n      <td>0.837749</td>\n      <td>0.752059</td>\n      <td>0.748631</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.230400</td>\n      <td>1.150624</td>\n      <td>0.752059</td>\n      <td>0.747039</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-3678 (score: 0.7520593080724877).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:36, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.710100</td>\n      <td>0.729627</td>\n      <td>0.700165</td>\n      <td>0.668853</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.602600</td>\n      <td>0.698634</td>\n      <td>0.736409</td>\n      <td>0.721260</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.353300</td>\n      <td>0.903453</td>\n      <td>0.744646</td>\n      <td>0.732206</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.244000</td>\n      <td>1.150242</td>\n      <td>0.736409</td>\n      <td>0.732604</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-3678 (score: 0.7446457990115322).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3678' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3678/4904 14:43 < 04:54, 4.16 it/s, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.726500</td>\n      <td>0.686998</td>\n      <td>0.720758</td>\n      <td>0.692170</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.575700</td>\n      <td>0.684614</td>\n      <td>0.751236</td>\n      <td>0.738765</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.331000</td>\n      <td>0.973908</td>\n      <td>0.747117</td>\n      <td>0.732642</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7512355848434926).\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:38, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.727000</td>\n      <td>0.712084</td>\n      <td>0.713344</td>\n      <td>0.683962</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.638200</td>\n      <td>0.695704</td>\n      <td>0.737232</td>\n      <td>0.723807</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.324400</td>\n      <td>0.869525</td>\n      <td>0.745470</td>\n      <td>0.735129</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.254900</td>\n      <td>1.126469</td>\n      <td>0.740527</td>\n      <td>0.738411</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-3678 (score: 0.7454695222405272).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4904\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4904' max='4904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4904/4904 19:37, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.727000</td>\n      <td>0.715475</td>\n      <td>0.703460</td>\n      <td>0.672203</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.583200</td>\n      <td>0.700296</td>\n      <td>0.742998</td>\n      <td>0.730058</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.364000</td>\n      <td>0.871863</td>\n      <td>0.750412</td>\n      <td>0.744476</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.284500</td>\n      <td>1.180633</td>\n      <td>0.733114</td>\n      <td>0.728366</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-3678\nConfiguration saved in ./results/checkpoint-3678/config.json\nModel weights saved in ./results/checkpoint-3678/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-3678/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-3678/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\nSaving model checkpoint to ./results/checkpoint-4904\nConfiguration saved in ./results/checkpoint-4904/config.json\nModel weights saved in ./results/checkpoint-4904/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-4904/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-4904/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-3678 (score: 0.7504118616144976).\nDeleting older checkpoint [results/checkpoint-3678] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-4904] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [152/152 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.689000</td>\n      <td>0.671063</td>\n      <td>0.724053</td>\n      <td>0.708760</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.553400</td>\n      <td>0.678851</td>\n      <td>0.731466</td>\n      <td>0.712602</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.343100</td>\n      <td>0.755990</td>\n      <td>0.746293</td>\n      <td>0.744534</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.224000</td>\n      <td>0.919033</td>\n      <td>0.738056</td>\n      <td>0.734555</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.7462932454695222).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.702000</td>\n      <td>0.673133</td>\n      <td>0.728171</td>\n      <td>0.719515</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.552500</td>\n      <td>0.697145</td>\n      <td>0.732290</td>\n      <td>0.713608</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.347100</td>\n      <td>0.756907</td>\n      <td>0.739703</td>\n      <td>0.736556</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.236300</td>\n      <td>0.920691</td>\n      <td>0.734761</td>\n      <td>0.731255</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.7397034596375618).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.706100</td>\n      <td>0.675854</td>\n      <td>0.715815</td>\n      <td>0.695075</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.576000</td>\n      <td>0.689090</td>\n      <td>0.721582</td>\n      <td>0.699489</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.355100</td>\n      <td>0.749004</td>\n      <td>0.738056</td>\n      <td>0.733153</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.237000</td>\n      <td>0.900966</td>\n      <td>0.753707</td>\n      <td>0.750294</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7537067545304778).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:21, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.688500</td>\n      <td>0.665402</td>\n      <td>0.723229</td>\n      <td>0.710728</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.546900</td>\n      <td>0.690440</td>\n      <td>0.730643</td>\n      <td>0.710562</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.334300</td>\n      <td>0.757644</td>\n      <td>0.740527</td>\n      <td>0.736749</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.225900</td>\n      <td>0.921424</td>\n      <td>0.741351</td>\n      <td>0.737079</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-2452 (score: 0.7413509060955519).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:17, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.703200</td>\n      <td>0.669617</td>\n      <td>0.730643</td>\n      <td>0.720140</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.557500</td>\n      <td>0.692083</td>\n      <td>0.733114</td>\n      <td>0.717699</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.359500</td>\n      <td>0.753943</td>\n      <td>0.741351</td>\n      <td>0.740165</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.229700</td>\n      <td>0.945377</td>\n      <td>0.734761</td>\n      <td>0.729591</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.7413509060955519).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"distilbert-base-uncased\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.26.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 9804\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2452\n  Number of trainable parameters = 66956548\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2452' max='2452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2452/2452 18:16, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.700100</td>\n      <td>0.671288</td>\n      <td>0.722405</td>\n      <td>0.705562</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.575300</td>\n      <td>0.677237</td>\n      <td>0.738056</td>\n      <td>0.721710</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.348400</td>\n      <td>0.752762</td>\n      <td>0.749588</td>\n      <td>0.745155</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.228200</td>\n      <td>0.919576</td>\n      <td>0.740527</td>\n      <td>0.738035</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-613\nConfiguration saved in ./results/checkpoint-613/config.json\nModel weights saved in ./results/checkpoint-613/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-613/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-613/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1226\nConfiguration saved in ./results/checkpoint-1226/config.json\nModel weights saved in ./results/checkpoint-1226/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1226/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1226/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-1839\nConfiguration saved in ./results/checkpoint-1839/config.json\nModel weights saved in ./results/checkpoint-1839/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-1839/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-1839/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-613] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\nSaving model checkpoint to ./results/checkpoint-2452\nConfiguration saved in ./results/checkpoint-2452/config.json\nModel weights saved in ./results/checkpoint-2452/pytorch_model.bin\ntokenizer config file saved in ./results/checkpoint-2452/tokenizer_config.json\nSpecial tokens file saved in ./results/checkpoint-2452/special_tokens_map.json\nDeleting older checkpoint [results/checkpoint-1226] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-1839 (score: 0.7495881383855024).\nDeleting older checkpoint [results/checkpoint-1839] due to args.save_total_limit\nDeleting older checkpoint [results/checkpoint-2452] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim. If main_text, explanation, claim_id, sources, fact_checkers, date_published, subjects, claim are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 1214\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [76/76 00:10]\n    </div>\n    "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▄▆▇▆▆▇▁▅▅▇█▅▄▆▄▅▅▅▆▁▆▅▄▄▄▇▆▇▆▇▆▄█▅▅▇</td></tr><tr><td>batch_size</td><td>▁▁▁▁▁▁██████▁▁▁▁▁▁██████▁▁▁▁▁▁██████</td></tr><tr><td>eval/accuracy</td><td>▆▇▇▇▇▇▄▆▆▄▇█▆▆▆▅▆▃▆▆▆▄▇▆▆▆▆█▇█▆▁▇▆▆▃▄▆▅▇</td></tr><tr><td>eval/f1</td><td>▆▆▇▆▆▇▄▇▅▅██▇▇▇▆▅▃▇▇▇▅▆▇▇▆▆█▇▇▆▁█▇▇▄▅▇▆█</td></tr><tr><td>eval/loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▁▇▄▃▂▁▃▆▂▁▁▃▂▁▂█▄▁▂▂▄▅▂▁▁▅▅▂</td></tr><tr><td>eval/runtime</td><td>▄▇▇▇▇▇▂▂▂▂▂▂▂▇▇█▇▇█▇▂▂▁▂▂▂█▇▇▇▆▆▇▂▂▃▂▁▁▂</td></tr><tr><td>eval/samples_per_second</td><td>▅▂▂▂▂▂▇▆▇▇▇▇▇▂▂▁▂▂▁▂▇▇█▇▇▇▁▂▂▂▂▃▂▇▇▆▇██▇</td></tr><tr><td>eval/steps_per_second</td><td>██████▁▁▁▁▁▁▁███████▁▁▁▁▁▁▇██████▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▃▃▃▃████████████</td></tr><tr><td>train/epoch</td><td>▃▂▆▅▄▂▁▆███▁▁▆▃▁█▅▂▆▃▇▁▅▄▄▄▃▇▄█▅▆▃▇▇██▁▆</td></tr><tr><td>train/global_step</td><td>▃▁▆▄▃▂▃▁▃▃▃▃▂▆▃▁▆▅▁▆▃▇▃▁▁▁▃▁█▅▁▆▄▁█▄▄▄▄▄</td></tr><tr><td>train/learning_rate</td><td>▃▂▂▂▃▃▂▂▂▁▃▃▅▂▄▄▁▄▁▃▄▁▄▃▅▄▅▅▃▆█▃▄▇▄▄▂▇▅▂</td></tr><tr><td>train/loss</td><td>▆▄▄▆▆▆▄▅▆▄█▆█▃▅▆▂▆▂▄▇▂▆▄█▆▅▆▃▅█▃▃▆▃▄▁▇▄▁</td></tr><tr><td>train/total_flos</td><td>▅▅▅▅▅▅▁██████▅████▅▁▅██▅▅██▅████████</td></tr><tr><td>train/train_loss</td><td>▅▅▆▅▅▆█▄▄▄▄▄▁▄▂▁▂▂▅█▅▂▃▅▄▂▁▄▂▁▁▂▂▁▂▂</td></tr><tr><td>train/train_runtime</td><td>▄▅▅▅▅▅▁▇▇▇▇▇█▅████▄▁▄▇▇▄▅██▅██▇▇▇▇▇▇</td></tr><tr><td>train/train_samples_per_second</td><td>▃▃▃▃▃▃█▁▁▁▁▁▁▃▁▁▁▁▄█▄▁▁▄▃▁▁▃▁▁▁▁▁▁▁▁</td></tr><tr><td>train/train_steps_per_second</td><td>██████▆▁▁▁▁▁▅█▅▅▅▅▃▆▃▁▁▂█▅▅█▅▅▁▁▁▁▁▁</td></tr><tr><td>warmup_step</td><td>▁▅█▁▅█▁▅█▁▅█▁▅█▁▅█▁▅█▁▅█▁▅█▁▅█▁▅█▁▅█</td></tr><tr><td>weight_decay</td><td>▁▁▁███▁▁▁███▁▁▁███▁▁▁███▁▁▁███▁▁▁███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.74959</td></tr><tr><td>batch_size</td><td>16</td></tr><tr><td>eval/accuracy</td><td>0.74959</td></tr><tr><td>eval/f1</td><td>0.74515</td></tr><tr><td>eval/loss</td><td>0.75276</td></tr><tr><td>eval/runtime</td><td>10.9199</td></tr><tr><td>eval/samples_per_second</td><td>111.173</td></tr><tr><td>eval/steps_per_second</td><td>6.96</td></tr><tr><td>learning_rate</td><td>5e-05</td></tr><tr><td>train/epoch</td><td>4.0</td></tr><tr><td>train/global_step</td><td>2452</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.2282</td></tr><tr><td>train/total_flos</td><td>5195026790940672.0</td></tr><tr><td>train/train_loss</td><td>0.50441</td></tr><tr><td>train/train_runtime</td><td>1096.4899</td></tr><tr><td>train/train_samples_per_second</td><td>35.765</td></tr><tr><td>train/train_steps_per_second</td><td>2.236</td></tr><tr><td>warmup_step</td><td>200</td></tr><tr><td>weight_decay</td><td>0.01</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">summer-sea-10</strong> at: <a href='https://wandb.ai/tansaku/pubhealth-hyperparameter-search/runs/va73sjls' target=\"_blank\">https://wandb.ai/tansaku/pubhealth-hyperparameter-search/runs/va73sjls</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20230321_101057-va73sjls/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"# from transformers import AutoModelForSequenceClassification\n# from transformers import Trainer, TrainingArguments\n# num_labels=4\n# # Define the model and tokenizer\n# tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\n# def main(verbose=False):\n#     # Initialise run\n#     run = wandb.init()\n#     model = (AutoModelForSequenceClassification\n#          .from_pretrained(model_ckpt, num_labels=num_labels)\n#          .to(device))\n\n#     training_args = TrainingArguments(\n#         output_dir='./results',\n#         num_train_epochs=4,\n#         per_device_train_batch_size=wandb.config.batch_size,\n#         per_device_eval_batch_size=wandb.config.batch_size,\n#         learning_rate=wandb.config.learning_rate,\n#         weight_decay=wandb.config.weight_decay,\n#         dropout_rate=wandb.config.dropout_rate,\n#         logging_dir='./logs',\n#         logging_steps=100,\n#         evaluation_strategy=\"epoch\",\n#         save_strategy=\"epoch\",\n#         save_total_limit=1,\n#         report_to=\"wandb\",\n#         load_best_model_at_end=True,\n#         metric_for_best_model=\"accuracy\",\n#         greater_is_better=True,\n#         patience=1,  # Number of epochs with no improvement after which training will be stopped\n#     )\n\n#     trainer = Trainer(\n#         model=model, \n#         args=training_args,\n#         compute_metrics=compute_metrics,\n#         train_dataset=health_fact_encoded[\"train\"],\n#         eval_dataset=health_fact_encoded[\"validation\"],\n#         tokenizer=tokenizer\n#     )\n\n#     # Train the model\n#     trainer.train()\n\n#     # Evaluate the model on the validation set\n#     results = trainer.evaluate(health_fact_encoded[\"validation\"])\n\n#     # Log the hyperparameters and results in wandb\n#     wandb.log({\n#         \"learning_rate\": learning_rate,\n#         \"batch_size\": batch_size,\n#         \"weight_decay\": weight_decay,\n#         \"warmup_step\": warmup_step,\n#         \"accuracy\": results[\"eval_accuracy\"],\n#     })","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:04:03.538122Z","iopub.status.idle":"2023-03-21T10:04:03.538893Z","shell.execute_reply.started":"2023-03-21T10:04:03.538621Z","shell.execute_reply":"2023-03-21T10:04:03.538648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sweep_configuration = {\n#     'method': 'bayes',  # random, grid or bayes\n#     'name': 'sweep-bayes',\n#     'metric': {'goal': 'maximize', 'name': 'eval_accuracy'},\n#     'parameters': \n#     {\n#         'batch_size': {'values': [8, 16, 32]},\n#         'learning_rate': {'max': 0.1, 'min': 0.0001},\n#         'weight_decay': {'values': [0.0, 0.1, 0.2]},\n#         'dropout_rate': {'max': 0.5, 'min': 0.1}\n#      }\n# }","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:04:03.540377Z","iopub.status.idle":"2023-03-21T10:04:03.541673Z","shell.execute_reply.started":"2023-03-21T10:04:03.541148Z","shell.execute_reply":"2023-03-21T10:04:03.541175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sweep_id = wandb.sweep(sweep=sweep_configuration, entity='tansaku', project='pubhealth-hyperparameter-search')","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:04:03.543056Z","iopub.status.idle":"2023-03-21T10:04:03.543814Z","shell.execute_reply.started":"2023-03-21T10:04:03.543547Z","shell.execute_reply":"2023-03-21T10:04:03.543573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Maximum 'count' runs\n# wandb.agent(sweep_id, function=main, count=30)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T10:04:03.545185Z","iopub.status.idle":"2023-03-21T10:04:03.545949Z","shell.execute_reply.started":"2023-03-21T10:04:03.545684Z","shell.execute_reply":"2023-03-21T10:04:03.545711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}